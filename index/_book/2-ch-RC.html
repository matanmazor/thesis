<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Paradoxical evidence weightings in confidence judgments for detection and discrimination | Self-Modeling in Inference about Absence</title>
  <meta name="description" content="Chapter 2 Paradoxical evidence weightings in confidence judgments for detection and discrimination | Self-Modeling in Inference about Absence" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Paradoxical evidence weightings in confidence judgments for detection and discrimination | Self-Modeling in Inference about Absence" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Paradoxical evidence weightings in confidence judgments for detection and discrimination | Self-Modeling in Inference about Absence" />
  
  
  

<meta name="author" content="Matan Mazor" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="1-ch-search.html"/>
<link rel="next" href="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"></a></li>
<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preliminary Content</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#preface"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#abstract"><i class="fa fa-check"></i>Abstract</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a><ul>
<li class="chapter" data-level="0.1" data-path="introduction.html"><a href="introduction.html#inference-about-absence"><i class="fa fa-check"></i><b>0.1</b> Inference about absence</a></li>
<li class="chapter" data-level="0.2" data-path="introduction.html"><a href="introduction.html#formalabsence"><i class="fa fa-check"></i><b>0.2</b> Probabilistic reasoning, criterion setting, and self knowledge</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#symmetrical-definition"><i class="fa fa-check"></i>Symmetrical definition:</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#dissymmetrical-definition"><i class="fa fa-check"></i>Dissymmetrical definition:</a></li>
<li class="chapter" data-level="0.2.1" data-path="introduction.html"><a href="introduction.html#second-order-cognition"><i class="fa fa-check"></i><b>0.2.1</b> Second-order cognition</a></li>
<li class="chapter" data-level="0.2.2" data-path="introduction.html"><a href="introduction.html#detectionmodels"><i class="fa fa-check"></i><b>0.2.2</b> Computational models of detection</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#htm"><i class="fa fa-check"></i>The High-Threshold model</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#sdt"><i class="fa fa-check"></i>Signal Detection Theory</a></li>
</ul></li>
<li class="chapter" data-level="0.3" data-path="introduction.html"><a href="introduction.html#detection-i-would-have-noticed-it"><i class="fa fa-check"></i><b>0.3</b> Detection: “I would have noticed it”</a></li>
<li class="chapter" data-level="0.4" data-path="introduction.html"><a href="introduction.html#intro:search"><i class="fa fa-check"></i><b>0.4</b> Visual search: “I would have found it”</a></li>
<li class="chapter" data-level="0.5" data-path="introduction.html"><a href="introduction.html#memory-i-would-have-remembered-it"><i class="fa fa-check"></i><b>0.5</b> Memory: “I would have remembered it”</a></li>
<li class="chapter" data-level="0.6" data-path="introduction.html"><a href="introduction.html#the-development-of-a-self-model"><i class="fa fa-check"></i><b>0.6</b> The development of a self-model</a></li>
<li class="chapter" data-level="0.7" data-path="introduction.html"><a href="introduction.html#this-thesis"><i class="fa fa-check"></i><b>0.7</b> This thesis</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-ch-search.html"><a href="1-ch-search.html"><i class="fa fa-check"></i><b>1</b> Zero-shot search termination reveals a dissociation between implicit and explicit metacognitive knowledge</a><ul>
<li class="chapter" data-level="1.1" data-path="1-ch-search.html"><a href="1-ch-search.html#introduction-1"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="1-ch-search.html"><a href="1-ch-search.html#experiment-1"><i class="fa fa-check"></i><b>1.2</b> Experiment 1</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-ch-search.html"><a href="1-ch-search.html#participants"><i class="fa fa-check"></i><b>1.2.1</b> Participants</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-ch-search.html"><a href="1-ch-search.html#procedure"><i class="fa fa-check"></i><b>1.2.2</b> Procedure</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-ch-search.html"><a href="1-ch-search.html#data-analysis"><i class="fa fa-check"></i><b>1.2.3</b> Data analysis</a></li>
<li class="chapter" data-level="1.2.4" data-path="1-ch-search.html"><a href="1-ch-search.html#results"><i class="fa fa-check"></i><b>1.2.4</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-ch-search.html"><a href="1-ch-search.html#experiment-2"><i class="fa fa-check"></i><b>1.3</b> Experiment 2</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-ch-search.html"><a href="1-ch-search.html#participants-1"><i class="fa fa-check"></i><b>1.3.1</b> Participants</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-ch-search.html"><a href="1-ch-search.html#procedure-1"><i class="fa fa-check"></i><b>1.3.2</b> Procedure</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-ch-search.html"><a href="1-ch-search.html#results-1"><i class="fa fa-check"></i><b>1.3.3</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-ch-search.html"><a href="1-ch-search.html#discussion"><i class="fa fa-check"></i><b>1.4</b> Discussion</a><ul>
<li class="chapter" data-level="1.4.1" data-path="1-ch-search.html"><a href="1-ch-search.html#is-implicit-metacognitive-knowledge-metacognitive"><i class="fa fa-check"></i><b>1.4.1</b> Is implicit metacognitive knowledge metacognitive?</a></li>
<li class="chapter" data-level="1.4.2" data-path="1-ch-search.html"><a href="1-ch-search.html#inference-about-absence-as-a-tool-for-studying-implicit-self-knowledge"><i class="fa fa-check"></i><b>1.4.2</b> Inference about absence as a tool for studying implicit self knowledge</a></li>
<li class="chapter" data-level="1.4.3" data-path="1-ch-search.html"><a href="1-ch-search.html#conclusion"><i class="fa fa-check"></i><b>1.4.3</b> Conclusion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-ch-RC.html"><a href="2-ch-RC.html"><i class="fa fa-check"></i><b>2</b> Paradoxical evidence weightings in confidence judgments for detection and discrimination</a><ul>
<li class="chapter" data-level="2.1" data-path="2-ch-RC.html"><a href="2-ch-RC.html#introduction-2"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="2-ch-RC.html"><a href="2-ch-RC.html#experiment-1-1"><i class="fa fa-check"></i><b>2.2</b> Experiment 1</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-ch-RC.html"><a href="2-ch-RC.html#methods"><i class="fa fa-check"></i><b>2.2.1</b> Methods</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-ch-RC.html"><a href="2-ch-RC.html#analysis"><i class="fa fa-check"></i><b>2.2.2</b> Analysis</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-ch-RC.html"><a href="2-ch-RC.html#results-2"><i class="fa fa-check"></i><b>2.2.3</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-ch-RC.html"><a href="2-ch-RC.html#experiment-2-1"><i class="fa fa-check"></i><b>2.3</b> Experiment 2</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-ch-RC.html"><a href="2-ch-RC.html#methods-1"><i class="fa fa-check"></i><b>2.3.1</b> Methods</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-ch-RC.html"><a href="2-ch-RC.html#results-3"><i class="fa fa-check"></i><b>2.3.2</b> Results</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-ch-RC.html"><a href="2-ch-RC.html#detection-signal-trials-1"><i class="fa fa-check"></i><b>2.3.3</b> Detection signal trials</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-ch-RC.html"><a href="2-ch-RC.html#discussion-1"><i class="fa fa-check"></i><b>2.4</b> Discussion</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-ch-RC.html"><a href="2-ch-RC.html#model-1-a-rational-agent-symmetric-evidence-structure"><i class="fa fa-check"></i><b>2.4.1</b> Model 1: a rational agent + symmetric evidence structure</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-ch-RC.html"><a href="2-ch-RC.html#model-2-a-rational-agent-symmetric-evidence-structure"><i class="fa fa-check"></i><b>2.4.2</b> Model 2: a rational agent + symmetric evidence structure</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-ch-RC.html"><a href="2-ch-RC.html#model-3-confidence-decision-cross"><i class="fa fa-check"></i><b>2.4.3</b> Model 3: confidence decision cross</a></li>
<li class="chapter" data-level="2.4.4" data-path="2-ch-RC.html"><a href="2-ch-RC.html#evidence-for-absence"><i class="fa fa-check"></i><b>2.4.4</b> Evidence for absence</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><i class="fa fa-check"></i><b>3</b> Distinct neural contributions to metacognition for detecting (but not discriminating) visual stimuli</a><ul>
<li class="chapter" data-level="3.1" data-path="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#introduction-3"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#methods-and-materials"><i class="fa fa-check"></i><b>3.2</b> Methods and Materials</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#participants-4"><i class="fa fa-check"></i><b>3.2.1</b> Participants</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#design-and-procedure"><i class="fa fa-check"></i><b>3.2.2</b> Design and procedure</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#scanning-parameters"><i class="fa fa-check"></i><b>3.2.3</b> Scanning parameters</a></li>
<li class="chapter" data-level="3.2.4" data-path="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#analysis-1"><i class="fa fa-check"></i><b>3.2.4</b> Analysis</a></li>
<li class="chapter" data-level="3.2.5" data-path="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#exclusion-criteria"><i class="fa fa-check"></i><b>3.2.5</b> Exclusion criteria</a></li>
<li class="chapter" data-level="3.2.6" data-path="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#response-conditional-type-ii-roc-curves"><i class="fa fa-check"></i><b>3.2.6</b> Response conditional type-II ROC curves</a></li>
<li class="chapter" data-level="3.2.7" data-path="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#imaging-analysis"><i class="fa fa-check"></i><b>3.2.7</b> Imaging analysis</a></li>
<li class="chapter" data-level="3.2.8" data-path="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#statistical-inference-1"><i class="fa fa-check"></i><b>3.2.8</b> Statistical inference</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#results-4"><i class="fa fa-check"></i><b>3.3</b> Results</a></li>
<li class="chapter" data-level="3.4" data-path="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#behavioural-results"><i class="fa fa-check"></i><b>3.4</b> Behavioural results</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#imaging-results"><i class="fa fa-check"></i><b>3.4.1</b> Imaging results</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#computational-models"><i class="fa fa-check"></i><b>3.4.2</b> Computational models</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#discussion-2"><i class="fa fa-check"></i><b>3.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-ch-MVS.html"><a href="4-ch-MVS.html"><i class="fa fa-check"></i><b>4</b> Prospective search time estimates for unseen displays reveal a rich intuitive theory of visual search</a><ul>
<li class="chapter" data-level="4.1" data-path="4-ch-MVS.html"><a href="4-ch-MVS.html#introduction-4"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="4-ch-MVS.html"><a href="4-ch-MVS.html#experiments-1-and-2-shape-orientation-and-color"><i class="fa fa-check"></i><b>4.2</b> Experiments 1 and 2: shape, orientation, and color</a><ul>
<li class="chapter" data-level="4.2.1" data-path="4-ch-MVS.html"><a href="4-ch-MVS.html#participants-5"><i class="fa fa-check"></i><b>4.2.1</b> Participants</a></li>
<li class="chapter" data-level="4.2.2" data-path="4-ch-MVS.html"><a href="4-ch-MVS.html#procedure-2"><i class="fa fa-check"></i><b>4.2.2</b> Procedure</a></li>
<li class="chapter" data-level="4.2.3" data-path="4-ch-MVS.html"><a href="4-ch-MVS.html#results-5"><i class="fa fa-check"></i><b>4.2.3</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-ch-MVS.html"><a href="4-ch-MVS.html#experiments-3-and-4-complex-unfamiliar-stimuli"><i class="fa fa-check"></i><b>4.3</b> Experiments 3 and 4: complex, unfamiliar stimuli</a><ul>
<li class="chapter" data-level="4.3.1" data-path="4-ch-MVS.html"><a href="4-ch-MVS.html#participants-6"><i class="fa fa-check"></i><b>4.3.1</b> Participants</a></li>
<li class="chapter" data-level="4.3.2" data-path="4-ch-MVS.html"><a href="4-ch-MVS.html#procedure-3"><i class="fa fa-check"></i><b>4.3.2</b> Procedure</a></li>
<li class="chapter" data-level="4.3.3" data-path="4-ch-MVS.html"><a href="4-ch-MVS.html#results-6"><i class="fa fa-check"></i><b>4.3.3</b> Results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html"><i class="fa fa-check"></i>General Discussion</a><ul>
<li class="chapter" data-level="4.4" data-path="general-discussion.html"><a href="general-discussion.html#summary-of-results"><i class="fa fa-check"></i><b>4.4</b> Summary of results</a></li>
<li class="chapter" data-level="4.5" data-path="general-discussion.html"><a href="general-discussion.html#what-i-didnt-find"><i class="fa fa-check"></i><b>4.5</b> What I didn’t find</a><ul>
<li class="chapter" data-level="4.5.1" data-path="general-discussion.html"><a href="general-discussion.html#chapter-1-no-correlation-with-explicit-metacognition"><i class="fa fa-check"></i><b>4.5.1</b> Chapter 1: no correlation with explicit metacognition</a></li>
<li class="chapter" data-level="4.5.2" data-path="general-discussion.html"><a href="general-discussion.html#chapter-2-no-effect-of-confidence-in-signal-presence"><i class="fa fa-check"></i><b>4.5.2</b> Chapter 2: no effect of confidence in signal presence</a></li>
<li class="chapter" data-level="4.5.3" data-path="general-discussion.html"><a href="general-discussion.html#chapter-3-small-differences-in-brain-activity-between-inference-about-absence-and-presence"><i class="fa fa-check"></i><b>4.5.3</b> Chapter 3: small differences in brain activity between inference about absence and presence</a></li>
<li class="chapter" data-level="4.5.4" data-path="general-discussion.html"><a href="general-discussion.html#section"><i class="fa fa-check"></i><b>4.5.4</b> </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="general-discussion.html"><a href="general-discussion.html#future-directions"><i class="fa fa-check"></i><b>4.6</b> Future directions</a><ul>
<li class="chapter" data-level="4.6.1" data-path="general-discussion.html"><a href="general-discussion.html#failures-of-a-self-model"><i class="fa fa-check"></i><b>4.6.1</b> Failures of a self-model</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="general-discussion.html"><a href="general-discussion.html#conclusion-1"><i class="fa fa-check"></i><b>4.7</b> Conclusion</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-app1-SDT.html"><a href="A-app1-SDT.html"><i class="fa fa-check"></i><b>A</b> Signal Detection Theory</a><ul>
<li class="chapter" data-level="A.1" data-path="A-app1-SDT.html"><a href="A-app1-SDT.html#app1:ROC"><i class="fa fa-check"></i><b>A.1</b> ROC and zROC curves</a></li>
<li class="chapter" data-level="A.2" data-path="A-app1-SDT.html"><a href="A-app1-SDT.html#app1:uvSDT"><i class="fa fa-check"></i><b>A.2</b> Unequal-variance (uv) SDT</a></li>
<li class="chapter" data-level="A.3" data-path="A-app1-SDT.html"><a href="A-app1-SDT.html#app1:mc"><i class="fa fa-check"></i><b>A.3</b> SDT Measures for Metacognition</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="B-supp-materials-for-ch-2.html"><a href="B-supp-materials-for-ch-2.html"><i class="fa fa-check"></i><b>B</b> Supp. materials for ch. 2</a><ul>
<li class="chapter" data-level="B.1" data-path="B-supp-materials-for-ch-2.html"><a href="B-supp-materials-for-ch-2.html#app2:PDRC"><i class="fa fa-check"></i><b>B.1</b> Pseudo-discrimination analysis</a><ul>
<li class="chapter" data-level="B.1.1" data-path="B-supp-materials-for-ch-2.html"><a href="B-supp-materials-for-ch-2.html#exp.-1"><i class="fa fa-check"></i><b>B.1.1</b> Exp. 1</a></li>
<li class="chapter" data-level="B.1.2" data-path="B-supp-materials-for-ch-2.html"><a href="B-supp-materials-for-ch-2.html#exp.-2"><i class="fa fa-check"></i><b>B.1.2</b> Exp. 2</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="B-supp-materials-for-ch-2.html"><a href="B-supp-materials-for-ch-2.html#app2:simulation"><i class="fa fa-check"></i><b>B.2</b> Unequal-variance model</a><ul>
<li class="chapter" data-level="B.2.1" data-path="B-supp-materials-for-ch-2.html"><a href="B-supp-materials-for-ch-2.html#discrimination"><i class="fa fa-check"></i><b>B.2.1</b> Discrimination</a></li>
<li class="chapter" data-level="B.2.2" data-path="B-supp-materials-for-ch-2.html"><a href="B-supp-materials-for-ch-2.html#detection-1"><i class="fa fa-check"></i><b>B.2.2</b> Detection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html"><i class="fa fa-check"></i><b>C</b> Supp. materials for ch. 3</a><ul>
<li class="chapter" data-level="C.1" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#app3:buttonpresses"><i class="fa fa-check"></i><b>C.1</b> Confidence button presses</a></li>
<li class="chapter" data-level="C.2" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#app3:zROC"><i class="fa fa-check"></i><b>C.2</b> zROC curves</a></li>
<li class="chapter" data-level="C.3" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#app3:GC-DM"><i class="fa fa-check"></i><b>C.3</b> Global confidence design matrix</a></li>
<li class="chapter" data-level="C.4" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#app3:ROIconf"><i class="fa fa-check"></i><b>C.4</b> Effect of confidence in our pre-specified ROIs</a></li>
<li class="chapter" data-level="C.5" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#app3:varianceRat"><i class="fa fa-check"></i><b>C.5</b> SDT variance ratio correlation with the quadratic confidence effect</a></li>
<li class="chapter" data-level="C.6" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#app3:efficiency"><i class="fa fa-check"></i><b>C.6</b> Correlation of metacognitive efficiency with linear and quadratic confidence effects</a></li>
<li class="chapter" data-level="C.7" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#app3:cross"><i class="fa fa-check"></i><b>C.7</b> Confidence-decision cross classification</a></li>
<li class="chapter" data-level="C.8" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#app3:SDT"><i class="fa fa-check"></i><b>C.8</b> Static Signal Detection Theory</a><ul>
<li class="chapter" data-level="C.8.1" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#discrimination-1"><i class="fa fa-check"></i><b>C.8.1</b> Discrimination</a></li>
<li class="chapter" data-level="C.8.2" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#detection-2"><i class="fa fa-check"></i><b>C.8.2</b> Detection</a></li>
</ul></li>
<li class="chapter" data-level="C.9" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#app3:Dynamic"><i class="fa fa-check"></i><b>C.9</b> Dynamic Criterion</a><ul>
<li class="chapter" data-level="C.9.1" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#discrimination-2"><i class="fa fa-check"></i><b>C.9.1</b> Discrimination</a></li>
<li class="chapter" data-level="C.9.2" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#detection-3"><i class="fa fa-check"></i><b>C.9.2</b> Detection</a></li>
</ul></li>
<li class="chapter" data-level="C.10" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#app3:Monitoring"><i class="fa fa-check"></i><b>C.10</b> Attention Monitoring</a><ul>
<li class="chapter" data-level="C.10.1" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#discrimination-3"><i class="fa fa-check"></i><b>C.10.1</b> Discrimination</a></li>
<li class="chapter" data-level="C.10.2" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#detection-4"><i class="fa fa-check"></i><b>C.10.2</b> Detection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Self-Modeling in Inference about Absence</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch:RC" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Paradoxical evidence weightings in confidence judgments for detection and discrimination</h1>
<div id="matan-mazor-lucie-charles-karl-j.-friston-stephen-m.-fleming" class="section level4 unnumbered">
<h4>Matan Mazor, Lucie Charles, Karl J. Friston &amp; Stephen M. Fleming</h4>
<p>In two experiments we asked what sensory evidence is incorporated into decisions and confidence judgments in perceptual decisions about stimulus presence or absence (detection) and stimulus category (discrimination). We successfully replicated the positive evidence bias in discrimination confidence ratings: subjective confidence was boosted more by supporting evidence than it was undermined by conflicting evidence, in line with a detection disposition to the discrimination task. We further find that detection judgments show the same positive evidence bias as discrimination confidence ratings. Paradoxically, confidence ratings in detection present a discrimination-like evidence weighting, with equal weighting of positive and negative evidence. First-order perceptual decision making models fail to account for the entire set of findings.</p>
</div>
<div id="introduction-2" class="section level2">
<h2><span class="header-section-number">2.1</span> Introduction</h2>
<!-- First-year statistics courses teach that “the absence of evidence is not evidence of absence” [@altman1995statistics]. In other words, failing to find a statistically significant effect is not sufficient justification for that an effect does not exist. To make claims about the absence of an effect, one needs to incorporate information about test sensitivity (statistical power), specificity (significance level), and the probability of an effect to exist prior to seeing any data. Similarly, when facing a perceptual detection problem (*is there any signal in the noise?*), failure to detect a signal is not by itself sufficient to conclude that a signal is absent. In this case, information of potential relevance is the sensitivity of one’s senses to external signals, the tendency to hallucinate in the absence of true signal, and the prior probability of signal presence. The same considerations are also relevant when forming subjective confidence in the presence or absence of external signals. -->
<!-- Adult humans have been shown to rationally integrate information about the prior of a signal and the statistical power of the test when forming confidence in the absence of a signal in an abstract decision making task [@hsu2017absence]. However, it remains unknown whether similar computational considerations apply in perceptual settings, where uncertainty about the presence or absence of a signal is affected by both external (signal intensity) and internal (sensory precision) sources, and where these sources are not explicitly signaled. What information is incorporated into confidence ratings in visual detection, and whether this varies between decisions about signal absence or presence, is still an open question. Specifically, while confidence in decisions about target presence can scale with stimulus intensity, it is unclear what stimulus features, if any, will affect confidence in stimulus absence, given that decisions about absence can only be based on the lack of perceptual evidence for stimulus presence.  -->
<p>When considering two alternative hypotheses, the probability of a chosen hypothesis to be correct is not only a function of the likelihood of the observations under the chosen hypothesis, but also of the likelihood of the observations under the unchosen one. For example, when deciding that a random dot display was drifting to the right and not to the left, confidence should not only positively weigh motion energy to the right (<em>positive evidence</em>), but also negatively weigh motion energy to the left (<em>negative evidence</em>). However, in their subjective confidence ratings subjects put unproportional weight on positive evidence, giving rise to a <em>positive evidence bias</em> <span class="citation">(Koizumi, Maniscalco, &amp; Lau, 2015; Sepulveda et al., 2020; Zylberberg, Barttfeld, &amp; Sigman, 2012)</span>. Put differently, confidence ratings in discrimination are sensitive not only to the <em>relative evidence</em> of the chosen hypothesis compared with the unchosen one, but also to the <em>sum evidence</em> for the two hypotheses <span class="citation">(also termed <em>visibility</em>; Rausch, Hellmann, &amp; Zehetleitner, 2018)</span>.</p>
<p>Focusing on sum rather than relative evidence is rational if subjects are rating their confidence not in the identity of the stimulus, but in the presence or absence of a signal. For example, when judging the direction of motion in a random dot kinematogram, if motion energy is high both to the left and to the right, confidence in the direction of motion should be low (low relative evidence), but confidence in the presence of coherent motion, regardless of its direction, should be high (high sum evidence). A positive evidence bias in discrimination judgments may indicate that participants are rating their confidence not in the accuracy of their choice, but in the presence of a signal.</p>
<p>This implied link between metacognitive evaluation and detection (judgments about the presence or absence of a signal) has led us to examine the contribution of perceptual evidence to decision and confidence in perceptual detection tasks. We were interested in three questions: first, when faced with a detection task where targets are drawn from two stimulus classes, would detection decision be sensitive to sum evidence (like in discrimination confidence), or to the relative evidence for presence for one category over the other? Second: would confidence in the presence of a target stimulus be susceptible to the same positive evidence bias as confidence in stimulus type? And finally, when making decisions about the absence of a signal, would confidence ratings be sensitive to some form of positive evidence for absence, or be entirely independent of sensory evidence?</p>
<!-- Some decisions however are asymmetric by design, not just in the cognitive or metacognitive processing of incoming evidence. For example, when deciding if a target object is present or absent in a display, evidence can only be available to support presence, and inference about absence is based on a failure to accumulate evidence for presence, not on the accumulation of evidence for absence.  -->
<p>In two experiments participants performed discrimination and detection decisions on noisy stimuli, and rated their confidence in their decisions. Using reverse correlation analysis we measured the influence of random fluctuations in stimulus energy on their responses and confidence ratings, as well as markers of a processing asymmetry between detection ‘yes’ and ‘no’ responses (response time, general confidence, and metacognitive sensitivity). To anticipate our results, we fully replicated previous findings of a positive evidence bias in discrimination responses <span class="citation">(Zylberberg et al., 2012)</span>. Paradoxically, although detection decisions were sensitive to sum evidence as expected, we found no positive evidence bias in confidence judgments following detection ‘yes’ responses. In Experiment 2, where reverse correlation revealed an accumulation of positive evidence for stimulus absence, we find no metacognitive sensitivity between the two detection responses. We discuss our findings as drawing a link between discrimination confidence ratings and detection responses, but not detection confidence ratings.</p>
</div>
<div id="experiment-1-1" class="section level2">
<h2><span class="header-section-number">2.2</span> Experiment 1</h2>
<div id="methods" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Methods</h3>
<div id="participants-2" class="section level4">
<h4><span class="header-section-number">2.2.1.1</span> Participants</h4>
<p>The research complied with all relevant ethical regulations, and was approved by the Research Ethics Committee of University College London (study ID number 1260/003). 10 participants were recruited via the UCL subject recruiting system, and gave their informed consent prior to their participation. Each participant performed four sessions of 600 trials each, in blocks of 100 trials. Sessions took place on different days and consisted of 3 discrimination blocks interleaved with 3 detection blocks.</p>
</div>
<div id="experimental-procedure" class="section level4">
<h4><span class="header-section-number">2.2.1.2</span> Experimental procedure</h4>
<p>The experimental procedure for Experiment 1 largely followed the procedure described in <span class="citation">Zylberberg et al. (2012)</span>, Experiment 1. Participants observed a random-dot kinematogram for a fixed duration of 700 ms. In discrimination trials, the direction of motion was one of two opposite directions with equal probability, and participants reported the observed direction by pressing one of two arrow keys on a standard keyboard. In detection blocks participants reported whether there was coherent motion by pressing one of two arrow keys on a standard keyboard. In half of the detection trials dots moved coherently to one of two opposite directions, and in the other half they moved randomly.</p>
<p>In both detection and discrimination blocks, following a decision participants indicated their confidence in their decision. Confidence was reported on a continuous scale ranging from chance to complete certainty. To avoid response bias in confidence reports, the orientation (vertical or horizontal) and polarity (e.g., right or left) of the scale was set to agree with the type 1 response. For example, following a down arrow press, a vertical confidence bar was presented where ‘guess’ is at the center of the screen and ‘certain’ appeared at the lower end of the scale (see Fig. <a href="2-ch-RC.html#fig:ch2-exp1-design">2.1</a>).</p>
<p>To control for response requirements, for 5 subjects the dots moved to the right or to the left, and for the 5 other subjects they moved upward or downward. The first group made discrimination judgments with the right and left keys and detection judgments with the up and down keys, and this mapping was reversed for the second group. The number of coherently moving dots (“motion coherence”) was adjusted to maintain performance at around 70% accuracy for detection and discrimination tasks independently. This was achieved by measuring mean accuracy once in every 20 trials, and adjusting coherence by a step of 3% if accuracy fell below 60% or went above 80%.</p>
<p>Stimuli for discrimination blocks were generated using the exact same procedure reported in <span class="citation">Zylberberg et al. (2012)</span><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. Trials started with a presentation of a fixation cross for one second, immediately followed by stimulus presentation. The stimulus consisted of 152 white dots (diameter = 0.14°), presented within a 6.5° circular aperture centered on the fixation point for 700 milliseconds (42 frames, frame rate = 60 HZ). Dots were grouped in two patches of equal sizes of 56 dots each. Every other frame, the dots of one patch were replaced with a new set of randomly positioned dots. For a coherence value of <span class="math inline">\(c’\)</span>, a proportion of <span class="math inline">\(c’\)</span> of the dots from the second patch moved coherently in one direction by a fixed distance of 0.33°, while the remaining dots in the patch moved in random directions by a fixed distance of 0.33°. On the next update, the patches were switched, to prevent participants from tracing the position of specific dots. Frame-specific coherence values were sampled for each screen update from a normal distribution centred around the coherence value <span class="math inline">\(c\)</span> with a standard deviation of 0.07, with the constraint that <span class="math inline">\(c&#39;\)</span> must be a number between 0 and 1.</p>
<p>Stimuli for detection blocks were generated using a similar procedure, with the only difference being that on a random half of the trials coherence was set to 0%, without random sampling of coherence values for different frames (see Fig. 1).</p>
<div class="figure" style="text-align: center"><span id="fig:ch2-exp1-design"></span>
<img src="figure/ch2/designExp1.png" alt="Task design for Experiment 1. In both tasks, participants viewed 700 milliseconds of a random dot motion array, after which they made a keyboard response to indicate their decision (motion direction in discrimination, signal absence or presence in detection), followed by a continuous confidence report using the mouse. 5 participants viewed vertically moving dots and indicated their detection responses on a horizontal scale, and 5 participants viewed horizontally moving dots and indicated their detection responses on a vertical scale. " width="\textwidth" />
<p class="caption">
Figure 2.1: Task design for Experiment 1. In both tasks, participants viewed 700 milliseconds of a random dot motion array, after which they made a keyboard response to indicate their decision (motion direction in discrimination, signal absence or presence in detection), followed by a continuous confidence report using the mouse. 5 participants viewed vertically moving dots and indicated their detection responses on a horizontal scale, and 5 participants viewed horizontally moving dots and indicated their detection responses on a vertical scale.
</p>
</div>
</div>
</div>
<div id="analysis" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Analysis</h3>
<!-- Our pre-registered objectives for this study were to: -->
<!-- 1. Replicate the finding that metacognitive sensitivity for ‘no’ responses is lower than for ‘yes’ responses in detection [@meuwese2014subjective; @kanai2010subjective; @kellij2018foundations], and generalize these findings to a different task (determining whether or not some dots moved coherently in a random dot kinematogram; RDK). -->
<!-- 2. Estimate the goodness of fit of an unequal-variance SDT model to perceptual detection data, and compare it to the fit of models that assume a qualitative difference between confidence in absence and confidence in presence. -->
<!-- 3. Replicate the results of @zylberberg2012construction for perceptual discrimination. Namely, show that confidence in a motion discrimination task is mostly influenced by evidence for the selected direction within a short time window around 300 milliseconds after stimulus onset.  -->
<!-- 4. Test the generality of the results of @zylberberg2012construction to perceptual detection, where, by definition, no evidence can be collected to support a ‘no’ decision. Examine the contribution of signal variance. -->
<div id="reverse-correlation-analysis" class="section level4 unnumbered">
<h4>Reverse correlation analysis</h4>
<p>For the reverse correlation analysis, we followed a procedure similar to the one described in <span class="citation">Zylberberg et al. (2012)</span>. For each of the four directions (right, left, up and down), we applied two spatiotemporal filters to the frames of the dot motion stimuli as described in previous studies <span class="citation">(Adelson &amp; Bergen, 1985; Zylberberg et al., 2012)</span>. The outputs of the two filters were squared and summed, resulting in a three-dimensional matrix with motion energy in the specific direction as a function of x, y, and time. We then took the mean of this matrix across the x and y dimensions to obtain an estimate of the overall temporal fluctuations in motion energy in the selected direction. Additionally, for every time point we extracted the variance along the x and y dimensions, to obtain a measure of temporal fluctuations in spatial variance. Using this filter, we obtained trial-wise estimates of temporal fluctuations in the mean and variance of motion energy for upward, downward, leftward and rightward motion. Given a high correlation between our mean and variance estimates, we focused our analysis on the mean motion energy.</p>
<p>In order to distill random fluctuations in motion energy from mean differences between stimulus categories, we subtracted the mean motion energy from trial-specific motion energy vectors. The mean motion energy vectors were extracted at the group level, separately for each motion coherence level and as a function of motion direction. We chose this approach instead of the linear regression approach used by <span class="citation">Zylberberg et al. (2012)</span> in order to control for nonlinear effects of coherence on motion energy.</p>
</div>
<div id="statistical-inference" class="section level4 unnumbered">
<h4>Statistical inference</h4>
<p>Statistics were extracted separately for each participant, and group-level inference was then performed on the first-order statistics. T-test Bayes factors were used to quantify the evidence for the null when appropriate, using a Jeffrey-Zellner-Siow Prior for the null distribution, with a unit prior scale <span class="citation">(Rouder, Speckman, Sun, Morey, &amp; Iverson, 2009)</span>.</p>
</div>
</div>
<div id="results-2" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Results</h3>
<div id="response-accuracy" class="section level4">
<h4><span class="header-section-number">2.2.3.1</span> Response accuracy</h4>
<p>Overall accuracy level was 0.74 in the discrimination and 0.72 in the detection task. Performance for discrimination was significantly higher than for detection (<span class="math inline">\(M_d = 0.02\)</span>, 95% CI <span class="math inline">\([0.00\)</span>, <span class="math inline">\(0.04]\)</span>, <span class="math inline">\(t(9) = 2.43\)</span>, <span class="math inline">\(p = .038\)</span>). This difference in task performance reflected a slower convergence of the staircasing procedure for the discrimination task during the first session. When discarding all data from the first session and analyzing only data from the last three sessions (1800 trials per participant), task performance was equated between the two tasks at the group level (<span class="math inline">\(M_d = 0.00\)</span>, 95% CI <span class="math inline">\([-0.02\)</span>, <span class="math inline">\(0.02]\)</span>, <span class="math inline">\(t(9) = -0.05\)</span>, <span class="math inline">\(p = .962\)</span>; <span class="math inline">\(\mathrm{BF}_{\textrm{01}} = 3.24\)</span>). In order to avoid conflating true differences between discrimination and detection with more general difficulty effects, the first session was excluded from all subsequent analyses.</p>
</div>
<div id="overall-properties-of-response-and-confidence-distributions" class="section level4">
<h4><span class="header-section-number">2.2.3.2</span> Overall properties of response and confidence distributions</h4>
<p>In detection, participants were more likely to respond ‘yes’ than ‘no’ (mean proportion of ‘yes’ responses: <span class="math inline">\(M = 0.59\)</span>, 95% CI <span class="math inline">\([0.53\)</span>, <span class="math inline">\(0.64]\)</span>, <span class="math inline">\(t(9) = 3.45\)</span>, <span class="math inline">\(p = .007\)</span>). We did not observe a consistent response bias for the discrimination data (mean proportion of ‘rightward’ or ‘upward’ responses: <span class="math inline">\(M = 0.52\)</span>, 95% CI <span class="math inline">\([0.47\)</span>, <span class="math inline">\(0.57]\)</span>, <span class="math inline">\(t(9) = 1.00\)</span>, <span class="math inline">\(p = .344\)</span>).</p>
<p>In detection, participants were generally slower to deliver ‘no’ responses compared to ‘yes’ responses (median difference: 85.37 ms, <span class="math inline">\(t(9) = -3.46\)</span>, <span class="math inline">\(p = .007\)</span> for a t-test on the log-transformed response times; see Fig. <a href="2-ch-RC.html#fig:ch2-exp1-hists">2.2</a>, upper panel). No significant difference in response times was observed for the discrimination task (median difference: 6.16 ms, <span class="math inline">\(t(9) = -0.43\)</span>, <span class="math inline">\(p = .676\)</span>).</p>
<p>Confidence in detection was generally higher than in discrimination (<span class="math inline">\(M_d = 0.06\)</span>, 95% CI <span class="math inline">\([0.01\)</span>, <span class="math inline">\(0.12]\)</span>, <span class="math inline">\(t(9) = 2.49\)</span>, <span class="math inline">\(p = .035\)</span>; see Fig. <a href="2-ch-RC.html#fig:ch2-exp1-hists">2.2</a>, lower panel). Within detection, confidence in ‘yes’ responses was generally higher than confidence in ‘no’ responses (<span class="math inline">\(M = 0.08\)</span>, 95% CI <span class="math inline">\([0.03\)</span>, <span class="math inline">\(0.13]\)</span>, <span class="math inline">\(t(9) = 3.49\)</span>, <span class="math inline">\(p = .007\)</span>). No difference in average confidence levels was found between the two discrimination responses (<span class="math inline">\(M = 0.02\)</span>, 95% CI <span class="math inline">\([-0.03\)</span>, <span class="math inline">\(0.06]\)</span>, <span class="math inline">\(t(9) = 0.91\)</span>, <span class="math inline">\(p = .384\)</span>).</p>
<div class="figure" style="text-align: center"><span id="fig:ch2-exp1-hists"></span>
<img src="thesis_files/figure-html/ch2-exp1-hists-1.png" alt="Response time (upper panel) and confidence (lower panel) histograms for the detection (left) and discrimination (right) tasks in Experiment 1. Vertical lines represent the median response time and the mean confidence rating for each response." width="672" />
<p class="caption">
Figure 2.2: Response time (upper panel) and confidence (lower panel) histograms for the detection (left) and discrimination (right) tasks in Experiment 1. Vertical lines represent the median response time and the mean confidence rating for each response.
</p>
</div>
</div>
<div id="response-conditional-roc-curves" class="section level4">
<h4><span class="header-section-number">2.2.3.3</span> Response conditional ROC curves</h4>
<p>Following <span class="citation">Meuwese et al. (2014)</span>, we extracted response-conditional type-2 ROC (rc-ROC) curves for the two tasks. Unlike traditional type-I ROC curves that provide a visual representation of subjects’ ability to distinguish between two external world states, type 2 ROC curves represent their ability to track the accuracy of their own responses. The area under the response-conditional ROC curve (auROC2) is a measure of metacognitive sensitivity, with higher values corresponding to more accurate metacognitive monitoring.</p>
<p>Mean response-conditional ROC curves for the two responses in the discrimination task closely matched (<span class="math inline">\(M = 0.00\)</span>, 95% CI <span class="math inline">\([-0.05\)</span>, <span class="math inline">\(0.05]\)</span>, <span class="math inline">\(t(9) = 0.13\)</span>, <span class="math inline">\(p = .900\)</span>), indicating that on average, participants had similar metacognitive insight into the accuracy of the two discrimination responses. In contrast, auROC2 estimates for ‘yes’ responses were significantly higher than for ‘no’ responses, indicating a metacognitive asymmetry between the two detection responses (group difference in auROC2: <span class="math inline">\(M = 0.11\)</span>, 95% CI <span class="math inline">\([0.03\)</span>, <span class="math inline">\(0.18]\)</span>, <span class="math inline">\(t(9) = 3.28\)</span>, <span class="math inline">\(p = .010\)</span>).</p>
<!-- This effect was still significant after subtracting the mean confidence for each experimental block and response from the ratings (t(9)=4.39, p=0.001), suggesting that this pattern originates from trial-specific differences in confidence levels, and not from more global confidence level adjustments occurring at the block level (see figure S1).  -->
<p>To better understand the origin of this difference between ‘yes’ and ‘no’ curves, we compared the detection auROC2 values with the average discrimination auROC2. We found both a significant increase in auROC2 for ‘yes’ responses (<span class="math inline">\(M = 0.06\)</span>, 95% CI <span class="math inline">\([0.01\)</span>, <span class="math inline">\(0.11]\)</span>, <span class="math inline">\(t(9) = 2.80\)</span>, <span class="math inline">\(p = .021\)</span>) and a marginally significant decrease in auROC2 for ‘no’ responses relative to discrimination (<span class="math inline">\(M = -0.05\)</span>, 95% CI <span class="math inline">\([-0.10\)</span>, <span class="math inline">\(0.00]\)</span>, <span class="math inline">\(t(9) = -2.16\)</span>, <span class="math inline">\(p = .059\)</span>). In other words, relative to our discrimination benchmark, metacognitive asymmetry in detection was driven by improved metacognitive insight into the accuracy of ‘yes’ responses, and degraded metacognitive insight into the accuracy of ‘no’ responses.</p>
<div class="figure" style="text-align: center"><span id="fig:ch2-exp1-rcROC"></span>
<img src="thesis_files/figure-html/ch2-exp1-rcROC-1.png" alt="Response conditional ROC curves for the two tasks and four responses in Exp. 1. The area under the curve is a measure of metacognitive sensitivity, and the difference in areas between the two responses a measure of metacognitive asymmetry. Lower panel: distributions of the area under the curve for the four responses, across participants. Error bars stand for the standard error of the mean." width="672" />
<p class="caption">
Figure 2.3: Response conditional ROC curves for the two tasks and four responses in Exp. 1. The area under the curve is a measure of metacognitive sensitivity, and the difference in areas between the two responses a measure of metacognitive asymmetry. Lower panel: distributions of the area under the curve for the four responses, across participants. Error bars stand for the standard error of the mean.
</p>
</div>
<p>A difference in response-conditional auROC estimates can emerge from higher-order differences in metacognitive monitoring for the two responses or from lower-level differences in the perceptual representations of signal and noise <span class="citation">(such as in first-order signal detection models where the signal variance is higher; Maniscalco &amp; Lau, 2014)</span>. Importantly, a difference can also emerge in first-order signal-detection models that assume equal variance, in the presence of a response bias or insufficient variance in confidence ratings. To test if the metacognitive asymmetry between ‘yes’ and ‘no’ responses could be accounted for a by an equal-variance SDT model, we simulated data that was identical to our empirical data except for confidence ratings in correct responses, which were chosen to perfectly agree with the assumptions of an equal-variance SDT model given participants’ decision criterion, sensitivity, and their confidence in incorrect responses. We then compared subject-wise differences between the response-conditional auROCs with the differences in this simulated dataset <span class="citation">(Mazor, Moran, &amp; Fleming, 2021)</span>. The difference in differences was significant, indicating that the observed metacognitive asymmetry could not be accounted for by a first-order equal-variance SDT model (<span class="math inline">\(M = 0.08\)</span>, 95% CI <span class="math inline">\([0.02\)</span>, <span class="math inline">\(0.14]\)</span>, <span class="math inline">\(t(9) = 2.96\)</span>, <span class="math inline">\(p = .016\)</span>).</p>
<!-- #### zROC curves -->
<!-- An asymmetry in metacognitive sensitivity for ‘yes’ and ‘no’ responses is predicted by unequal-variance Signal Detection Theory (*uvSDT*). Specifically, if the ‘signal’ distribution is wider than the ‘noise’ distribution, the overlap between the distributions will be more pronounced for misses and correct rejections than for hits and false alarms, making metacognitive judgments for ‘no’ responses objectively more difficult. Unequal-variance SDT predicts that plotting the type-1 ROC curve in z-space (taking the inverse cumulative distribution of the confidence rating histogram) will result in a straight line with a slope equal to $\frac{\sigma_{noise}}{\sigma_{signal}}$. Because the variance of the signal distribution is higher than that of the noise distribution, zROC slopes are typically shallow, with slopes below 1. -->
<!-- We used linear regression to estimate the slope of the zROC curve. To control for underestimation of the slope due to regression to the mean [@wickens2002elementary, 56], we fitted two regression models for the task data of each participant: one predicting $Z(h)$ based on $Z(f)$ (slope $s_1$) and one predicting $Z(f)$ based on $Z(h)$ (slope $s_2$). We then used $\frac{log(s_1)-log(s2)}{2}$ as a bias-free measure of the zROC slope. In equal-variance SDT, this value is predicted to be 0, corresponding to a slope of 1. -->
<!-- Indeed, slopes were generally shallow for detection zROC curves (as predicted by an unequal-variance SDT model; $M = -0.15$, 95\% CI $[-0.27$, $-0.04]$, $t(9) = -2.95$, $p = .016$), and not significantly different from 1 for discrimination zROC curves (as predicted by equal-variance SDT; $M = 0.00$, 95\% CI $[-0.09$, $0.10]$, $t(9) = 0.07$, $p = .946$).  -->
<!-- These results support a difference in the variance-structure of the representation of signal and noise, such that the representation of signal is more varied across trials. However, it is still possible that some of the metacognitive asymmetry in detection (the difference in auROC between 'yes' and 'no' responses) reflects additional higher-order processes that cannot be captured by a first-order signal-detection model. If this was the case, zROC curves for detection should not only be more shallow, but also less linear than for discrimination, reflecting poorer fit of the signal-detection model to detection. In order to test if this was the case, we compared the subject-wise $R^2$ values for the detection and discrimination zROC regression lines. $R^2$ values reflect the goodness of fit of a linear model to the data. These values were similar for the two tasks ($M_d = -0.01$, 95\% CI $[-0.03$, $0.01]$, $t(9) = -0.91$, $p = .385$), suggesting that a first-order SDT model accounted equally well for the two tasks. -->
</div>
<div id="reverse-correlation" class="section level4">
<h4><span class="header-section-number">2.2.3.4</span> Reverse Correlation</h4>
<p>Random fluctuations in motion energy made it possible to apply reverse correlation and test which stimulus features are incorporated into decisions and confidence ratings in detection and discrimination. Following <span class="citation">Zylberberg et al. (2012)</span>, our analysis focused on the first 300 milliseconds since stimulus onset.</p>
<div id="e2-disc-RC" class="section level5 unnumbered">
<h5>Discrimination</h5>
<div class="figure" style="text-align: center"><span id="fig:ch2-exp1-discrimination-RC"></span>
<img src="thesis_files/figure-html/ch2-exp1-discrimination-RC-1.png" alt="Decision and confidence discrimination kernels, Experiment 1. Upper left: motion energy in the chosen (green) and unchosen (purple) direction as a function of time. Lower left: a subtraction between energy in the chosen and unchosen directions. Upper right: confidence effects for motion energy in the chosen (green) and unchosen (purple) directions. Lower right: a subtraction between confidence effects in the chosen and unchosen directions. Shaded areas represent the the mean +- one standard error. The first 300 milliseconds of the trial are marked in yellow" width="672" />
<p class="caption">
Figure 2.4: Decision and confidence discrimination kernels, Experiment 1. Upper left: motion energy in the chosen (green) and unchosen (purple) direction as a function of time. Lower left: a subtraction between energy in the chosen and unchosen directions. Upper right: confidence effects for motion energy in the chosen (green) and unchosen (purple) directions. Lower right: a subtraction between confidence effects in the chosen and unchosen directions. Shaded areas represent the the mean +- one standard error. The first 300 milliseconds of the trial are marked in yellow
</p>
</div>
<p>Reverse correlation analysis quantified the effect of random fluctuations in motion energy on the probability of responding ‘right’ and ‘left’ (or ‘up’ or ‘down’), and the temporal dynamics of decision formation. Similar to the results obtained by Zylberberg et. al., participants’ decisions were sensitive to motion energy fluctuations during the first 300 milliseconds of the trial (<span class="math inline">\(t(9) = 7.73\)</span>, <span class="math inline">\(p &lt; .001\)</span>; see Fig. <a href="2-ch-RC.html#fig:ch2-exp1-discrimination-RC">2.4</a>, left panels). We note that the symmetry of the two time courses around the x axis does not by itself entail an equal contribution of negative and positive evidence to the final decision, because negative and positive evidence are defined based on participants’ decision, making it impossible to test their contribution to decisions without engaging in circular inference. Instead, we tested the contribution of motion energy in the true and opposite directions (defined with respect to the stimulus, not the subject’s decision) to discrimination decision. Fluctuations in motion energy in both directions contributed significantly to discrimination decision (<span class="math inline">\(t(9) = 8.38\)</span>, <span class="math inline">\(p &lt; .001\)</span>), with no significant difference between them (<span class="math inline">\(t(9) = -0.65\)</span>, <span class="math inline">\(p = .529\)</span>). To conclude, in agreement with the interpretation of <span class="citation">Zylberberg et al. (2012)</span>, we observed no positive evidence bias in discrimination responses, even when positive and negative evidence were defined with respect to the stimulus itself.</p>
<p>We then turned to the contribution of motion energy to subjective confidence ratings. The median confidence rating in each experimental session was used to separate all motion energy vectors into four groups, according to decision (chosen or unchosen directions) and confidence level (high or low). Confidence kernels for the chosen and unchosen directions were then extracted by subtracting the mean low confidence vectors from the mean high confidence vectors for both the chosen and unchosen directions. We observed a significant effect of motion energy on confidence within this time window (<span class="math inline">\(t(19) = 2.52\)</span>, <span class="math inline">\(p = .021\)</span>; see Fig. <a href="2-ch-RC.html#fig:ch2-exp1-discrimination-RC">2.4</a>, right panels). This effect was significantly stronger for motion energy in the chosen direction, compared to the unchosen direction (<span class="math inline">\(t(9) = 2.81\)</span>, <span class="math inline">\(p = .020\)</span>). In other words, confidence ratings in the discrimination task were more sensitive to positive evidence than to negative evidence. This is again a successful direct replication of the Positive Evidence Bias observed in <span class="citation">Zylberberg et al. (2012)</span>.</p>
</div>
<div id="detection" class="section level5 unnumbered">
<h5>Detection</h5>
<p>We next turned to the effects of motion energy on detection responses and confidence ratings. Reverse correlation for detection introduces a challenge: while ‘no’ responses reflect a belief in the absence of any coherent motion, ‘yes’ responses can result from three different belief states: participants can detect motion in any of the two directions, or in both. We chose to have two possible motion directions in the detection task in order to prevent participants from making ‘no’ responses based on significant motion in an unexpected direction. While this choice ensured that participants cannot trivially accumulate evidence for absence, it also made the reverse correlation analysis more difficult, as we did not have full access to participants’ beliefs about the stimulus in their ‘yes’ responses.</p>
<p>As a first approximation, we tested whether sum motion energy along the relevant dimension (horizontal or vertical), regardless of direction (up/down or left/right), affected the probability of a ‘yes’ response. Sum motion energy did not have a significant effect on participants’ responses during the first 300 milliseconds (<span class="math inline">\(t(9) = 1.23\)</span>, <span class="math inline">\(p = .249\)</span>; see Fig. <a href="2-ch-RC.html#fig:ch2-exp1-detection-RC-figure">2.5</a>, left panel) or at any other time point. The effect of sum motion energy during the first 300 milliseconds on decision confidence was marginally significant (<span class="math inline">\(t(9) = 2.15\)</span>, <span class="math inline">\(p = .060\)</span>; see Fig. <a href="2-ch-RC.html#fig:ch2-exp1-detection-RC-figure">2.5</a>, right panel). Response-specific effects of sum motion energy on decision confidence were not significant for both responses.</p>
<div class="figure" style="text-align: center"><span id="fig:ch2-exp1-detection-RC-figure"></span>
<img src="thesis_files/figure-html/ch2-exp1-detection-RC-figure-1.png" alt="Decision and confidence detection kernels, Experiment 1. Upper left: sum motion energy along the relevant dimension in 'yes' (blue) and 'no' (red) responses as a function of time. Lower left: a subtraction between energy in 'yes' and 'no' responses. Upper right: confidence effects for motion energy in 'yes' and 'no' responses. Lower right: a subtraction between confidence effects 'yes' and 'no' responses. Shaded areas represent the the mean +- one standard error. The first 300 milliseconds of the trial are marked in yellow" width="672" />
<p class="caption">
Figure 2.5: Decision and confidence detection kernels, Experiment 1. Upper left: sum motion energy along the relevant dimension in ‘yes’ (blue) and ‘no’ (red) responses as a function of time. Lower left: a subtraction between energy in ‘yes’ and ‘no’ responses. Upper right: confidence effects for motion energy in ‘yes’ and ‘no’ responses. Lower right: a subtraction between confidence effects ‘yes’ and ‘no’ responses. Shaded areas represent the the mean +- one standard error. The first 300 milliseconds of the trial are marked in yellow
</p>
</div>
</div>
</div>
<div id="detection-signal-trials" class="section level4 unnumbered">
<h4>Detection signal trials</h4>
<p>A failure to find significant effects of sum motion energy on detection decision and confidence may be due to the fact that participants were sensitive to relative evidence (e.g., ‘more dots are moving to the right’) rather than to the sum motion along the relevant axis. However, as we mention above, for any single trial, we cannot tell whether a ‘yes’ response means ‘I perceived coherent motion to the right’ or ‘I perceived coherent motion to the left’. As a way to approximate participants’ perception, we focused on detection signal trials. In these trials, a ‘yes’ response is most likely to reflect the detection of the true direction of motion. We therefore asked whether fluctuations in the true and opposite directions of motion contributed to detection decision and confidence. This was done by subtracting the motion energy vectors for ‘yes’ and ‘no’ responses in the true and opposite motion directions.</p>
<p>Like discrimination decisions, detection decisions were most sensitive to perceptual evidence in the first 300 milliseconds of the trial (see Fig. <a href="2-ch-RC.html#fig:ch2-exp1-signal-RC-figure">2.6</a>, left panels). However, in contrast to discrimination, a positive evidence bias effect in detection was apparent in the decision itself: when deciding whether a stimulus contained coherent motion, participants were more sensitive to fluctuations in motion energy that strengthened the true direction of motion, in comparison to fluctuations that weakened motion in the opposite direction (<span class="math inline">\(t(9) = 2.31\)</span>, <span class="math inline">\(p = .046\)</span>).</p>
<!-- We note again that the apparent symmetry in the discrimination decision kernel is an artefact of the analysis method. In the discrimination analysis, 'chosen' and 'unchosen' vectors mirror each other in the limit, because they must sum to zero. This is not true for this analysis, where we subtract motion energy for 'yes' and 'no' responses within each of the two vectors.  -->
<p>Motion fluctuations in the first 300 milliseconds of the trial also contributed to confidence in detection ‘yes’ responses (contrasting high and low confidence hit trials; <span class="math inline">\(t(9) = 6.13\)</span>, <span class="math inline">\(p &lt; .001\)</span>). But unlike in the discrimination task here we found no evidence for a positive evidence bias in confidence ratings (<span class="math inline">\(t(9) = 0.11\)</span>, <span class="math inline">\(p = .913\)</span>). To reiterate, while detection decisions were mostly sensitive to facilitating fluctuations in motion energy, confidence in detection ‘yes’ responses was equally sensitive to facilitating fluctuations in the true direction of motion, and to interfering fluctuations in the opposite direction of motion.
Confidence in ‘miss’ trials was independent of motion energy (<span class="math inline">\(t(9) = 0.16\)</span>, <span class="math inline">\(p = .874\)</span>). This was true for motion energy in the true direction of motion (<span class="math inline">\(t(9) = 0.12\)</span>, <span class="math inline">\(p = .908\)</span>) as well as for motion energy in the opposite direction (<span class="math inline">\(t(9) = -0.08\)</span>, <span class="math inline">\(p = .941\)</span>).</p>
<div class="figure" style="text-align: center"><span id="fig:ch2-exp1-signal-RC-figure"></span>
<img src="thesis_files/figure-html/ch2-exp1-signal-RC-figure-1.png" alt="Decision and confidence detection kernels in signal trials, Experiment 1. Upper left: difference in motion energy between 'yes' and 'no' responses in the true (blue) and opposite (red) directions as a function of time. Upper middle and right: confidence effects for motion energy in the true and opposite directions for 'yes' and 'no' responses, respectively. Lower panels: the substraction of decision and confidence kernels for the true and opposite directions. Shaded areas represent the the mean +- one standard error. The first 300 milliseconds of the trial are marked in yellow" width="672" />
<p class="caption">
Figure 2.6: Decision and confidence detection kernels in signal trials, Experiment 1. Upper left: difference in motion energy between ‘yes’ and ‘no’ responses in the true (blue) and opposite (red) directions as a function of time. Upper middle and right: confidence effects for motion energy in the true and opposite directions for ‘yes’ and ‘no’ responses, respectively. Lower panels: the substraction of decision and confidence kernels for the true and opposite directions. Shaded areas represent the the mean +- one standard error. The first 300 milliseconds of the trial are marked in yellow
</p>
</div>
<!-- #### Perceptual sample analysis {-} -->
<!-- According to  -->
</div>
</div>
</div>
<div id="experiment-2-1" class="section level2">
<h2><span class="header-section-number">2.3</span> Experiment 2</h2>
<p>In Exp. 1, we found that detection ‘yes’ responses are faster and are accompanied by higher subjective confidence than detection ‘no’ responses. We also replicated the metacognitive asymmetry between detection ‘yes’ and ‘no’ responses as measured with response-conditional ROC curves.</p>
<!-- A first-order signal detection model provided good fits to detection and discrimination responses alike.  -->
<p>Examining random fluctuations in motion energy, we replicated the positive evidence bias in discrimination confidence, such that evidence in support of a decision was given more weight in the construction of confidence than evidence against it. This is consistent with the proposal that participants adopt a detection disposition when rating their confidence in discrimination responses. In detection, decision and confidence were sensitive to fluctuations in motion energy at around the same time window as in discrimination. However, unlike discrimination, in detection a positive evidence bias was apparent in the decision, but not in the confidence kernels. Equal weighting of positive and negative evidence suggests that participants were rating their confidence not in the presence of a signal, but in its category. Furthermore, confidence in detection ‘no’ responses was not affected by fluctuations in motion energy.</p>
<p>In Experiment 2 we tested the robustness of these findings to a different type of stimuli (flickering patches) and mode of data collection (a ~10 minute online experiment). Specifically, our pre-registered objectives (see our pre-registration document: <a href="https://osf.io/8u7dk/">https://osf.io/8u7dk/</a>) were to first, replicate the positive evidence bias in discrimination, second, replicate the absence of a positive evidence bias in detection confidence ratings, and third, replicate the absence of an effect for positive or negative evidence on confidence in ‘no’ judgments.</p>
<div id="methods-1" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Methods</h3>
<div id="participants-3" class="section level4">
<h4><span class="header-section-number">2.3.1.1</span> Participants</h4>
<p>The research complied with all relevant ethical regulations, and was approved by the Research Ethics Committee of University College London (study ID number 1260/003). 147 participants were recruited via Prolific, and gave their informed consent prior to their participation. They were selected based on their acceptance rate (&gt;95%) and for being native English speakers. Following our pre-registration, we aimed to collect data until we had reached 100 included participants based on our pre-specified inclusion criteria (see <a href="https://osf.io/8u7dk/">https://osf.io/8u7dk/</a>). Our final data set includes observations from 102 included participants. The entire experiment took around 10 minutes to complete. Participants were paid £1.25 for their participation, equivalent to an hourly wage of £7.5.</p>
</div>
<div id="experimental-paradigm" class="section level4">
<h4><span class="header-section-number">2.3.1.2</span> Experimental paradigm</h4>
<p>The experiment consisted of two tasks (Detection and Discrimination) presented in separate blocks. A total of 56 trials of each task was delivered in 2 blocks of 28 trials each. The order of experimental blocks was interleaved, starting with discrimination.</p>
<p>The first discrimination block started after an introduction section, which included instructions about the stimuli and confidence scale, four practice trials and four confidence practice trials. A second introduction section was presented before the second block. Introduction sections were followed by multiple-choice comprehension questions, to monitor participants’ understanding of the main task and confidence reporting interface. To encourage concentration, feedback was given at the end of the second and fourth blocks about overall performance and mean confidence in the task.</p>
<p>Importantly, unlike the lab-based experiment, there was no calibration of difficulty for the two tasks. The rationale for this is that in Experiment 1 participants’ perceptual thresholds for motion discrimination were highly similar, and staircasing took a long time to converge. Furthermore, in Exp. 1 we aimed to control for task difficulty, but this introduced differences between the stimulus intensity in detection and discrimination. To complement our findings, here we aimed to match stimulus intensity between the two tasks, and allow for differences in task performance.</p>
<div id="trial-structure" class="section level5 unnumbered">
<h5>Trial structure</h5>
<p>In discrimination blocks, trial structure closely followed Experiment 2 from <span class="citation">Zylberberg et al. (2012)</span>, with a few adaptations. Following a fixation cross (500 ms), a rapid serial visual presentation (RSVP) was be presented (12 frames, presented at 25Hz), consisting of two sets of four adjacent vertical gray bars, displayed to the left and right of the fixation cross (see Fig. <a href="2-ch-RC.html#fig:ch2-exp2-design">2.7</a>). On each frame, the luminance of the bars was randomly sampled from a Gaussian distribution with a standard deviation of 10/255 units in the standard RGB 0-255 coordinate system. The average luminance of one set of bars was that of the background (128/255). The average luminance of the other set was 133/255, making this patch brighter on average. Participants then reported which of the two sets was brighter on average using the ‘D’ and ‘F’ keys on the keyboard. After their response, they rated their confidence on a continuous scale, by controlling the size of a colored circle with their mouse. High confidence was mapped to a big, blue circle, and low confidence to a small, red circle. To discourage hasty confidence ratings, the confidence rating scale stayed on the screen for at least 2000 milliseconds. Feedback about response accuracy was delivered after the confidence rating phase.</p>
<div class="figure" style="text-align: center"><span id="fig:ch2-exp2-design"></span>
<img src="figure/ch2/designExp2.png" alt="Task design for Experiment 2. In both tasks, participants viewed 480 milliseconds of two flicketing patches, after which they made a keyboard response to indicate which of the patches was bright (discrimination) or whether any of the patches was bright (detection). " width="\textwidth" />
<p class="caption">
Figure 2.7: Task design for Experiment 2. In both tasks, participants viewed 480 milliseconds of two flicketing patches, after which they made a keyboard response to indicate which of the patches was bright (discrimination) or whether any of the patches was bright (detection).
</p>
</div>
<p>Detection blocks were similar to discrimination blocks, with the exception that decisions were made about whether the average luminance of either of the two sets was brighter than the gray backgroud, or not. In ‘different’ trials, luminance of the four bars in one of the sets was sampled from a Gaussian distribution with mean 133/255, and the luminance of the other set from a Gaussian distribution with mean 128/255. In ‘same’ trials, the luminance of both sets was sampled from a distribution centered at 128/255. Decisions in Detection trials were reported using the ‘y’ and ‘n keys (‘y’ for ‘yes’ and ‘n’ for ‘no’). Confidence ratings and feedback were as in the discrimination task.</p>
</div>
</div>
</div>
<div id="results-3" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Results</h3>
<div id="response-accuracy-1" class="section level4">
<h4><span class="header-section-number">2.3.2.1</span> Response accuracy</h4>
<p>Overall accuracy level was 0.85 in the discrimination and 0.67 in the detection task. Performance for discrimination was significantly higher than for detection (<span class="math inline">\(M_d = 0.18\)</span>, 95% CI <span class="math inline">\([0.16\)</span>, <span class="math inline">\(0.20]\)</span>, <span class="math inline">\(t(101) = 18.01\)</span>, <span class="math inline">\(p &lt; .001\)</span>). Unlike in Experiment 1, where we aimed to control for task difficulty, here we decided to match stimulus intensity between the two tasks, so a difference between detection and discrimination performance was expected <span class="citation">(Wickens, 2002, p. 104)</span>.</p>
</div>
<div id="overall-properties-of-response-and-confidence-distributions-1" class="section level4">
<h4><span class="header-section-number">2.3.2.2</span> Overall properties of response and confidence distributions</h4>
<p>Similar to Exp. 1, participants were more likely to respond ‘yes’ than ‘no’ in the detection task (mean proportion of ‘yes’ responses: <span class="math inline">\(M = 0.54\)</span>, 95% CI <span class="math inline">\([0.53\)</span>, <span class="math inline">\(0.56]\)</span>, <span class="math inline">\(t(101) = 4.78\)</span>, <span class="math inline">\(p &lt; .001\)</span>). We did not observe a consistent response bias in discrimination (mean proportion of ‘right’ responses: <span class="math inline">\(M = 0.50\)</span>, 95% CI <span class="math inline">\([0.48\)</span>, <span class="math inline">\(0.51]\)</span>, <span class="math inline">\(t(101) = -0.62\)</span>, <span class="math inline">\(p = .537\)</span>).</p>
<p>Participants were also slower to deliver ‘no’ responses compared to ‘yes’ responses (median difference: 77.12 ms, <span class="math inline">\(t(101) = -6.84\)</span>, <span class="math inline">\(p &lt; .001\)</span> for a t-test on the log-transformed response times; see Fig. <a href="2-ch-RC.html#fig:ch2-exp2-hists">2.8</a>, upper panel). No significant difference in response times was observed for the discrimination task (median difference: 10.90 ms, <span class="math inline">\(t(101) = -1.40\)</span>, <span class="math inline">\(p = .165\)</span>).</p>
<p>Confidence in detection was generally lower than in discrimination, consistent with lower accuracy in this task (<span class="math inline">\(M_d = -0.09\)</span>, 95% CI <span class="math inline">\([-0.11\)</span>, <span class="math inline">\(-0.07]\)</span>, <span class="math inline">\(t(101) = -8.41\)</span>, <span class="math inline">\(p &lt; .001\)</span>; see Fig. <a href="2-ch-RC.html#fig:ch2-exp2-hists">2.8</a>, lower panel). Within detection, confidence in ‘yes’ responses was generally higher than confidence in ‘no’ responses (<span class="math inline">\(M = 0.10\)</span>, 95% CI <span class="math inline">\([0.07\)</span>, <span class="math inline">\(0.12]\)</span>, <span class="math inline">\(t(101) = 8.15\)</span>, <span class="math inline">\(p &lt; .001\)</span>). No difference in average confidence levels was observed between the two discrimination responses (<span class="math inline">\(M = 0.00\)</span>, 95% CI <span class="math inline">\([-0.02\)</span>, <span class="math inline">\(0.02]\)</span>, <span class="math inline">\(t(101) = -0.03\)</span>, <span class="math inline">\(p = .974\)</span>).</p>
<div class="figure" style="text-align: center"><span id="fig:ch2-exp2-hists"></span>
<img src="thesis_files/figure-html/ch2-exp2-hists-1.png" alt="Response time (upper panel) and confidence (lower panel) histograms for the detection (left) and discrimination (right) tasks in Experiment 2. Vertical lines represent the median response time and the mean confidence rating for each response." width="672" />
<p class="caption">
Figure 2.8: Response time (upper panel) and confidence (lower panel) histograms for the detection (left) and discrimination (right) tasks in Experiment 2. Vertical lines represent the median response time and the mean confidence rating for each response.
</p>
</div>
</div>
<div id="response-conditional-roc-curves-1" class="section level4">
<h4><span class="header-section-number">2.3.2.3</span> Response conditional ROC curves</h4>
<p>In contrast to the results of Experiment 1, auROC2 for ‘yes’ and ‘no’ responses were not significantly different (group difference in area under the response-conditional curve, AUROC2: <span class="math inline">\(M = 0.02\)</span>, 95% CI <span class="math inline">\([-0.02\)</span>, <span class="math inline">\(0.06]\)</span>, <span class="math inline">\(t(58) = 1.13\)</span>, <span class="math inline">\(p = .264\)</span>; see Fig. <a href="2-ch-RC.html#fig:ch2-exp2-rcROC">2.9</a>). In the Discussion, we discuss a candidate explanation for this null finding. Importantly, similar metacognitive sensitivity for ‘yes’ and ‘no’ responses should not affect the interpretation of our reverse correlation findings.</p>
<div class="figure" style="text-align: center"><span id="fig:ch2-exp2-rcROC"></span>
<img src="thesis_files/figure-html/ch2-exp2-rcROC-1.png" alt="Response conditional ROC curves for the two tasks and four responses in Exp. 2. The area under the curve is a measure of metacognitive sensitivity. Lower panel: distributions of the area under the curve for the four responses, across participants. Error bars stand for the standard error of the mean." width="672" />
<p class="caption">
Figure 2.9: Response conditional ROC curves for the two tasks and four responses in Exp. 2. The area under the curve is a measure of metacognitive sensitivity. Lower panel: distributions of the area under the curve for the four responses, across participants. Error bars stand for the standard error of the mean.
</p>
</div>
<!-- #### zROC curves -->
<!-- Unlike in Experiment 1, detection zROC slopes were not significantly different from 1  ($M = -0.04$, 95\% CI $[-0.09$, $0.01]$, $t(100) = -1.52$, $p = .131$), whereas discrimination zROC slopes were significantly shallower than 1 ($M = -0.14$, 95\% CI $[-0.25$, $-0.02]$, $t(93) = -2.29$, $p = .024$). This unexpected result indicates equal variance for the signal and noise distributions, but higher variance for targets presented on the right than on the left. Furthermore, first-order SDT fitted the data significantly better for the detection task than for the discrimination (difference in $R^2$ for the two tasks: $M = 0.15$, 95\% CI $[0.12$, $0.18]$, $t(93) = 8.85$, $p < .001$). This may reflect more frequent reports of the maximum confidence rating in the discrimination task, giving rise to non-zero y- intercepts in the discrimination, but not in the detection tasks. [ADD EXPLANATION AND STATS] -->
</div>
<div id="reverse-correlation-1" class="section level4">
<h4><span class="header-section-number">2.3.2.4</span> Reverse Correlation</h4>
<p>Stimuli in Exp. 2 consisted of two flickering patches, each comprising 4 gray bars presented for 12 frames. Together, this summed to 96 random luminance values per trial, which we subjected to reverse correlation analysis, following the analysis of Exp 2. in <span class="citation">Zylberberg et al. (2012)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:ch2-exp2-discrimination-RC"></span>
<img src="thesis_files/figure-html/ch2-exp2-discrimination-RC-1.png" alt="Decision and confidence discrimination kernels, Experiment 2. Upper panels: decision (left) and confidence (right) kernels for the flickering patch stimuli. Black frame signify a significant effect at the 0.05 significance level controlling for family-wise error rate across the 48 (12 timepoint x 4 positions) comparisons. Lower panels: decision and confidence kernels, averaged across the four bars to yield a single timecourse for the chosen (green) and unchosen (purple) stimuli. Shaded areas represent the the mean +- one standard error. The first 300 milliseconds of the trial are marked in yellow" width="672" />
<p class="caption">
Figure 2.10: Decision and confidence discrimination kernels, Experiment 2. Upper panels: decision (left) and confidence (right) kernels for the flickering patch stimuli. Black frame signify a significant effect at the 0.05 significance level controlling for family-wise error rate across the 48 (12 timepoint x 4 positions) comparisons. Lower panels: decision and confidence kernels, averaged across the four bars to yield a single timecourse for the chosen (green) and unchosen (purple) stimuli. Shaded areas represent the the mean +- one standard error. The first 300 milliseconds of the trial are marked in yellow
</p>
</div>
<div id="discrimination-decisions" class="section level5 unnumbered">
<h5>Discrimination decisions</h5>
<p>First, we asked whether random fluctuations in luminance had an effect on participants’ discrimination responses. Similar to the results obtained by Zylberberg et. al., discrimination decisions were sensitive to motion energy fluctuations during the first 300 milliseconds of the trial (<span class="math inline">\(t(101) = 10.98\)</span>, <span class="math inline">\(p &lt; .001\)</span>; see Fig. <a href="2-ch-RC.html#fig:ch2-exp2-discrimination-RC">2.10</a>, left panels). As per our comment in section <a href="2-ch-RC.html#e2-disc-RC"><strong>??</strong></a>, in order to test for decision biases we need to divide evidence not based on participants’ decision, but based on the true signal. Participants’ decisions were significantly more sensitive to fluctuations in luminance in the foil compared with the signal stimulus within the first 300 miliseconds of the trial (<span class="math inline">\(t(100) = -2.29\)</span>, <span class="math inline">\(p = .024\)</span>).</p>
</div>
<div id="discrimination-confidence" class="section level5 unnumbered">
<h5>Discrimination confidence</h5>
<p>We observed a significant effect of motion energy on confidence within the first 300 milliseconds of the stimulus (<span class="math inline">\(t(100) = 7.14\)</span>, <span class="math inline">\(p &lt; .001\)</span>; see Fig. <a href="2-ch-RC.html#fig:ch2-exp2-discrimination-RC">2.10</a>, right panels). Replicating <span class="citation">Zylberberg et al. (2012)</span>, this effect was significantly stronger for motion energy in the chosen direction, compared to the unchosen direction (<span class="math inline">\(t(100) = 2.56\)</span>, <span class="math inline">\(p = .012\)</span>).</p>
</div>
<div id="e2-det-RC" class="section level5 unnumbered">
<h5>Detection decisions</h5>
<!-- To make a detection decision, participants could either compare the luminance of the right and left stimuli and respond 'yes' if they thought one was brighter than the other, or alternatively pool luminance values from both stimuli and respond 'yes' if the average stimulus was brighter than the background. Participants who employ the second strategy should be more likely to respond 'yes' when both stimuli are equally bright, but are brighter than the background. The second strategy however predicts that overall luminance should have no effect on detection decision.  -->
<p>We pooled luminance values from both right and left stimuli and contrasted the resulting values as a function of detection response. The sum luminance had a significant effect on participants’ responses during the first 300 milliseconds (<span class="math inline">\(t(101) = 6.10\)</span>, <span class="math inline">\(p &lt; .001\)</span>; see Fig. <a href="2-ch-RC.html#fig:ch2-exp2-detection-RC-figure">2.11</a>, left panel), suggesting that participants were sensitive to sum evidence (overall luminance) in their detection responses.</p>
<div class="figure" style="text-align: center"><span id="fig:ch2-exp2-detection-RC-figure"></span>
<img src="thesis_files/figure-html/ch2-exp2-detection-RC-figure-1.png" alt="Decision and confidence detection kernels, Experiment 2. Upper panels: decision (left) and confidence (right) kernels for the flickering patch stimuli, showing the effect of overall luminance (across both stimuli) on decision and confidence. Black frame signify a significant effect at the 0.05 significance level controlling for family-wise error rate across the 48 (12 timepoint x 4 positions) comparisons. Lower panels: decision and confidence kernels, averaged across the four bars to yield a single timecourse for the difference in luminance effects in 'yes' and 'no' responses. Shaded areas represent the the mean +- one standard error. The first 300 milliseconds of the trial are marked in yellow" width="672" />
<p class="caption">
Figure 2.11: Decision and confidence detection kernels, Experiment 2. Upper panels: decision (left) and confidence (right) kernels for the flickering patch stimuli, showing the effect of overall luminance (across both stimuli) on decision and confidence. Black frame signify a significant effect at the 0.05 significance level controlling for family-wise error rate across the 48 (12 timepoint x 4 positions) comparisons. Lower panels: decision and confidence kernels, averaged across the four bars to yield a single timecourse for the difference in luminance effects in ‘yes’ and ‘no’ responses. Shaded areas represent the the mean +- one standard error. The first 300 milliseconds of the trial are marked in yellow
</p>
</div>
<p>We then asked if overall luminance had an effect on decision confidence, such that participants are more confident in their ‘yes’ responses for brighter displays, and more confident in their ‘no’ responses for darker displays. Interestingly, and in contrast with our hypothesis, sum luminance had no effect on decision confidence in ‘yes’ responses (<span class="math inline">\(t(99) = -0.02\)</span>, <span class="math inline">\(p = .983\)</span>), but had a significant effect on confidence in ‘no’ responses (<span class="math inline">\(t(99) = -2.43\)</span>, <span class="math inline">\(p = .017\)</span>; see Fig. <a href="2-ch-RC.html#fig:ch2-exp2-detection-RC-figure">2.11</a>, middle and right panels). As we show below, confidence in ‘yes’ responses was sensitive to the relative evidence for the two stimulus categories, rather than to the overall luminance of the screen. Our next analysis of detection signal trials diverged from our pre-registered plan. For the pre-registered analysis, see Appendix section <a href="B-supp-materials-for-ch-2.html#app2:PDRC">B.1</a>.</p>
</div>
</div>
</div>
<div id="detection-signal-trials-1" class="section level3">
<h3><span class="header-section-number">2.3.3</span> Detection signal trials</h3>
<div class="figure" style="text-align: center"><span id="fig:ch2-exp2-signal-RC"></span>
<img src="thesis_files/figure-html/ch2-exp2-signal-RC-1.png" alt="Decision and confidence kernels for detection signal trials, Experiment 2. Upper left: mean difference in luminance between 'yes' and 'no' responses for the target stimulus and foil stimuli. Upper middle and right panels: mean effect of luminance on confidence in the target and foil stimuli, in 'yes' and 'no' responses. Lower panels: the effects of luminance on decision and confidence, averaged across the four spatial locations. Shaded areas represent the the mean +- one standard error. The first 300 milliseconds of the trial are marked in yellow" width="672" />
<p class="caption">
Figure 2.12: Decision and confidence kernels for detection signal trials, Experiment 2. Upper left: mean difference in luminance between ‘yes’ and ‘no’ responses for the target stimulus and foil stimuli. Upper middle and right panels: mean effect of luminance on confidence in the target and foil stimuli, in ‘yes’ and ‘no’ responses. Lower panels: the effects of luminance on decision and confidence, averaged across the four spatial locations. Shaded areas represent the the mean +- one standard error. The first 300 milliseconds of the trial are marked in yellow
</p>
</div>
<p>We next focused on detection signal trials. In these trials, we could separate stimuli to a signal channel (the bright stimulus) and a noise channel (the foil), and ask how random variability in luminance in each channel affected detection decision and confidence. As in Exp. 1, a positive evidence bias effect in detection was apparent in the decision itself: when deciding whether one of the flickering patches was brighter, participants were sensitive to positive noise in the bright patch, but not to negative noise in the foil patch (<span class="math inline">\(t(101) = 6.10\)</span>, <span class="math inline">\(p &lt; .001\)</span>). Random fluctuations in luminance in the first 300 milliseconds of the trial also contributed to confidence in detection ‘yes’ responses (hit trials; <span class="math inline">\(t(99) = 5.08\)</span>, <span class="math inline">\(p &lt; .001\)</span>). Similar to the results of Exp. 1, detection confidence was not susceptible to a positive evidence bias (<span class="math inline">\(t(99) = -0.12\)</span>, <span class="math inline">\(p = .901\)</span>). To reiterate, while detection decisions were mostly sensitive to facilitating noise, confidence in detection ‘yes’ responses was equally sensitive to facilitating noise in the target stimulus, and to interfering noise in the foil stimulus.</p>
<!-- In fact, the only comparison that survived a correction for multiple comparisons for 12 timepoints and 4 spatial positions was in the foil stimulus, such that participants were more confident in their 'yes' responses if the foil stimulus was darker 75 ms after stimulus onset.  -->
<p>Consistent with the results of Exp. 1, confidence in ‘miss’ trials was independent of the contrast in luminance between the right and left stimuli (<span class="math inline">\(t(98) = 1.26\)</span>, <span class="math inline">\(p = .210\)</span>). However, as described in section <a href="2-ch-RC.html#e2-det-RC"><strong>??</strong></a>, confidence in ‘no’ responses was sensitive to the overall luminance of the display. A negative effect of luminance on confidence in ‘no’ responses was significant for the foil stimulus (<span class="math inline">\(t(98) = -2.64\)</span>, <span class="math inline">\(p = .010\)</span>), and marginally significant for the target stimulus (<span class="math inline">\(t(98) = -1.67\)</span>, <span class="math inline">\(p = .099\)</span>). Importantly, for both stimuli higher confidence was associated with lower luminance values, consistent with our observation that confidence in detection ‘no’ responses was based on the overall darkness of the display, rather than on relative evidence.</p>
</div>
</div>
<div id="discussion-1" class="section level2">
<h2><span class="header-section-number">2.4</span> Discussion</h2>
<p>In two experiments, we compared participants’ decisions and confidence ratings in discrimination and detection, matched for difficulty (Exp. 1) and signal strength (Exp. 2). In order to measure the contribution of perceptual evidence to confidence in detection and discrimination confidence ratings, we followed <span class="citation">Zylberberg et al. (2012)</span> and applied reverse correlation to noisy stimuli in perceptual decision making tasks. We fully replicated the main results of Zylberberg and colleagues: decision and confidence were affected mostly by perceptual evidence in the first 300 milliseconds of the trial, peaking at around 200 milliseconds. We also successfully replicated the positive-evidence bias: confidence in the discrimination task was more affected by supporting evidence than by conflicting evidence, giving rise to a ‘positive evidence bias’. A positive evidence bias in discrimination confidence judgments may indicate that participants adopt a detection disposition in their metacognitive monitoring, and focus on sum evidence rather than relative evidence.</p>
<p>In both experiments, evidence accumulation for detection responses had a similar temporal profile to that of discrimination. However, detection decisions but not confidence ratings showed a positive evidence bias: when making a detection response participants mostly ignored random fluctuations in stimulus energy that were not aligned with the true, presented signal, but these fluctuations were later taken into account when rating their confidence. In both experiments, relative evidence contributed to decision confidence in ‘yes’ responses, but was ignored in ‘no’ responses. Finally, in Experiment 2, but not in Experiment 1, sum evidence (the overall luminance of the display) significantly contributed to confidence in ‘no’ responses. Below we explore the predictions of several Bayes-rational models and their alignment with our observations.</p>
<div id="model-1-a-rational-agent-symmetric-evidence-structure" class="section level3">
<h3><span class="header-section-number">2.4.1</span> Model 1: a rational agent + symmetric evidence structure</h3>
<p>The first model made optimal decisions based on the likelihood ratio between the two hypotheses. This model had full access to the stimulus. Stimuli were modeled as ordered pairs of numbers, corresponding to the two sensory channels (for example, right and left motion, or right and left flickering patch). For simplicity, we ignored the temporal and spatial dynamics of evidence accumulation in our simulations, and focused on the general patterns of evidence weightings instead. In noise trials, both numbers were modeled as sampled from a normal distribution with mean 0 and standard deviation 1 (<span class="math inline">\(E_n\sim \mathcal{N}(0,1)\)</span>). In signal trials, one of the two numbers was sampled from a normal distribution with mean 1 (<span class="math inline">\(E_s \sim \mathcal{N}(1,1)\)</span>). The agent observes the two numbers, and decides (based on the likelihood ratio, and having full access to the true underlying distributions) if a stimulus was present or not (detection), or which of the two numbers was sampled from the signal distribution (detection). Their confidence is then proportional to the log likelihood ratio between the two hypotheses (signal presence of absence, or signal 1 or 2).</p>
<p>This model makes accurate predictions for the contribution of positive and negative perceptual evidence to discrimination and detection decisions: equal in discrimination, but asymmetric for detection (see Fig. <a href="2-ch-RC.html#fig:ch2-analyze-simulation1">2.13</a>. However, its predictions for confidence ratings are the exact opposite of what we observe in our data. The model predicts a positive evidence bias in detection confidence ratings, but we find symmetrical confidence kernels for detection confidence. In discrimination, where the model predicts equal contribution of positive and negative evidence to confidence, we find a significant positive evidence bias.</p>
<div class="figure" style="text-align: center"><span id="fig:ch2-analyze-simulation1"></span>
<img src="thesis_files/figure-html/ch2-analyze-simulation1-1.png" alt="Simulated reverse-correlation analysis in Model 1. A bias emerges in detection, but not in discrimination confidence ratings - the opposite of what we observe." width="672" />
<p class="caption">
Figure 2.13: Simulated reverse-correlation analysis in Model 1. A bias emerges in detection, but not in discrimination confidence ratings - the opposite of what we observe.
</p>
</div>
<!-- ### Explanation 1: misunderstanding of task instructions -->
<!-- One possibility is that some participants approached the detection task holding the wrong belief that on some trials both target stimuli will be presented simultaneously. For example, if participants thought that it was possible for both left and right flickering patches to be brighter than the background, it would be rational of them to only give positive weight to the brightness of the brighter stimulus, but not the darkness of the darker stimulus, in making a 'yes' response. In fact, for such participants, a pair of equally bright stimuli should generate a high-confidence 'yes' response. Similarly, if participants in Experiment 1 wrongly assumed that on some trials dots will simultaneously move in opposite directions, only positive evidence for coherent motion should be taken into account when making a detection judgment. Although we made sure to explain to participants that in detection only one of the two stimulus types will be present, it is still possible that participants misunderstood the instructions or were unable to follow them in practice. -->
<!-- This first explanation can be immediately ruled out. Participants' confidence in their 'yes' responses were significantly sensitive to negative evidence, suggesting that they were aware that a high contrast between the two candidate signals (e.g., a high contrast in luminance between the right and left patches) makes the stimulus more likely to be a 'target present' one. Although participants held the right beliefs about the space of possible stimuli, they showed a positive bias in their detection responses. -->
<!-- ### Explanation 2: selective attention -->
<!-- An alternative is that at any given moment in time, participants did not have full access to both perceptual channels, but only to one. This is easy to see in Experiment 2, where spatial attention may have been directed to one stimulus patch at a time, but is also possible for Experiment 1, where feature-based attention may have been directed to one motion direction at any given moment. As a cartoon example, we can assume that participants only focus on one sensory channel (motion direction, or flickering patch) per trial, and make their decision based on the observed evidence. -->
<!-- What would this model predict for a detection task? On signal trials in which participants happened to sample the true signal channel (e.g., focusing on the right patch when the right patch is in fact the bright one), their likelihood of responding 'yes' should scale with the intensity of the observed signal. This is consistent with our results of a positive effect of stimulus energy in the true direction/side on detection response. However this model also predicts that in case participants sample the opposite channel (e.g., focusing on the right patch when the left patch is in fact the bright one), lower stimulus intensity should increase the likelihood of responding 'no'. This is in contrast to the asymmetry we observe for detection responses, where stimulus intensity in the opposite channel has no effect on detection response.  -->
</div>
<div id="model-2-a-rational-agent-symmetric-evidence-structure" class="section level3">
<h3><span class="header-section-number">2.4.2</span> Model 2: a rational agent + symmetric evidence structure</h3>
<p>One possible driver of the positive evidence bias in confidence ratings is higher informational value in signal than in noise, such that giving more weight to information from this channel is rational. This is the case in unequal-variance SDT settings, where signal is sampled from a wider range of values than noise. As an example, if noise is sampled from a Gaussian distribution with mean 0 and variance 1 and signal from a Gaussian distribution with mean 2 and variance 3, sampling the value 6 is much more informative than sampling the value -2, because the first is only likely if sampled from the signal distribution (likelihood ratio &gt; 1,000,000), but the second is likely under both distributions (likelihood ratio = 1). Similarly, if the representation of coherent motion is more variable across trials than the representation of random motion, participants would be rational to give more weight to evidence for coherent motion in one channel than evidence for its absence in the other channel.</p>
<p>Higher variability in the representation of signal is often built into the experiment itself. For example, in our Exp. 1, following <span class="citation">Zylberberg et al. (2012)</span>, the number of coherently moving dots was itself randomly determined, sampled from a Gaussian distribution once in every four frames. This means that there were two sources of variability for the true direction of motion (variability in the direction of randomly moving dots and variability in the number of coherently moving dots), but only one source of variability for the opposite direction (variability in the direction of randomly moving dots). But even when signal is not made more variable by design, the representation of signal is expected to be more variable based on the Weber-Fechner law <span class="citation">(Fechner &amp; Adler, 1860)</span> and from the coupling between firing rate and firing rate variability implied by the Poisson form of neuronal firing distributions.</p>
<p>To obtain qualitative predictions, we simulated an unequal-variance first-order SDT model (full simulation details, including the source python code are available in appendix <a href="#REF"><strong>??</strong></a>). This model was identical to model 1 with one exception. In this model the artificial agent had access only to a degraded version of the two sensory samples, corrupted by additional noise. To model the unequal variance nature of the perception of signal and noise, this perceptual noise was sampled from a normal distribution with mean 0 and a standard deviation proportional to the magnitude of the sensory sample (<span class="math inline">\(x&#39;=x+\epsilon; \epsilon \sim \mathcal{N}(0,0.5\times x)\)</span>). The had full knowledge of this generative model for extracting a Log Likelihood Ratio in the process of making a decision and rating their confidence.</p>
<p>This simulation gave rise to a pronounced positive evidence bias in discrimination confidence ratings and in detection decisions (see Fig. <a href="#ch2-analyze-simulation2"><strong>??</strong></a>. Simulated agents were more sensitive to variations in the signal channel for deciding whether a signal was present or not, and when rating their confidence in discriminating between two stimulus classes. However, in contrast with the observed data, our unequal-variance model also predicted a positive evidence bias in detection confidence ratings and an effect of relative evidence on confidence in ‘no’ responses, which we do not observe in the actual data.</p>
<div class="figure" style="text-align: center"><span id="fig:ch2-analyze-simulation2"></span>
<img src="thesis_files/figure-html/ch2-analyze-simulation2-1.png" alt="Simulated reverse-correlation analysis in Model 2. A bias emerges in detection as well as in discrimination confidence ratings, in contrast to our finding of symmetrical confidence kernels in detection." width="672" />
<p class="caption">
Figure 2.14: Simulated reverse-correlation analysis in Model 2. A bias emerges in detection as well as in discrimination confidence ratings, in contrast to our finding of symmetrical confidence kernels in detection.
</p>
</div>
</div>
<div id="model-3-confidence-decision-cross" class="section level3">
<h3><span class="header-section-number">2.4.3</span> Model 3: confidence decision cross</h3>
<p>Models 1 and 2 described the behaviour of a rational agent but were unsuccessful in accounting for the mismatch between decision and confidence kernels. Model 3 drops the rationality assumption. This model is identical to Model 1 when it comes to the modeling of perceptual samples and the decision process. However, when coming to rate its confidence in a discrimination judgment, this model extracts the Log Likelihood Ratio not between stimulus category 1 and 2, but between signal presence or absence. Similarly, confidence in discrimination judgments is based on the Log Likelihood Ratio between the presenec of stimulus 1 or 2.</p>
<div class="figure" style="text-align: center"><span id="fig:ch2-analyze-simulation3"></span>
<img src="thesis_files/figure-html/ch2-analyze-simulation3-1.png" alt="Simulated reverse-correlation analysis in Model 3. A positive evidence bias emerges in discrimination confidence ratings, and a negative evidence bias emerges in detection confidence ratings. This is in contrast to our finding of symmetrical confidence kernels in detection." width="672" />
<p class="caption">
Figure 2.15: Simulated reverse-correlation analysis in Model 3. A positive evidence bias emerges in discrimination confidence ratings, and a negative evidence bias emerges in detection confidence ratings. This is in contrast to our finding of symmetrical confidence kernels in detection.
</p>
</div>
</div>
<div id="evidence-for-absence" class="section level3">
<h3><span class="header-section-number">2.4.4</span> Evidence for absence</h3>
<p>The results of the two experiments were highly similar, with two exceptions. First, the observed metacognitive asymmetry between confidence judgments for detection responses in Experiment 1 was not replicated in Experiment 2. In the second experiment, participants had similar metacognitive insight into their judgments about target presence and absence. Second, in Exp. 1 we found no effect of stimulus energy on confidence judgments in detection ‘no’ responses, whereas in Experiment 2 participants were more confident in the absence of the stimulus when overall stimulus energy was low. We suggest that these two observations may be related, and that the difference may lie in the availability of evidence for absence in the two experiments.</p>
<p>In Exp. 2, signal presence was defined as one of the flickering patches being brighter than the gray background. This meant that participants could be highly confident in the absence of a signal when both stimuli were particularly dark. This is what we observe in our reverse correlation analysis of detection ‘no’ responses (Fig. <a href="2-ch-RC.html#fig:ch2-exp2-detection-RC-figure">2.11</a> and Fig. <a href="2-ch-RC.html#fig:ch2-exp2-signal-RC">2.12</a>, right panels). In contrast, in Exp. 2 the presence of a signal could mean coherent motion to one of two opposite directions. This means that evidence for absence was never available: the opposite of the presence of rightward motion is leftward motion, not random motion. Indeed, motion energy had no effect on confidence in ‘no’ responses in Exp. 1 (Fig. <a href="2-ch-RC.html#fig:ch2-exp1-detection-RC-figure">2.5</a> and Fig. <a href="#fig:ch2-exp1-signal-RC"><strong>??</strong></a>, right panels).</p>
<p>The availability of positive evidence for signal absence may have boosted metacognitive sensitivity for detection ‘no’ responses in Exp. 2. Interestingly, however, even in Experiment 2, overall confidence in absence was lower than in presence with a similar effect size to that of Exp. 1 (mean differences of 0.08 and 0.10 of the confidence scale in Exp. 1 and 2, respectively), and ‘yes’ responses were faster on average (median differences -85.37 and -77.12). This may hint to the fact that RT and confidence differences between judgments of presence and absence are unrelated to the informational asymmetry between evidence for presence and for absence.</p>
<p>In summary, in two experiments we replicated the positive evidence bias for discrimination confidence judgments and found a similar bias in detection decisions. A first-order unequal variance framework accounted for this, but failed to account for the absence of a positive evidence bias for confidence judgments in signal presence: participants were more confident in the presence of a signal not only when the true signal was stronger, but also when the opposite signal was weaker. Our findings hint at a qualitative difference in the way subjects evaluate evidence for presence, absence, stimulus class.</p>
<p>In both experiments, detection ‘yes’ responses were faster on average, and accompanied by higher levels of subjective confidence compared with detection ‘no’ responses. In contrast, discrimination responses were similar at the group level. These behavioural asymmetries are in line with the classic interpretation of detection responses: ‘yes’ responses reflect the successful accumulation of evidence for signal presence, and ‘no’ responses reflect a failure to accumulate such evidence rather than the successful accumulation of evidence for signal absence.</p>
<!-- In Exp. 2 however luminance values for both dark and gray patches were sampled from Gaussian distributions with the equal variance. Even in such cases, the representation of signal may be more noisy than the representation of noise, as evident from the better fit of unequal-variance SDT models to visual detection behavioural data. -->
<!-- different features of the stimulus in Experiments 1 and 2. In Exp. 1, detection responses were mostly based on the contrast between evidence for the two stimulus categories, such that a 'yes' response was triggered not by overall high levels of motion energy, but by an asymmetry between rightward and leftward (or up and down) motion. In contrast, in Experiment 2 the sum brightness of the display had a significant effect on detection response, as well as the difference in brightness between the right and left stimuli. This is similar to taking a discrimination disposition to the detection task: in addition to detecting a bright stimulus, participants' responses reflected a discrimination between overall bright and dark displays. This allowed participants to accumulate evidence for absence in Experiment 2 (in the form of global darkness of the display), but not in Experiment 1. Interestingly, a metacognitive advantage for 'yes' responses was observed in Experiment 1, but contrary to our hypothesis, not in Experiment 2. This may have to do with the fact that in Experiment 2, but not in Experiment 1, participants could base their confidence in 'no' responses on the accumulated evidence for the absence of a stimulus.  -->
<!-- A difference in the availability of evidence for absence is also hinted by the contribution of perceptual evidence to confidence in detection, although the results of this analysis were less conclusive. In Experiment 1, our pseudo-discrimination analysis revealed a significant contribution of perceptual evidence to confidence in detection 'yes' responses, but not to confidence in detection 'no' responses. In Experiment 2, sum luminance had a significant effect on confidence in detection 'no' responses, but not in detection 'yes' responses. In the context of this study, we should be especially cautious in interpreting the absence of test significance as evidence for the absence of an effect. However, we do note that this pattern is consistent with the idea that only in Exp. 2, participants could base their detection 'no' responses and their confidence in these responses on direct evidence for absence. -->
<!-- A positive-evidence bias was observed in discrimination, but not in detection confidence ratings. However, as per our previous comment, we are cautious in interpreting this as strong evidence for equal consideration of supporting and conflicting evidence in detection confidence, especially in light of the fact that the effect of conflicting evidence on detection confidence was not significant in any of the two experiments (Exp. 1: ; Exp. 2: ).  In what follows we  -->
<!-- unpack one implication of our findings to the understanding of decision and confidence in perceptual detection: the relation between visibility, absolute evidence, and the positive-evidence bias -->
<!-- When considering two alternative hypotheses, the probability of a chosen hypothesis to be correct is as sensitive to the likelihood of observations under the unchosen hypothesis as it is sensitive to the likelihood of observations under the chosen one. However, findings suggest that evidence in favour of the chosen hypothesis is weighed more in post-decisional confidence ratings compared with evidence against it [@zylberberg2012construction; @koizumi2015does]. More support for this idea comes from the positive correlation between confidence and stimulus strength in incorrect responses that emerges in some perceptual decision making tasks [@kiani2014choice; @rausch2018confidence]. Simple signal detection models, which predict a negative correlation between confidence and stimulus strength in incorrect responses, fail to account for this pattern. In contrast, if participants base their confidence rating on a more global impression of absolute amount of evidence, a positive correlation can be observed between confidence and stimulus strength, even in incorrect trials [@rausch2018confidence]. -->
<!-- @rausch2018confidence proposed a two-stream model, in which visual discrimination decisions are informed by evidence, while confidence ratings are informed by evidence and an additional ‘visibility’ variable. The weighting of the two sources is dependent on context, task and participant. This model can account for the paradoxical positive correlation between stimulus strength and confidence in incorrect decisions, and also for the positive evidence bias in confidence ratings. If confidence ratings are affected both by the proportion of evidence for and against a decision (‘evidence’) and by the global availability of evidence (‘visibility’), evidence in favour of the decision will be positively correlated with confidence for both information streams, whereas evidence against a decision will be positively correlated with the ‘visibility’ variable but negatively correlated with the ‘evidence’ variable, reducing its absolute contribution to confidence. -->
<!-- The weighted visibility-evidence (WEV) model is specified with respect to discrimination tasks, but it makes interesting predictions for detection too. In detection, evidence and visibility are intimately linked. Evidence for presence and absence *is* high and low visibility, respectively. As a result, this model naturally accounts for generally lower levels of confidence for ‘no stimulus’ responses (because of low visibility) and their lower correlation with objective performance (because visibility and evidence pull in opposite directions for 'no' responses).  -->
<!-- An alternative is that the  -->
<!-- This model also provides a framework that accounts for the contrast between the reverse correlation analyses for the detection tasks in experiments 1 and 2. In Exp. 1, participants used success of the sum-luminance reverse correlation analysis in accounting for detection responses (Fig. \@ref(fig:ch2-exp2-discrimination-RC)). The same correlation between overall luminance of the display and discrimination confidence which is expressed in the positive-evidence bias also reflected in a correlation between overall luminance and response in the detection task.  -->

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>We reused the original Matlab code that was used for Experiment 1 in Zylberberg et. al. (2012), kindly shared by Ariel Zylberberg. <a href="2-ch-RC.html#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="1-ch-search.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="3-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["thesis.pdf", "PDF"], ["thesis.epub", "EPUB"], ["thesis.docx", "Word"]],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
