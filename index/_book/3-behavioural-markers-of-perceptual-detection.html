<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Behavioural markers of perceptual detection | Self-Modeling in Inference about Absence</title>
  <meta name="description" content="Chapter 3 Behavioural markers of perceptual detection | Self-Modeling in Inference about Absence" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Behavioural markers of perceptual detection | Self-Modeling in Inference about Absence" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Behavioural markers of perceptual detection | Self-Modeling in Inference about Absence" />
  
  
  

<meta name="author" content="Matan Mazor" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="2-ch-search.html"/>
<link rel="next" href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"></a></li>
<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> This will automatically install the {remotes} package and {thesisdown}</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#inference-about-absence"><i class="fa fa-check"></i><b>1.1</b> Inference about absence</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#formalabsence"><i class="fa fa-check"></i><b>1.2</b> Probabilistic reasoning, criterion setting, and self knowledge</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#symmetrical-definition"><i class="fa fa-check"></i>Symmetrical definition:</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dissymmetrical-definition"><i class="fa fa-check"></i>Dissymmetrical definition:</a></li>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#second-order-cognition"><i class="fa fa-check"></i><b>1.2.1</b> Second-order cognition</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#detectionmodels"><i class="fa fa-check"></i><b>1.2.2</b> Computational models of detection</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#htm"><i class="fa fa-check"></i>The High-Threshold model</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#sdt"><i class="fa fa-check"></i>Signal Detection Theory</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#detection-i-would-have-noticed-it"><i class="fa fa-check"></i><b>1.3</b> Detection: “I would have noticed it”</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#intro:search"><i class="fa fa-check"></i><b>1.4</b> Visual search: “I would have found it”</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#memory-i-would-have-remembered-it"><i class="fa fa-check"></i><b>1.5</b> Memory: “I would have remembered it”</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#the-development-of-a-self-model"><i class="fa fa-check"></i><b>1.6</b> The development of a self-model</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#this-thesis"><i class="fa fa-check"></i><b>1.7</b> This thesis</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-ch-search.html"><a href="2-ch-search.html"><i class="fa fa-check"></i><b>2</b> Zero-shot search termination reveals a dissociation between implicit and explicit metacognitive knowledge</a><ul>
<li class="chapter" data-level="2.1" data-path="2-ch-search.html"><a href="2-ch-search.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="2-ch-search.html"><a href="2-ch-search.html#experiment-1"><i class="fa fa-check"></i><b>2.2</b> Experiment 1</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-ch-search.html"><a href="2-ch-search.html#participants"><i class="fa fa-check"></i><b>2.2.1</b> Participants</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-ch-search.html"><a href="2-ch-search.html#procedure"><i class="fa fa-check"></i><b>2.2.2</b> Procedure</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-ch-search.html"><a href="2-ch-search.html#data-analysis"><i class="fa fa-check"></i><b>2.2.3</b> Data analysis</a></li>
<li class="chapter" data-level="2.2.4" data-path="2-ch-search.html"><a href="2-ch-search.html#results"><i class="fa fa-check"></i><b>2.2.4</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-ch-search.html"><a href="2-ch-search.html#experiment-2"><i class="fa fa-check"></i><b>2.3</b> Experiment 2</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-ch-search.html"><a href="2-ch-search.html#participants-1"><i class="fa fa-check"></i><b>2.3.1</b> Participants</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-ch-search.html"><a href="2-ch-search.html#procedure-1"><i class="fa fa-check"></i><b>2.3.2</b> Procedure</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-ch-search.html"><a href="2-ch-search.html#results-1"><i class="fa fa-check"></i><b>2.3.3</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-ch-search.html"><a href="2-ch-search.html#discussion"><i class="fa fa-check"></i><b>2.4</b> Discussion</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-ch-search.html"><a href="2-ch-search.html#is-implicit-metacognitive-knowledge-metacognitive"><i class="fa fa-check"></i><b>2.4.1</b> Is implicit metacognitive knowledge metacognitive?</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-ch-search.html"><a href="2-ch-search.html#inference-about-absence-as-a-tool-for-studying-implicit-self-knowledge"><i class="fa fa-check"></i><b>2.4.2</b> Inference about absence as a tool for studying implicit self knowledge</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-ch-search.html"><a href="2-ch-search.html#conclusion"><i class="fa fa-check"></i><b>2.4.3</b> Conclusion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-behavioural-markers-of-perceptual-detection.html"><a href="3-behavioural-markers-of-perceptual-detection.html"><i class="fa fa-check"></i><b>3</b> Behavioural markers of perceptual detection</a><ul>
<li class="chapter" data-level="3.1" data-path="3-behavioural-markers-of-perceptual-detection.html"><a href="3-behavioural-markers-of-perceptual-detection.html#experiment-1-1"><i class="fa fa-check"></i><b>3.1</b> Experiment 1</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-behavioural-markers-of-perceptual-detection.html"><a href="3-behavioural-markers-of-perceptual-detection.html#methods"><i class="fa fa-check"></i><b>3.1.1</b> Methods</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-behavioural-markers-of-perceptual-detection.html"><a href="3-behavioural-markers-of-perceptual-detection.html#analysis"><i class="fa fa-check"></i><b>3.1.2</b> Analysis</a></li>
<li class="chapter" data-level="3.1.3" data-path="3-behavioural-markers-of-perceptual-detection.html"><a href="3-behavioural-markers-of-perceptual-detection.html#results-2"><i class="fa fa-check"></i><b>3.1.3</b> Results</a></li>
<li class="chapter" data-level="" data-path="3-behavioural-markers-of-perceptual-detection.html"><a href="3-behavioural-markers-of-perceptual-detection.html#discussion-exp.-1"><i class="fa fa-check"></i>Discussion: Exp. 1</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><i class="fa fa-check"></i><b>4</b> Distinct neural contributions to metacognition for detecting (but not discriminating) visual stimuli</a><ul>
<li class="chapter" data-level="4.1" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#introduction-1"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#methods-and-materials"><i class="fa fa-check"></i><b>4.2</b> Methods and Materials</a><ul>
<li class="chapter" data-level="4.2.1" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#participants-3"><i class="fa fa-check"></i><b>4.2.1</b> Participants</a></li>
<li class="chapter" data-level="4.2.2" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#design-and-procedure"><i class="fa fa-check"></i><b>4.2.2</b> Design and procedure</a></li>
<li class="chapter" data-level="4.2.3" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#scanning-parameters"><i class="fa fa-check"></i><b>4.2.3</b> Scanning parameters</a></li>
<li class="chapter" data-level="4.2.4" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#analysis-1"><i class="fa fa-check"></i><b>4.2.4</b> Analysis</a></li>
<li class="chapter" data-level="4.2.5" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#exclusion-criteria"><i class="fa fa-check"></i><b>4.2.5</b> Exclusion criteria</a></li>
<li class="chapter" data-level="4.2.6" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#response-conditional-type-ii-roc-curves"><i class="fa fa-check"></i><b>4.2.6</b> Response conditional type-II ROC curves</a></li>
<li class="chapter" data-level="4.2.7" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#imaging-analysis"><i class="fa fa-check"></i><b>4.2.7</b> Imaging analysis</a></li>
<li class="chapter" data-level="4.2.8" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#statistical-inference-1"><i class="fa fa-check"></i><b>4.2.8</b> Statistical inference</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#results-3"><i class="fa fa-check"></i><b>4.3</b> Results</a></li>
<li class="chapter" data-level="4.4" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#behavioural-results"><i class="fa fa-check"></i><b>4.4</b> Behavioural results</a><ul>
<li class="chapter" data-level="4.4.1" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#imaging-results"><i class="fa fa-check"></i><b>4.4.1</b> Imaging results</a></li>
<li class="chapter" data-level="4.4.2" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#computational-models"><i class="fa fa-check"></i><b>4.4.2</b> Computational models</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#discussion-1"><i class="fa fa-check"></i><b>4.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="conclusion-1.html"><a href="conclusion-1.html"><i class="fa fa-check"></i>Conclusion</a></li>
<li class="chapter" data-level="5" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html"><i class="fa fa-check"></i><b>5</b> Signal Detection Theory</a><ul>
<li class="chapter" data-level="5.1" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app1:ROC"><i class="fa fa-check"></i><b>5.1</b> ROC and zROC curves</a></li>
<li class="chapter" data-level="5.2" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app1:uvSDT"><i class="fa fa-check"></i><b>5.2</b> Unequal-variance (uv) SDT</a></li>
<li class="chapter" data-level="5.3" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app1:mc"><i class="fa fa-check"></i><b>5.3</b> SDT Measures for Metacognition</a></li>
<li class="chapter" data-level="5.4" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app3:buttonpresses"><i class="fa fa-check"></i><b>5.4</b> Confidence button presses</a></li>
<li class="chapter" data-level="5.5" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app3:zROC"><i class="fa fa-check"></i><b>5.5</b> zROC curves</a></li>
<li class="chapter" data-level="5.6" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app3:GC-DM"><i class="fa fa-check"></i><b>5.6</b> Global confidence design matrix</a></li>
<li class="chapter" data-level="5.7" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app3:ROIconf"><i class="fa fa-check"></i><b>5.7</b> Effect of confidence in our pre-specified ROIs</a></li>
<li class="chapter" data-level="5.8" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app3:varianceRat"><i class="fa fa-check"></i><b>5.8</b> SDT variance ratio correlation with the quadratic confidence effect</a></li>
<li class="chapter" data-level="5.9" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app3:efficiency"><i class="fa fa-check"></i><b>5.9</b> Correlation of metacognitive efficiency with linear and quadratic confidence effects</a></li>
<li class="chapter" data-level="5.10" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app3:cross"><i class="fa fa-check"></i><b>5.10</b> Confidence-decision cross classification</a></li>
<li class="chapter" data-level="5.11" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app3:SDT"><i class="fa fa-check"></i><b>5.11</b> Static Signal Detection Theory</a><ul>
<li class="chapter" data-level="5.11.1" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#discrimination"><i class="fa fa-check"></i><b>5.11.1</b> Discrimination</a></li>
<li class="chapter" data-level="5.11.2" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#detection-1"><i class="fa fa-check"></i><b>5.11.2</b> Detection</a></li>
</ul></li>
<li class="chapter" data-level="5.12" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app3:Dynamic"><i class="fa fa-check"></i><b>5.12</b> Dynamic Criterion</a><ul>
<li class="chapter" data-level="5.12.1" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#discrimination-1"><i class="fa fa-check"></i><b>5.12.1</b> Discrimination</a></li>
<li class="chapter" data-level="5.12.2" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#detection-2"><i class="fa fa-check"></i><b>5.12.2</b> Detection</a></li>
</ul></li>
<li class="chapter" data-level="5.13" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app3:Monitoring"><i class="fa fa-check"></i><b>5.13</b> Attention Monitoring</a><ul>
<li class="chapter" data-level="5.13.1" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#discrimination-2"><i class="fa fa-check"></i><b>5.13.1</b> Discrimination</a></li>
<li class="chapter" data-level="5.13.2" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#detection-3"><i class="fa fa-check"></i><b>5.13.2</b> Detection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Self-Modeling in Inference about Absence</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="behavioural-markers-of-perceptual-detection" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Behavioural markers of perceptual detection</h1>
<p>First-year statistics courses teach that “the absence of evidence is not evidence of absence” <span class="citation">(Altman &amp; Bland, 1995)</span>. In other words, failing to find a statistically significant effect is not sufficient justification for that an effect does not exist. To make claims about the absence of an effect, one needs to incorporate information about test sensitivity (statistical power), specificity (significance level), and the probability of an effect to exist prior to seeing any data. Similarly, when facing a perceptual detection problem (<em>is there any signal in the noise?</em>), failure to detect a signal is not by itself sufficient to conclude that a signal is absent. In this case, information of potential relevance is the sensitivity of one’s senses to external signals, the tendency to hallucinate in the absence of true signal, and the prior probability of signal presence. The same considerations are also relevant when forming subjective confidence in the presence or absence of external signals.</p>
<p>Adult humans have been shown to rationally integrate information about the prior of a signal and the statistical power of the test when forming confidence in the absence of a signal in an abstract decision making task <span class="citation">(Hsu, Horng, Griffiths, &amp; Chater, 2017)</span>. However, it remains unknown whether similar computational considerations apply in perceptual settings, where uncertainty about the presence or absence of a signal is affected by both external (signal intensity) and internal (sensory precision) sources, and where these sources are not explicitly signaled. What information is incorporated into confidence ratings in visual detection, and whether this varies between decisions about signal absence or presence, is still an open question. Specifically, while confidence in decisions about target presence can scale with stimulus intensity, it is unclear what stimulus features, if any, will affect confidence in stimulus absence, given that decisions about absence can only be based on the lack of perceptual evidence for stimulus presence.</p>
<div id="experiment-1-1" class="section level2">
<h2><span class="header-section-number">3.1</span> Experiment 1</h2>
<div id="methods" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Methods</h3>
<div id="participants-2" class="section level4">
<h4><span class="header-section-number">3.1.1.1</span> Participants</h4>
<p>10 healthy participants took part in the experiment. Each participant performed four sessions of 600 trials each, in blocks of 100 trials. Sessions took place on different days and consisted of 3 discrimination blocks interleaved with 3 detection blocks.</p>
</div>
<div id="experimental-procedure" class="section level4">
<h4><span class="header-section-number">3.1.1.2</span> Experimental procedure</h4>
<p>The experimental procedure for this study largely followed the procedure described in <span class="citation">Zylberberg, Barttfeld, &amp; Sigman (2012)</span>, experiment 1. Participants observed a random-dot kinematogram for a fixed duration of 700 ms. In discrimination trials, the direction of motion was one of two opposite directions with equal probability, and participants reported the observed direction by pressing one of two arrow keys on a standard keyboard. In detection blocks, the direction of motion was specified as one of two opposite directions on half of the trials. On the other half, dots moved randomly without any direction. Participants reported whether there was coherent motion by pressing one of two arrow keys on a standard keyboard.</p>
<p>In both detection and discrimination blocks, following a decision participants indicated their confidence (the degree to which they considered their response to be correct). Confidence was reported on a continuous scale ranging from chance to complete certainty. To avoid response bias in confidence reports, the orientation (vertical or horizontal) and polarity (e.g., right or left of the scale was set to agree with the type 1 response. For example, following a down arrow press, a vertical confidence bar was presented where ‘guess’ is at the center of the screen and ‘certain’ appeared at the lower end of the scale.</p>
<p>To control for response requirements, for 5 subjects the dots moved to the right or to the left, and for the 5 other subjects they moved upward or downward. The first group made discrimination judgments with the right and left keys and detection judgments with the up and down keys, and this mapping was reversed for the second group. The number of coherently moving dots (“motion coherence”) was adjusted to keep participants’ performance at around 70% accuracy for detection and discrimination tasks independently. This was achieved by following these steps once every 20 trials:</p>
<ol style="list-style-type: decimal">
<li>Take the mean accuracy of the last 20 trials <span class="math inline">\(\bar{x}\)</span>.</li>
<li>If <span class="math inline">\(\bar{x}&gt;0.8\)</span>, decrease the coherence level by 3%: <span class="math inline">\(c→c-0.03\)</span></li>
<li>If <span class="math inline">\(\bar{x}&lt;0.6\)</span>, increase the coherence level by 3%: <span class="math inline">\(c→c+0.03\)</span></li>
<li>Else, continue.</li>
</ol>
<p>Stimuli for discrimination blocks were generated using the exact same procedure reported in <span class="citation">Zylberberg et al. (2012)</span><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. Trials started with a presentation of a fixation cross for one second, immediately followed by stimulus presentation. The stimulus consisted of 152 white dots (diameter = 0.14°), presented within a 6.5° circular aperture centered on the fixation point for 700 milliseconds (42 frames, frame rate = 60 HZ). Dots were grouped in two patches of equal sizes of 56 dots each. Every other frame, the dots of one patch were replaced with a new set of randomly positioned dots. For a coherence value of <span class="math inline">\(c’\)</span>, a proportion of <span class="math inline">\(c’\)</span> of the dots from the second patch moved coherently in one direction by a fixed distance of 0.33°, while the remaining dots in the patch moved in random directions by a fixed distance of 0.33°. On the next update, the patches were switched, to prevent participants from tracing the position of specific dots. Frame-specific coherence values were sampled for each screen update from a normal distribution centred around the coherence value <span class="math inline">\(c\)</span> with a standard deviation of 0.07, with the constraint that <span class="math inline">\(c&#39;\)</span> must be a number between 0 and 1.</p>
<p>Stimuli for detection blocks were generated using a similar procedure, with the only difference being that on a random half of the trials coherence was set to 0%, without random sampling of coherence values for different frames (see Fig. 1).</p>
<div class="figure" style="text-align: center"><span id="fig:ch2-exp1-design"></span>
<img src="figure/ch2/designExp1.png" alt="Task design for Experiment 1. In both tasks, participants viewed 700 milliseconds of a random dot motion array, after which they made a keyboard response to indicate their decision (motion direction in discrimination, signal absence or presence in detection), followed by a continuous confidence report using the mouse. 5 participants viewed vertically moving dots and indicated their detection responses on a horizontal scale, and 5 participants viewed horizontally moving dots and indicated their detection responses on a vertical scale. " width="\textwidth" />
<p class="caption">
Figure 3.1: Task design for Experiment 1. In both tasks, participants viewed 700 milliseconds of a random dot motion array, after which they made a keyboard response to indicate their decision (motion direction in discrimination, signal absence or presence in detection), followed by a continuous confidence report using the mouse. 5 participants viewed vertically moving dots and indicated their detection responses on a horizontal scale, and 5 participants viewed horizontally moving dots and indicated their detection responses on a vertical scale.
</p>
</div>
</div>
</div>
<div id="analysis" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Analysis</h3>
<p>Our pre-registered objectives for this study were to:</p>
<ol style="list-style-type: decimal">
<li><p>Replicate the finding that metacognitive sensitivity for ‘no’ responses is lower than for ‘yes’ responses in detection <span class="citation">(Kanai, Walsh, &amp; Tseng, 2010; Kellij, Fahrenfort, Lau, Peters, &amp; Odegaard, 2018; Meuwese, Loon, Lamme, &amp; Fahrenfort, 2014)</span>, and generalize these findings to a different task (determining whether or not some dots moved coherently in a random dot kinematogram; RDK).</p></li>
<li><p>Estimate the goodness of fit of an unequal-variance SDT model to perceptual detection data, and compare it to the fit of models that assume a qualitative difference between confidence in absence and confidence in presence.</p></li>
<li><p>Replicate the results of <span class="citation">Zylberberg et al. (2012)</span> for perceptual discrimination. Namely, show that confidence in a motion discrimination task is mostly influenced by evidence for the selected direction within a short time window around 300 milliseconds after stimulus onset.</p></li>
<li><p>Test the generality of the results of <span class="citation">Zylberberg et al. (2012)</span> to perceptual detection, where, by definition, no evidence can be collected to support a ‘no’ decision. Examine the contribution of signal variance.</p></li>
</ol>
<div id="reverse-correlation-analysis" class="section level4 unnumbered">
<h4>Reverse correlation analysis</h4>
<p>For the reverse correlation analysis, we followed a procedure similar to the one described in <span class="citation">Zylberberg et al. (2012)</span>. For each of the four directions (right, left, up and down), we applied two spatiotemporal filters to the frames of the dot motion stimuli as described in previous studies <span class="citation">(Adelson &amp; Bergen, 1985; Zylberberg et al., 2012)</span>. The outputs of the two filters were squared and summed, resulting in a three-dimensional matrix with motion energy in the specific direction as a function of x, y, and time. We then took the mean of this matrix across the x and y dimensions to obtain an estimate of the overall temporal fluctuations in motion energy in the selected direction. Additionally, for every time point we extracted the variance along the x and y dimensions, to obtain a measure of temporal fluctuations in spatial variance. Using this filter, we obtained trial-wise estimates of temporal fluctuations in the mean and variance of motion energy for upward, downward, leftward and rightward motion. Given a high correlation between our mean and variance estimates, we focused our analysis on the mean motion energy.</p>
<p>In order to distill random fluctuations in motion energy from mean differences between stimulus categories, we subtracted the mean motion energy from trial-specific motion energy vectors. The mean motion energy vectors were extracted at the group level, separately for each motion coherence level and as a function of motion direction. We chose this approach instead of the linear regression approach used by <span class="citation">Zylberberg et al. (2012)</span> in order to control for nonlinear effects of coherence on motion energy.</p>
</div>
<div id="statistical-inference" class="section level4 unnumbered">
<h4>Statistical inference</h4>
<p>Statistics were extracted separately for each participant, and group-level inference was then performed on the first-order statistics. T-test Bayes factors were used to quantify the evidence for the null when appropriate, using a Jeffrey-Zellner-Siow Prior for the null distribution, with a unit prior scale <span class="citation">(Rouder, Speckman, Sun, Morey, &amp; Iverson, 2009)</span>.</p>
</div>
</div>
<div id="results-2" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Results</h3>
<div id="response-accuracy" class="section level4">
<h4><span class="header-section-number">3.1.3.1</span> Response accuracy</h4>
<p>Overall accuracy level was 0.75 in the discrimination and 0.72 in the detection task. Performance for discrimination was significantly higher than for detection (<span class="math inline">\(M_d = 0.02\)</span>, 95% CI <span class="math inline">\([0.00\)</span>, <span class="math inline">\(0.04]\)</span>, <span class="math inline">\(t(8) = 2.37\)</span>, <span class="math inline">\(p = .046\)</span>). This difference in task performance reflected a slower convergence of the staircasing procedure for the discrimination task during the first session. When discarding all data from the first session and analyzing only data from the last three sessions (1800 trials per participant), task performance was equated between the two tasks at the group level (<span class="math inline">\(M_d = 0.00\)</span>, 95% CI <span class="math inline">\([-0.03\)</span>, <span class="math inline">\(0.02]\)</span>, <span class="math inline">\(t(8) = -0.39\)</span>, <span class="math inline">\(p = .707\)</span>; <span class="math inline">\(\mathrm{BF}_{\textrm{01}} = 2.92\)</span>). In order to avoid conflating true differences between discrimination and detection with more general difficulty effects, the first session was excluded from all subsequent analyses.</p>
</div>
<div id="overall-properties-of-response-and-confidence-distributions" class="section level4">
<h4><span class="header-section-number">3.1.3.2</span> Overall properties of response and confidence distributions</h4>
<p>Participants were more likely to respond ‘yes’ than ‘no’ (mean proportion of ‘yes’ responses: <span class="math inline">\(M = 0.60\)</span>, 95% CI <span class="math inline">\([0.55\)</span>, <span class="math inline">\(0.65]\)</span>, <span class="math inline">\(t(8) = 4.63\)</span>, <span class="math inline">\(p = .002\)</span>). As a result, accuracy for ‘no’ responses was higher on average (mean accuracy in ‘yes’ responses: 0.68, mean accuracy in ‘no’ responses: 0.78, <span class="math inline">\(t(8) = -4.01\)</span>, <span class="math inline">\(p = .004\)</span>). We did not observe a consistent response bias for the discrimination data (mean proportion of ‘rightward’ or ‘upward’ responses: <span class="math inline">\(M = 0.54\)</span>, 95% CI <span class="math inline">\([0.52\)</span>, <span class="math inline">\(0.56]\)</span>, <span class="math inline">\(t(8) = 3.76\)</span>, <span class="math inline">\(p = .006\)</span>).</p>
<p>In detection, participants were generally slower to deliver ‘no’ responses compared to ‘yes’ responses (median difference: 93.16 ms, <span class="math inline">\(t(8) = -4.25\)</span>, <span class="math inline">\(p = .003\)</span> for a t-test on the log-transformed response times; see Fig. <a href="3-behavioural-markers-of-perceptual-detection.html#fig:ch2e1hists">3.2</a>, upper panel). No significant difference in response times was observed for the discrimination task (median difference: 7.42 ms, <span class="math inline">\(t(8) = -0.74\)</span>, <span class="math inline">\(p = .480\)</span>).</p>
<p>Confidence in detection was generally higher than in discrimination (<span class="math inline">\(M_d = 0.07\)</span>, 95% CI <span class="math inline">\([0.00\)</span>, <span class="math inline">\(0.13]\)</span>, <span class="math inline">\(t(8) = 2.31\)</span>, <span class="math inline">\(p = .050\)</span>; see Fig. <a href="3-behavioural-markers-of-perceptual-detection.html#fig:ch2e1hists">3.2</a>, lower panel). Within detection, confidence in ‘yes’ responses was generally higher than confidence in ‘no’ responses (<span class="math inline">\(M = 0.08\)</span>, 95% CI <span class="math inline">\([0.02\)</span>, <span class="math inline">\(0.14]\)</span>, <span class="math inline">\(t(8) = 3.22\)</span>, <span class="math inline">\(p = .012\)</span>). No difference in average confidence levels was found between the two discrimination responses (<span class="math inline">\(M = 0.02\)</span>, 95% CI <span class="math inline">\([-0.03\)</span>, <span class="math inline">\(0.07]\)</span>, <span class="math inline">\(t(8) = 1.06\)</span>, <span class="math inline">\(p = .321\)</span>).</p>
<div class="figure" style="text-align: center"><span id="fig:ch2e1hists"></span>
<img src="thesis_files/figure-html/ch2e1hists-1.png" alt="Response time (upper panel) and confidence (lower panel) histograms for the detection (left) and discrimination (right) tasks in Experiment 1. Vertical lines represent the median response time and the mean confidence rating for each response." width="672" />
<p class="caption">
Figure 3.2: Response time (upper panel) and confidence (lower panel) histograms for the detection (left) and discrimination (right) tasks in Experiment 1. Vertical lines represent the median response time and the mean confidence rating for each response.
</p>
</div>
</div>
<div id="response-conditional-roc-curves" class="section level4">
<h4><span class="header-section-number">3.1.3.3</span> Response conditional ROC curves</h4>
<p>Following <span class="citation">Meuwese et al. (2014)</span>, we extracted response-conditional type-2 ROC (rc-ROC) curves for the two tasks. Unlike traditional type-I ROC curves that provide a visual representation of subjects’ ability to distinguish between two external world states, type 2 ROC curves represent their ability to track the accuracy of their own responses. The area under the response-conditional ROC curve (auROC2) is a measure of metacognitive sensitivity, with higher values corresponding to more accurate metacognitive monitoring.</p>
<p>Mean response-conditional ROC curves for the two responses in the discrimination task closely matched (<span class="math inline">\(M = 0.01\)</span>, 95% CI <span class="math inline">\([-0.05\)</span>, <span class="math inline">\(0.07]\)</span>, <span class="math inline">\(t(8) = 0.37\)</span>, <span class="math inline">\(p = .720\)</span>), indicating that on average, participants had similar metacognitive insight into the accuracy of the two discrimination responses. In contrast, response-conditional ROC curves for ‘yes’ and ‘no’ responses diverged significantly, indicating a metacognitive advantage for ‘yes’ responses (group difference in area under the response-conditional curve, AUROC2: <span class="math inline">\(M = 0.10\)</span>, 95% CI <span class="math inline">\([0.02\)</span>, <span class="math inline">\(0.19]\)</span>, <span class="math inline">\(t(8) = 2.82\)</span>, <span class="math inline">\(p = .023\)</span>).</p>
<!-- This effect was still significant after subtracting the mean confidence for each experimental block and response from the ratings (t(9)=4.39, p=0.001), suggesting that this pattern originates from trial-specific differences in confidence levels, and not from more global confidence level adjustments occurring at the block level (see figure S1).  -->
<p>To better understand the origin of this difference between ‘yes’ and ‘no’ curves, we compared the detection auROC2 values with the average discrimination auUROC2. We found both a significant increase in auROC2 for ‘yes’ responses (<span class="math inline">\(M = 0.05\)</span>, 95% CI <span class="math inline">\([0.00\)</span>, <span class="math inline">\(0.11]\)</span>, <span class="math inline">\(t(8) = 2.39\)</span>, <span class="math inline">\(p = .044\)</span>) and a marginally significant decrease in auROC2 for ‘no’ responses relative to discrimination (<span class="math inline">\(M = -0.05\)</span>, 95% CI <span class="math inline">\([-0.11\)</span>, <span class="math inline">\(0.01]\)</span>, <span class="math inline">\(t(8) = -1.85\)</span>, <span class="math inline">\(p = .102\)</span>). In other words, relative to our discrimination benchmark, metacognitive asymmetry in detection was driven by improved metacognitive insight into the accuracy of ‘yes’ responses, and degraded metacognitive insight into the accuracy of ‘no’ responses.</p>
<div class="figure" style="text-align: center"><span id="fig:ch2e1ROC"></span>
<img src="thesis_files/figure-html/ch2e1ROC-1.png" alt="Response conditional ROC curves for the two tasks and four responses in Exp. 1. The area under the curve is a measure of metacognitive sensitivity. Lower panel: distributions of the area under the curve for the four responses, across participants. Error bars stand for the standard error of the mean." width="672" />
<p class="caption">
Figure 3.3: Response conditional ROC curves for the two tasks and four responses in Exp. 1. The area under the curve is a measure of metacognitive sensitivity. Lower panel: distributions of the area under the curve for the four responses, across participants. Error bars stand for the standard error of the mean.
</p>
</div>
<p>A difference in response-conditional auROC estimates can emerge from higher-order differences in metacognitive monitoring for the two responses or in lower-level differences in the representations of signal and noise <span class="citation">(such as in first-order signal detection models where the signal variance is higher; Maniscalco &amp; Lau, 2014)</span>. Importantly, a difference can also emerge in first-order signal-detection models that assume equal variance, in the presence of a response bias or insufficient variance in confidence ratings. To test if this was the case here, we simulated data that was identical to our empirical data except for confidence ratings in correct responses, that were chosen to perfectly agree with the assumptions of an equal-variance SDT model, given participants’ decision criterion, sensitivity, and their confidence in incorrect responses. We then compared subject-wise differences between the response-conditional auROCs with the differences in this simulated dataset <span class="citation">(Mazor, Moran, &amp; Fleming, 2021)</span>. The difference in differences was significant, indicating that the observed metacognitive asymmetry could not be accounted for by a first-order equal-variance SDT model (<span class="math inline">\(M = 0.07\)</span>, 95% CI <span class="math inline">\([0.01\)</span>, <span class="math inline">\(0.14]\)</span>, <span class="math inline">\(t(8) = 2.49\)</span>, <span class="math inline">\(p = .037\)</span>).</p>
</div>
<div id="zroc-curves" class="section level4">
<h4><span class="header-section-number">3.1.3.4</span> zROC curves</h4>
<p>An asymmetry in metacognitive sensitivity for ‘yes’ and ‘no’ responses is predicted by unequal-variance Signal Detection Theory (<em>uvSDT</em>). Specifically, if the ‘signal’ distribution is wider than the ‘noise’ distribution, the overlap between the distributions will be more pronounced for misses and correct rejections than for hits and false alarms, making metacognitive judgments for ‘no’ responses objectively more difficult. Furthermore, uvSDT predicts that plotting the type-1 ROC curve in z-space (taking the inverse cumulative distribution of the confidence rating histogram) will reveal a straight line with a slope equal to <span class="math inline">\(\frac{\sigma_{noise}}{\sigma_{signal}}\)</span>.</p>
<p>We used linear regression to estimate the slope of the zROC curve. However, linear regression systematically underestimates the regression slope in case of any deviation from a perfect fit, due to regression to the mean <span class="citation">(Wickens, 2002, p. 56)</span>. To control for this, we fitted two regression models for the task data of each participant: one predicting <span class="math inline">\(Z(h)\)</span> based on <span class="math inline">\(Z(f)\)</span> (yielding slope <span class="math inline">\(s_1\)</span>) and one predicting <span class="math inline">\(Z(f)\)</span> based on <span class="math inline">\(Z(h)\)</span> (yielding slope <span class="math inline">\(s_2\)</span>). We then used <span class="math inline">\(\frac{log(s_1)-log(s2)}{2}\)</span> as a bias-free measure of the zROC slope. In equal-variance SDT, this value is predicted to be 0, corresponding to a slope of 1.</p>
<p>Indeed, slopes were generally shallow for detection zROC curves (as predicted by an unequal-variance SDT model; <span class="math inline">\(M = -0.12\)</span>, 95% CI <span class="math inline">\([-0.23\)</span>, <span class="math inline">\(-0.02]\)</span>, <span class="math inline">\(t(8) = -2.69\)</span>, <span class="math inline">\(p = .028\)</span>), and not significantly different from 1 for discrimination zROC curves (as predicted by equal-variance SDT; <span class="math inline">\(M = 0.02\)</span>, 95% CI <span class="math inline">\([-0.09\)</span>, <span class="math inline">\(0.12]\)</span>, <span class="math inline">\(t(8) = 0.36\)</span>, <span class="math inline">\(p = .729\)</span>).</p>
<p>These results support a difference in the variance-structure of the representation of signal and noise, such that the representation of signal is more varied across trials. However, it is still possible that some of the metacognitive asymmetry in detection (the difference in auROC between ‘yes’ and ‘no’ responses) reflects additional higher-order processes that cannot be captured by a first-order signal-detection model. If this was the case, zROC curves for detection should not only be more shallow, but also less linear than for discrimination, reflecting poorer fit of the signal-detection model to detection. In order to test if this was the case, we compared the subject-wise <span class="math inline">\(R^2\)</span> values for the detection and discrimination zROC slopes. <span class="math inline">\(R^2\)</span> values reflect the goodness of fit of a linear model to the data. These values were similar for the two tasks (<span class="math inline">\(M_d = -0.01\)</span>, 95% CI <span class="math inline">\([-0.03\)</span>, <span class="math inline">\(0.00]\)</span>, <span class="math inline">\(t(8) = -1.91\)</span>, <span class="math inline">\(p = .093\)</span>). This suggests that a first-order SDT model can account equally well for behaviour in the two tasks.</p>
</div>
<div id="reverse-correlation" class="section level4">
<h4><span class="header-section-number">3.1.3.5</span> Reverse Correlation</h4>
<p>Stimuli in Exp. 1 randomly varied in the proportion of dots that moved coherently and in the overall direction of the randomly moving dots, both between stimuli of the same category and between timepoints of the same stimulus. To test whether participants incorporated this random variation into their decision-making process, we applied reverse correlation analysis to identify stimulus features that affect decisions and confidence estimates in detection and discrimination <span class="citation">(Zylberberg et al., 2012)</span>.</p>
<div id="discrimination-decisions" class="section level5 unnumbered">
<h5>Discrimination decisions</h5>
<div class="figure" style="text-align: center"><span id="fig:ch2Exp1rcDiscrimination"></span>
<img src="thesis_files/figure-html/ch2Exp1rcDiscrimination-1.png" alt="Decision and confidence discrimination kernels, Experiment 1. Upper left: motion energy in the chosen (green) and unchosen (purple) direction as a function of time. Upper right: a subtraction between energy in the chosen and unchosen directions. Lower left: confidence effects for motion energy in the chosen (green) and unchosen (purple) directions. Upper right: a subtraction between confidence effects in the chosen and unchosen directions. Shaded areas represent the the mean +- one standard error. The first 300 milliseconds of the trial are marked in yellow" width="672" />
<p class="caption">
Figure 3.4: Decision and confidence discrimination kernels, Experiment 1. Upper left: motion energy in the chosen (green) and unchosen (purple) direction as a function of time. Upper right: a subtraction between energy in the chosen and unchosen directions. Lower left: confidence effects for motion energy in the chosen (green) and unchosen (purple) directions. Upper right: a subtraction between confidence effects in the chosen and unchosen directions. Shaded areas represent the the mean +- one standard error. The first 300 milliseconds of the trial are marked in yellow
</p>
</div>
<p>Subject-wise motion energy vectors in the chosen and unchosen directions were fed into a group-level analysis. This analysis quantified the effect of random fluctuations in motion energy on the probability of responding ‘right’ and ‘left’ (or ‘up’ or ‘down’), and the temporal dynamics of decision formation. Similar to the results obtained by Zylberberg et. al., participants’ decisions were sensitive to motion energy fluctuations during the first 300 milliseconds of the trial (<span class="math inline">\(t(8) = 7.74\)</span>, <span class="math inline">\(p &lt; .001\)</span>; see Fig. <a href="3-behavioural-markers-of-perceptual-detection.html#fig:ch2Exp1rcDiscrimination">3.4</a>, left panels). We note that the symmetry of the two time courses around the x axis does not by itself entail an equal contribution of similar levels of negative and positive energy to the final decision. The reverse correlation analysis is performed on demeaned motion energy vectors. Assuming no response bias, the two vectors are bound to sum to zero, and consequently should mirror each other.</p>
</div>
<div id="discrimination-confidence" class="section level5 unnumbered">
<h5>Discrimination confidence</h5>
<p>The median confidence rating in each experimental session was used to separate all motion energy vectors into four groups, according to decision (chosen or unchosen directions) and confidence level (high or low). Confidence kernels for the chosen and unchosen directions were then extracted by subtracting the mean low confidence vectors from the mean high confidence vectors for both the chosen and unchosen directions. Following Zylberberg et. al., motion energy analysis focused on the first 300 milliseconds of the trial.</p>
<p>We observed a significant effect of motion energy on confidence within this time window (<span class="math inline">\(t(17) = 2.63\)</span>, <span class="math inline">\(p = .018\)</span>; see Fig. <a href="3-behavioural-markers-of-perceptual-detection.html#fig:ch2Exp1rcDiscrimination">3.4</a>, right panels). This effect was significantly stronger for motion energy in the chosen direction, compared to the unchosen direction (<span class="math inline">\(t(8) = 2.41\)</span>, <span class="math inline">\(p = .043\)</span>). In other words, participants’ confidence ratings in the discrimination task were more sensitive to evidence that supported their decision, replicating the Positive Evidence Bias observed in <span class="citation">Zylberberg et al. (2012)</span>.</p>
</div>
<div id="detection" class="section level5 unnumbered">
<h5>Detection</h5>
<p>We next turned to the effects of motion energy on detection responses and confidence ratings. Reverse correlation for detection introduces a challenge: while ‘no’ responses reflect a belief in the absence of any coherent motion, ‘yes’ responses can result from at least two different belief states: participants can detect motion in any of the two directions, or in both. We chose to have two possible motion directions in the detection task in order to prevent participants from making ‘no’ responses based on significant motion in an unexpected direction. While this choice ensured that participants cannot trivially accumulate evidence for absence, it also made the reverse correlation analysis more difficult, as we did not have full access to participants’ beliefs about the stimulus in their ‘yes’ responses.</p>
<p>As a first approximation, we tested whether high motion energy along the relevant dimension (horizontal or vertical), regardless of direction (up/down or left/right), would increase the probability of a ‘yes’ response. The sum motion energy did not have a significant effect on participants’ responses during the first 300 milliseconds (<span class="math inline">\(t(8) = 2.44\)</span>, <span class="math inline">\(p = .041\)</span>; see Fig. <a href="3-behavioural-markers-of-perceptual-detection.html#fig:ch2Exp1rcDetectionPlot">3.5</a>, left panel) or at any other time point. The effect of sum motion energy during the first 300 milliseconds on decision confidence was marginally significant (<span class="math inline">\(t(8) = 1.95\)</span>, <span class="math inline">\(p = .086\)</span>; see Fig. <a href="3-behavioural-markers-of-perceptual-detection.html#fig:ch2Exp1rcDetectionPlot">3.5</a>, right panel).</p>
<div class="figure" style="text-align: center"><span id="fig:ch2Exp1rcDetectionPlot"></span>
<img src="thesis_files/figure-html/ch2Exp1rcDetectionPlot-1.png" alt="Decision and confidence detection kernels, Experiment 1. Upper left: overall motion energy along the relevant dimension in 'yes' (blue) and 'no' (red) responses as a function of time. Upper right: a subtraction between energy in 'yes' and 'no' responses. Lower left: confidence effects for motion energy in 'yes' and 'no' responses. Upper right: a subtraction between confidence effects 'yes' and 'no' responses. Shaded areas represent the the mean +- one standard error. The first 300 milliseconds of the trial are marked in yellow" width="672" />
<p class="caption">
Figure 3.5: Decision and confidence detection kernels, Experiment 1. Upper left: overall motion energy along the relevant dimension in ‘yes’ (blue) and ‘no’ (red) responses as a function of time. Upper right: a subtraction between energy in ‘yes’ and ‘no’ responses. Lower left: confidence effects for motion energy in ‘yes’ and ‘no’ responses. Upper right: a subtraction between confidence effects ‘yes’ and ‘no’ responses. Shaded areas represent the the mean +- one standard error. The first 300 milliseconds of the trial are marked in yellow
</p>
</div>
</div>
</div>
<div id="pseudo-discrimination-analysis" class="section level4 unnumbered">
<h4>Pseudo-discrimination analysis</h4>
<p>A failure to find significant effects of net motion energy on detection decision and confidence may be due to the fact that participants were sensitive to the perceived directionality of motion (e.g., ‘more dots are moving to the right’) rather than to the sum motion along the relevant axis. However, as we mention above, for any single trial, we cannot tell whether a ‘yes’ response means ‘I perceived coherent motion to the right’ or ‘I perceived coherent motion to the left’. As a way to approximate participants’ perception, all signal trials from the detection task were subjected to a <em>pseudo-discrimination analysis</em>. In this analysis, we assumed that in the majority of signal trials, when participants responded ‘yes’ they also identified the true direction of motion. For example, a detection ‘yes’ response in a trial in which a dots moved coherently to the right was analyzed as a pseudo-discrimination ‘right’ response. Conversely, a detection ‘no’ response in a trial in which dots moved coherently to the right was treated as a pseudo-discrimination ‘left’ response. These hypothetical responses were then used to extract decision and confidence kernels in the same manner as for the discrimination analysis.</p>
<div class="figure" style="text-align: center"><span id="fig:ch2Exp1rcPDFig"></span>
<img src="thesis_files/figure-html/ch2Exp1rcPDFig-1.png" alt="Decision and confidence pseudo-discrimination kernels, Experiment 1. Upper left: motion energy in the chosen (green) and unchosen (purple) direction as a function of time. Upper right: a subtraction between energy in the chosen and unchosen directions. Lower left: confidence effects for motion energy in the chosen (green) and unchosen (purple) directions. Upper right: a subtraction between confidence effects in the chosen and unchosen directions. Shaded areas represent the the mean +- one standard error. The first 300 milliseconds of the trial are marked in yellow" width="672" />
<p class="caption">
Figure 3.6: Decision and confidence pseudo-discrimination kernels, Experiment 1. Upper left: motion energy in the chosen (green) and unchosen (purple) direction as a function of time. Upper right: a subtraction between energy in the chosen and unchosen directions. Lower left: confidence effects for motion energy in the chosen (green) and unchosen (purple) directions. Upper right: a subtraction between confidence effects in the chosen and unchosen directions. Shaded areas represent the the mean +- one standard error. The first 300 milliseconds of the trial are marked in yellow
</p>
</div>
<p>Pseudo-discrimination decision kernels (see Fig. <a href="3-behavioural-markers-of-perceptual-detection.html#fig:ch2Exp1rcPDFig">3.6</a>, left panels) were highly similar to discrimination decision kernels (see Fig. <a href="3-behavioural-markers-of-perceptual-detection.html#fig:ch2Exp1rcDiscrimination">3.4</a>). Here also, motion energy during the first 300 milliseconds of the stimulus had a significant effect on decision (<span class="math inline">\(t(8) = 3.64\)</span>, <span class="math inline">\(p = .007\)</span>) and on decision confidence (<span class="math inline">\(t(8) = 3.25\)</span>, <span class="math inline">\(p = .012\)</span>). However, unlike discrimination, where motion energy in the chosen direction influenced decision confidence more than motion energy in the unchosen direction, no such bias was observed for detection responses (<span class="math inline">\(t(8) = 0.06\)</span>, <span class="math inline">\(p = .953\)</span>).</p>
<p>Finally, we asked whether the same stimulus features supported decision confidence in ‘yes’ and ‘no’ responses. While motion energy during the first 300 milliseconds of the trial significantly affected confidence in ‘yes’ responses (<span class="math inline">\(t(8) = 5.21\)</span>, <span class="math inline">\(p = .001\)</span>), it had no significant effect on confidence in ‘no’ responses (<span class="math inline">\(t(8) = 0.01\)</span>, <span class="math inline">\(p = .994\)</span>). However, given that the pseudo-discrimination analysis was performed on signal trials only, confidence kernels for ‘no’ responses were based on fewer trials than confidence kernels for ‘yes’ responses, such that the absence of a significant effect in ‘no’ responses may reflect insufficient statistical power to detect one.</p>
</div>
<div id="perceptual-sample-analysis" class="section level4 unnumbered">
<h4>Perceptual sample analysis</h4>
<p>According to</p>
<div class="figure" style="text-align: center"><span id="fig:ch2Exp1MEhistDisc"></span>
<img src="thesis_files/figure-html/ch2Exp1MEhistDisc-1.png" alt="Decision and confidence discrimination kernels, Experiment 1. Upper left: motion energy in the chosen (green) and unchosen (purple) direction as a function of time. Upper right: a subtraction between energy in the chosen and unchosen directions. Lower left: confidence effects for motion energy in the chosen (green) and unchosen (purple) directions. Upper right: a subtraction between confidence effects in the chosen and unchosen directions. Shaded areas represent the the mean +- one standard error. The first 300 milliseconds of the trial are marked in yellow" width="672" />
<p class="caption">
Figure 3.7: Decision and confidence discrimination kernels, Experiment 1. Upper left: motion energy in the chosen (green) and unchosen (purple) direction as a function of time. Upper right: a subtraction between energy in the chosen and unchosen directions. Lower left: confidence effects for motion energy in the chosen (green) and unchosen (purple) directions. Upper right: a subtraction between confidence effects in the chosen and unchosen directions. Shaded areas represent the the mean +- one standard error. The first 300 milliseconds of the trial are marked in yellow
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:ch2Exp1MEhistDet"></span>
<img src="thesis_files/figure-html/ch2Exp1MEhistDet-1.png" alt="Decision and confidence discrimination kernels, Experiment 1. Upper left: motion energy in the chosen (green) and unchosen (purple) direction as a function of time. Upper right: a subtraction between energy in the chosen and unchosen directions. Lower left: confidence effects for motion energy in the chosen (green) and unchosen (purple) directions. Upper right: a subtraction between confidence effects in the chosen and unchosen directions. Shaded areas represent the the mean +- one standard error. The first 300 milliseconds of the trial are marked in yellow" width="672" />
<p class="caption">
Figure 3.8: Decision and confidence discrimination kernels, Experiment 1. Upper left: motion energy in the chosen (green) and unchosen (purple) direction as a function of time. Upper right: a subtraction between energy in the chosen and unchosen directions. Lower left: confidence effects for motion energy in the chosen (green) and unchosen (purple) directions. Upper right: a subtraction between confidence effects in the chosen and unchosen directions. Shaded areas represent the the mean +- one standard error. The first 300 milliseconds of the trial are marked in yellow
</p>
</div>
</div>
</div>
<div id="discussion-exp.-1" class="section level3 unnumbered">
<h3>Discussion: Exp. 1</h3>
<p>In Exp. 1, we found that detection ‘yes’ responses are faster and are accompanied by higher subjective confidence than detection ‘no’ responses. We replicated the metacognitive asymmetry between detection ‘yes’ and ‘no’ responses as measured with response-conditional ROC curves. A first-order signal detection model provided good fits to detection and discrimination responses alike.</p>
<p>Looking at random fluctuations in the motion energy, we replicated the positive evidence bias in discrimination confidence, whereby evidence in support of a decision is given more weight in the construction of confidence than evidence against it. In detection, decision and confidence were sensitive to fluctuations in motion energy at around the same time window as in discrimination. However, unlike discrimination, confidence in detection did not show the positive evidence bias. Furthermore, confidence in detection ‘no’ responses was not affected by fluctuations in motion energy.</p>
<div class="figure" style="text-align: center"><span id="fig:e2hists"></span>
<img src="thesis_files/figure-html/e2hists-1.png" alt="Response conditional ROC curves for the two discrimination responses in Exp. 1. The area under the curve is a measure of metacognitive sensitivity. Bottom right inset: distributions of the area under the curve for the two responses, across participants. Overall, participants had lower metacognitive insight into the accuracy of their 'O' responses. Error bars stand for the standard error of the mean. For illustration, the response-conditional ROC curves of the first 20 participants are included." width="672" />
<p class="caption">
Figure 3.9: Response conditional ROC curves for the two discrimination responses in Exp. 1. The area under the curve is a measure of metacognitive sensitivity. Bottom right inset: distributions of the area under the curve for the two responses, across participants. Overall, participants had lower metacognitive insight into the accuracy of their ‘O’ responses. Error bars stand for the standard error of the mean. For illustration, the response-conditional ROC curves of the first 20 participants are included.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:e2ROC"></span>
<img src="thesis_files/figure-html/e2ROC-1.png" alt="Response conditional ROC curves for the two discrimination responses in Exp. 1. The area under the curve is a measure of metacognitive sensitivity. Bottom right inset: distributions of the area under the curve for the two responses, across participants. Overall, participants had lower metacognitive insight into the accuracy of their 'O' responses. Error bars stand for the standard error of the mean. For illustration, the response-conditional ROC curves of the first 20 participants are included." width="672" />
<p class="caption">
Figure 3.10: Response conditional ROC curves for the two discrimination responses in Exp. 1. The area under the curve is a measure of metacognitive sensitivity. Bottom right inset: distributions of the area under the curve for the two responses, across participants. Overall, participants had lower metacognitive insight into the accuracy of their ‘O’ responses. Error bars stand for the standard error of the mean. For illustration, the response-conditional ROC curves of the first 20 participants are included.
</p>
</div>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>We reused the original Matlab code that was used for Experiment 1 in Zylberberg et. al. (2012), kindly shared by Ariel Zylberberg. <a href="3-behavioural-markers-of-perceptual-detection.html#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="2-ch-search.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["thesis.pdf", "PDF"], ["thesis.epub", "EPUB"], ["thesis.docx", "Word"]],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
