<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Prospective search time estimates for unseen displays reveal a rich intuitive theory of visual search | Self-Modeling in Inference about Absence</title>
  <meta name="description" content="Chapter 5 Prospective search time estimates for unseen displays reveal a rich intuitive theory of visual search | Self-Modeling in Inference about Absence" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Prospective search time estimates for unseen displays reveal a rich intuitive theory of visual search | Self-Modeling in Inference about Absence" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Prospective search time estimates for unseen displays reveal a rich intuitive theory of visual search | Self-Modeling in Inference about Absence" />
  
  
  

<meta name="author" content="Matan Mazor" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"/>
<link rel="next" href="6-app1-SDT.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"></a></li>
<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> This will automatically install the {remotes} package and {thesisdown}</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#inference-about-absence"><i class="fa fa-check"></i><b>1.1</b> Inference about absence</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#formalabsence"><i class="fa fa-check"></i><b>1.2</b> Probabilistic reasoning, criterion setting, and self knowledge</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#symmetrical-definition"><i class="fa fa-check"></i>Symmetrical definition:</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dissymmetrical-definition"><i class="fa fa-check"></i>Dissymmetrical definition:</a></li>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#second-order-cognition"><i class="fa fa-check"></i><b>1.2.1</b> Second-order cognition</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#detectionmodels"><i class="fa fa-check"></i><b>1.2.2</b> Computational models of detection</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#htm"><i class="fa fa-check"></i>The High-Threshold model</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#sdt"><i class="fa fa-check"></i>Signal Detection Theory</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#detection-i-would-have-noticed-it"><i class="fa fa-check"></i><b>1.3</b> Detection: “I would have noticed it”</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#intro:search"><i class="fa fa-check"></i><b>1.4</b> Visual search: “I would have found it”</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#memory-i-would-have-remembered-it"><i class="fa fa-check"></i><b>1.5</b> Memory: “I would have remembered it”</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#the-development-of-a-self-model"><i class="fa fa-check"></i><b>1.6</b> The development of a self-model</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#this-thesis"><i class="fa fa-check"></i><b>1.7</b> This thesis</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-ch-search.html"><a href="2-ch-search.html"><i class="fa fa-check"></i><b>2</b> Zero-shot search termination reveals a dissociation between implicit and explicit metacognitive knowledge</a><ul>
<li class="chapter" data-level="2.1" data-path="2-ch-search.html"><a href="2-ch-search.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="2-ch-search.html"><a href="2-ch-search.html#experiment-1"><i class="fa fa-check"></i><b>2.2</b> Experiment 1</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-ch-search.html"><a href="2-ch-search.html#participants"><i class="fa fa-check"></i><b>2.2.1</b> Participants</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-ch-search.html"><a href="2-ch-search.html#procedure"><i class="fa fa-check"></i><b>2.2.2</b> Procedure</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-ch-search.html"><a href="2-ch-search.html#data-analysis"><i class="fa fa-check"></i><b>2.2.3</b> Data analysis</a></li>
<li class="chapter" data-level="2.2.4" data-path="2-ch-search.html"><a href="2-ch-search.html#results"><i class="fa fa-check"></i><b>2.2.4</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-ch-search.html"><a href="2-ch-search.html#experiment-2"><i class="fa fa-check"></i><b>2.3</b> Experiment 2</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-ch-search.html"><a href="2-ch-search.html#participants-1"><i class="fa fa-check"></i><b>2.3.1</b> Participants</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-ch-search.html"><a href="2-ch-search.html#procedure-1"><i class="fa fa-check"></i><b>2.3.2</b> Procedure</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-ch-search.html"><a href="2-ch-search.html#results-1"><i class="fa fa-check"></i><b>2.3.3</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-ch-search.html"><a href="2-ch-search.html#discussion"><i class="fa fa-check"></i><b>2.4</b> Discussion</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-ch-search.html"><a href="2-ch-search.html#is-implicit-metacognitive-knowledge-metacognitive"><i class="fa fa-check"></i><b>2.4.1</b> Is implicit metacognitive knowledge metacognitive?</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-ch-search.html"><a href="2-ch-search.html#inference-about-absence-as-a-tool-for-studying-implicit-self-knowledge"><i class="fa fa-check"></i><b>2.4.2</b> Inference about absence as a tool for studying implicit self knowledge</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-ch-search.html"><a href="2-ch-search.html#conclusion"><i class="fa fa-check"></i><b>2.4.3</b> Conclusion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-ch-RC.html"><a href="3-ch-RC.html"><i class="fa fa-check"></i><b>3</b> Paradoxical evidence weightings in confidence judgments for detection and discrimination</a><ul>
<li class="chapter" data-level="3.1" data-path="3-ch-RC.html"><a href="3-ch-RC.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="3-ch-RC.html"><a href="3-ch-RC.html#experiment-1-1"><i class="fa fa-check"></i><b>3.2</b> Experiment 1</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-ch-RC.html"><a href="3-ch-RC.html#methods"><i class="fa fa-check"></i><b>3.2.1</b> Methods</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-ch-RC.html"><a href="3-ch-RC.html#analysis"><i class="fa fa-check"></i><b>3.2.2</b> Analysis</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-ch-RC.html"><a href="3-ch-RC.html#results-2"><i class="fa fa-check"></i><b>3.2.3</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-ch-RC.html"><a href="3-ch-RC.html#experiment-2-1"><i class="fa fa-check"></i><b>3.3</b> Experiment 2</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-ch-RC.html"><a href="3-ch-RC.html#methods-1"><i class="fa fa-check"></i><b>3.3.1</b> Methods</a></li>
<li class="chapter" data-level="3.3.2" data-path="3-ch-RC.html"><a href="3-ch-RC.html#results-3"><i class="fa fa-check"></i><b>3.3.2</b> Results</a></li>
<li class="chapter" data-level="3.3.3" data-path="3-ch-RC.html"><a href="3-ch-RC.html#detection-signal-trials-1"><i class="fa fa-check"></i><b>3.3.3</b> Detection signal trials</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-ch-RC.html"><a href="3-ch-RC.html#discussion-1"><i class="fa fa-check"></i><b>3.4</b> Discussion</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-ch-RC.html"><a href="3-ch-RC.html#model-1-a-rational-agent-symmetric-evidence-structure"><i class="fa fa-check"></i><b>3.4.1</b> Model 1: a rational agent + symmetric evidence structure</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-ch-RC.html"><a href="3-ch-RC.html#model-2-a-rational-agent-symmetric-evidence-structure"><i class="fa fa-check"></i><b>3.4.2</b> Model 2: a rational agent + symmetric evidence structure</a></li>
<li class="chapter" data-level="3.4.3" data-path="3-ch-RC.html"><a href="3-ch-RC.html#model-3-confidence-decision-cross"><i class="fa fa-check"></i><b>3.4.3</b> Model 3: confidence decision cross</a></li>
<li class="chapter" data-level="3.4.4" data-path="3-ch-RC.html"><a href="3-ch-RC.html#evidence-for-absence"><i class="fa fa-check"></i><b>3.4.4</b> Evidence for absence</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><i class="fa fa-check"></i><b>4</b> Distinct neural contributions to metacognition for detecting (but not discriminating) visual stimuli</a><ul>
<li class="chapter" data-level="4.1" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#introduction-2"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#methods-and-materials"><i class="fa fa-check"></i><b>4.2</b> Methods and Materials</a><ul>
<li class="chapter" data-level="4.2.1" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#participants-4"><i class="fa fa-check"></i><b>4.2.1</b> Participants</a></li>
<li class="chapter" data-level="4.2.2" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#design-and-procedure"><i class="fa fa-check"></i><b>4.2.2</b> Design and procedure</a></li>
<li class="chapter" data-level="4.2.3" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#scanning-parameters"><i class="fa fa-check"></i><b>4.2.3</b> Scanning parameters</a></li>
<li class="chapter" data-level="4.2.4" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#analysis-1"><i class="fa fa-check"></i><b>4.2.4</b> Analysis</a></li>
<li class="chapter" data-level="4.2.5" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#exclusion-criteria"><i class="fa fa-check"></i><b>4.2.5</b> Exclusion criteria</a></li>
<li class="chapter" data-level="4.2.6" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#response-conditional-type-ii-roc-curves"><i class="fa fa-check"></i><b>4.2.6</b> Response conditional type-II ROC curves</a></li>
<li class="chapter" data-level="4.2.7" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#imaging-analysis"><i class="fa fa-check"></i><b>4.2.7</b> Imaging analysis</a></li>
<li class="chapter" data-level="4.2.8" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#statistical-inference-1"><i class="fa fa-check"></i><b>4.2.8</b> Statistical inference</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#results-4"><i class="fa fa-check"></i><b>4.3</b> Results</a></li>
<li class="chapter" data-level="4.4" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#behavioural-results"><i class="fa fa-check"></i><b>4.4</b> Behavioural results</a><ul>
<li class="chapter" data-level="4.4.1" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#imaging-results"><i class="fa fa-check"></i><b>4.4.1</b> Imaging results</a></li>
<li class="chapter" data-level="4.4.2" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#computational-models"><i class="fa fa-check"></i><b>4.4.2</b> Computational models</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#discussion-2"><i class="fa fa-check"></i><b>4.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-ch-MVS.html"><a href="5-ch-MVS.html"><i class="fa fa-check"></i><b>5</b> Prospective search time estimates for unseen displays reveal a rich intuitive theory of visual search</a><ul>
<li class="chapter" data-level="5.1" data-path="5-ch-MVS.html"><a href="5-ch-MVS.html#introduction-3"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="5-ch-MVS.html"><a href="5-ch-MVS.html#experiments-1-and-2-shape-orientation-and-color"><i class="fa fa-check"></i><b>5.2</b> Experiments 1 and 2: shape, orientation, and color</a><ul>
<li class="chapter" data-level="5.2.1" data-path="5-ch-MVS.html"><a href="5-ch-MVS.html#participants-5"><i class="fa fa-check"></i><b>5.2.1</b> Participants</a></li>
<li class="chapter" data-level="5.2.2" data-path="5-ch-MVS.html"><a href="5-ch-MVS.html#procedure-2"><i class="fa fa-check"></i><b>5.2.2</b> Procedure</a></li>
<li class="chapter" data-level="5.2.3" data-path="5-ch-MVS.html"><a href="5-ch-MVS.html#results-5"><i class="fa fa-check"></i><b>5.2.3</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-ch-MVS.html"><a href="5-ch-MVS.html#experiments-3-and-4-complex-unfamiliar-stimuli"><i class="fa fa-check"></i><b>5.3</b> Experiments 3 and 4: complex, unfamiliar stimuli</a><ul>
<li class="chapter" data-level="5.3.1" data-path="5-ch-MVS.html"><a href="5-ch-MVS.html#participants-6"><i class="fa fa-check"></i><b>5.3.1</b> Participants</a></li>
<li class="chapter" data-level="5.3.2" data-path="5-ch-MVS.html"><a href="5-ch-MVS.html#procedure-3"><i class="fa fa-check"></i><b>5.3.2</b> Procedure</a></li>
<li class="chapter" data-level="5.3.3" data-path="5-ch-MVS.html"><a href="5-ch-MVS.html#results-6"><i class="fa fa-check"></i><b>5.3.3</b> Results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html"><i class="fa fa-check"></i><b>6</b> Signal Detection Theory</a><ul>
<li class="chapter" data-level="6.1" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app1:ROC"><i class="fa fa-check"></i><b>6.1</b> ROC and zROC curves</a></li>
<li class="chapter" data-level="6.2" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app1:uvSDT"><i class="fa fa-check"></i><b>6.2</b> Unequal-variance (uv) SDT</a></li>
<li class="chapter" data-level="6.3" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app1:mc"><i class="fa fa-check"></i><b>6.3</b> SDT Measures for Metacognition</a></li>
<li class="chapter" data-level="6.4" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app2:PDRC"><i class="fa fa-check"></i><b>6.4</b> Pseudo-discrimination analysis</a><ul>
<li class="chapter" data-level="6.4.1" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#exp.-1"><i class="fa fa-check"></i><b>6.4.1</b> Exp. 1</a></li>
<li class="chapter" data-level="6.4.2" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#exp.-2"><i class="fa fa-check"></i><b>6.4.2</b> Exp. 2</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app2:simulation"><i class="fa fa-check"></i><b>6.5</b> Unequal-variance model</a><ul>
<li class="chapter" data-level="6.5.1" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#discrimination"><i class="fa fa-check"></i><b>6.5.1</b> Discrimination</a></li>
<li class="chapter" data-level="6.5.2" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#detection-1"><i class="fa fa-check"></i><b>6.5.2</b> Detection</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app3:buttonpresses"><i class="fa fa-check"></i><b>6.6</b> Confidence button presses</a></li>
<li class="chapter" data-level="6.7" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app3:zROC"><i class="fa fa-check"></i><b>6.7</b> zROC curves</a></li>
<li class="chapter" data-level="6.8" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app3:GC-DM"><i class="fa fa-check"></i><b>6.8</b> Global confidence design matrix</a></li>
<li class="chapter" data-level="6.9" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app3:ROIconf"><i class="fa fa-check"></i><b>6.9</b> Effect of confidence in our pre-specified ROIs</a></li>
<li class="chapter" data-level="6.10" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app3:varianceRat"><i class="fa fa-check"></i><b>6.10</b> SDT variance ratio correlation with the quadratic confidence effect</a></li>
<li class="chapter" data-level="6.11" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app3:efficiency"><i class="fa fa-check"></i><b>6.11</b> Correlation of metacognitive efficiency with linear and quadratic confidence effects</a></li>
<li class="chapter" data-level="6.12" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app3:cross"><i class="fa fa-check"></i><b>6.12</b> Confidence-decision cross classification</a></li>
<li class="chapter" data-level="6.13" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app3:SDT"><i class="fa fa-check"></i><b>6.13</b> Static Signal Detection Theory</a><ul>
<li class="chapter" data-level="6.13.1" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#discrimination-1"><i class="fa fa-check"></i><b>6.13.1</b> Discrimination</a></li>
<li class="chapter" data-level="6.13.2" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#detection-2"><i class="fa fa-check"></i><b>6.13.2</b> Detection</a></li>
</ul></li>
<li class="chapter" data-level="6.14" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app3:Dynamic"><i class="fa fa-check"></i><b>6.14</b> Dynamic Criterion</a><ul>
<li class="chapter" data-level="6.14.1" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#discrimination-2"><i class="fa fa-check"></i><b>6.14.1</b> Discrimination</a></li>
<li class="chapter" data-level="6.14.2" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#detection-3"><i class="fa fa-check"></i><b>6.14.2</b> Detection</a></li>
</ul></li>
<li class="chapter" data-level="6.15" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app3:Monitoring"><i class="fa fa-check"></i><b>6.15</b> Attention Monitoring</a><ul>
<li class="chapter" data-level="6.15.1" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#discrimination-3"><i class="fa fa-check"></i><b>6.15.1</b> Discrimination</a></li>
<li class="chapter" data-level="6.15.2" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#detection-4"><i class="fa fa-check"></i><b>6.15.2</b> Detection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html"><i class="fa fa-check"></i>General Discussion</a><ul>
<li class="chapter" data-level="6.16" data-path="general-discussion.html"><a href="general-discussion.html#summary-of-results"><i class="fa fa-check"></i><b>6.16</b> Summary of results</a></li>
<li class="chapter" data-level="6.17" data-path="general-discussion.html"><a href="general-discussion.html#inference-about-absence-without-self-knowledge"><i class="fa fa-check"></i><b>6.17</b> Inference about absence without self-knowledge?</a></li>
<li class="chapter" data-level="6.18" data-path="general-discussion.html"><a href="general-discussion.html#future-directions"><i class="fa fa-check"></i><b>6.18</b> Future directions</a><ul>
<li class="chapter" data-level="6.18.1" data-path="general-discussion.html"><a href="general-discussion.html#failures-of-a-self-model"><i class="fa fa-check"></i><b>6.18.1</b> Failures of a self-model</a></li>
</ul></li>
<li class="chapter" data-level="6.19" data-path="general-discussion.html"><a href="general-discussion.html#conclusion-1"><i class="fa fa-check"></i><b>6.19</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Self-Modeling in Inference about Absence</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch:MVS" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Prospective search time estimates for unseen displays reveal a rich intuitive theory of visual search</h1>
<div id="matan-mazor-max-siegel-joshua-b.-tenenbaum" class="section level4 unnumbered">
<h4>Matan Mazor, Max Siegel &amp; Joshua B. Tenenbaum</h4>
<p>abstract</p>
</div>
<div id="introduction-3" class="section level2">
<h2><span class="header-section-number">5.1</span> Introduction</h2>
<p>The <em>Intuitive Theories</em> approach to cognitive science <span class="citation">(Gerstenberg &amp; Tenenbaum, 2017)</span> has been successful in accounting for human knowledge and reasoning in the domains of physics <span class="citation">(McCloskey, 1983)</span>, psychology <span class="citation">(Baker, Saxe, &amp; Tenenbaum, 2011)</span> and semantic knowledge <span class="citation">(Gelman &amp; Legare, 2011)</span>. In recent years, careful experimental and computational work has advanced our understanding of these simplified theories: their ontologies and causal laws, the abstractions that they make, and the consequences of these abstractions for faithfully and efficiently modeling the real world. For example, the computational specification of the intuitive physics model and its deviation from Newtonian physics was informed by empirical measures of biased intuitions about the consequences of object collisions <span class="citation">(Sanborn, Mansinghka, &amp; Griffiths, 2013; Smith &amp; Vul, 2013)</span>.</p>
<p>Theoretically, there is no reason to believe that Intuitive Theories should be limited in their scope to modeling the external environment and other agents. Indeed, agents may benefit from having an intuitive theory or a simplified model of their own perceptual, cognitive and psychological states. For example, in the context of memory, it has been suggested that knowing which items are more subjectively memorable is useful for making negative recognition judgments <span class="citation">(“I would have remembered this object if I saw it”; Brown, Lewis, &amp; Monk, 1977)</span>, and self-modeling has been proposed to play an important role in inference about absence more broadly <span class="citation">(Mazor, n.d.)</span>. In the context of perception and attention, <span class="citation">Graziano &amp; Webb (2015)</span> argued that having a simplified <em>Attention Schema</em> (an intuitive theory of attention and its dynamics) is crucial for monitoring and controlling one’s attention, similar to how a body-schema supports motor control.</p>
<p>Still, little experimental work has been devoted to characterizing the computational specifications of this intuitive theory of attention. Is it based on a simulation engine <span class="citation">(similar to the game engine proposal; Ullman, Spelke, Battaglia, &amp; Tenenbaum, 2017)</span>? Or instead formatted as a list of propositions (e.g., <em>‘My attention span is shorter when I am tired’</em>)? How accurate is it? To what extent is it learned from experience and what inductive biases guide its acquisition and tuning based on experience?</p>
<p>Here we take a first step in this direction, using visual search as our model test-case. Participants estimated their prospective search times in visual search tasks and then performed the same searches. Similar to using colliding balls and falling blocks to study intuitive physics, here we chose visual search for being thoroughly studied and amenable to relatively simple modeling. In Experiments 1 and 2, we used simple colorful shapes as our stimuli, and compared participants’ intuitive theories to scientific theories of attention that distinguish parallel from serial processing. We found that participants were sensitive to the parallel/serial distinction, but had a persistent bias to assume serial search. In experiments 3 and 4 we used unfamiliar stimuli from the Omniglot dataset <span class="citation">(Lake, Salakhutdinov, Gross, &amp; Tenenbaum, 2011)</span> to demonstrate the richness and compositional nature of participants’ intuitive theories, and their reliance of idiosyncratic knowledge.</p>
</div>
<div id="experiments-1-and-2-shape-orientation-and-color" class="section level2">
<h2><span class="header-section-number">5.2</span> Experiments 1 and 2: shape, orientation, and color</h2>
<p>A good intuitive theory needs to have good predictive value without being overly complex. A reasonable first candidate for what an intuitive theory of visual search may look like is Anne Treisman’s <em>Feature Integration Theory</em> (FIT). According to FIT, visual search comprises two stages: a <em>pre-attentive</em> parallel stage, and a serial <em>focused attention</em> stage <span class="citation">(Treisman, 1986; Treisman &amp; Sato, 1990)</span>. In the first stage, visual features (such as color, orientation, and intensity) are extracted from the display to generate spatial ‘feature maps’. Search targets that are defined by a single feature with respect to its surroundings (<em>feature search</em>; for example searching for a red car in a road full of yellow taxis) can be located based on the feature map. Since the extraction of the feature map is pre-attentive, in these cases the search can be completed immediately. However, sometimes the target can only be identified by integrating over multiple features (<em>conjunction search</em>; for example if the road has not only yellow taxis, but also red buses). In such cases, attention must be serially deployed to items in the display until the target is identified.</p>
<p>In its simplest form, Treisman’s FIT predicts that search time should linearly scale with the number of distractors in conjunction search, but not in feature searches. This model provides reasonably accurate search time predictions for simple displays with only three parameters: non-decision time (the y-intercept of the set size X search time curve), the time cost of deploying attention to an item (the slope of the same curve), and a list of the features that can be found without serially scanning the display (for example, color, orientation, and size).</p>
<p>In Experiments 1 and 2 we used stimuli that lend themselves to a categorical distinction between parallel and serial search: simple geometric shapes of different colors and orientations. We asked whether participants’ intuitive theory of visual search can predict which search displays demand serial deployment of attention and which don’t. Critically, participants gave their search time estimates before they were asked to perform searches involving these or similar stimuli, so their search time estimates reflected prior beliefs about search efficiency. Our hypotheses and analysis plan for Experiment 2, based on the results of Experiment 1, were pre-registered prior to data collection (pre-registration document: <a href="osf.io/2dpq9">osf.io/2dpq9</a>).</p>
<div id="participants-5" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Participants</h3>
<p>For Exp. 1, 100 participants were recruited from Amazon’s crowdsourcing web-service Mechanical Turk. Exp. 1 took about 20
minutes to complete. Each participant was paid $2.50. The highest performing 30% of participants received an additional bonus of $1.50. For Exp. 2, 100 participants were recruited from the Prolific crowdsourcing web-service. The experiment took about 15
minutes to complete. Each participant was paid £1.5. The highest performing 30% of participants received an additional bonus of £1.</p>
</div>
<div id="procedure-2" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Procedure</h3>
<p>The study was built using the Lab.js platform <span class="citation">(Henninger, Shevchenko, Mertens, Kieslich, &amp; Hilbig, 2019)</span> and hosted on a JATOS server <span class="citation">(Lange, Kühn, &amp; Filevich, 2015)</span>.</p>
<div id="familiarization" class="section level4 unnumbered">
<h4>Familiarization</h4>
<p>First, participants were acquainted with the visual search task. The instructions for this part were as follows:</p>
<blockquote>
<p>In the first part, you will find a target hidden among distractors. First, a gray cross will appear on the screen. Look at the cross. Then, the target and distractors will appear. When you spot the target, press the spacebar as quickly as possible. Upon pressing the spacebar, the target and distractors will be replaced by up to 5 numbers. To move to the next trial, type in the number that replaced the target.</p>
</blockquote>
<p>The instructions were followed by four trials of an example visual search task (searching for a <em>T</em> among 7 <em>L</em>s). Feedback was delivered on speed and accuracy. The purpose of this part of the experiment was to familiarize participants with the task.</p>
</div>
<div id="estimation" class="section level4 unnumbered">
<h4>Estimation</h4>
<p>After familiarization, participants estimated how long it would take them to perform various visual search tasks involving novel stimuli and various set sizes. On each trial, they were presented with a target stimulus and a display of distractors and were asked to estimate how long it would take to find the target if it was hidden among the distractors (see Fig. <a href="5-ch-MVS.html#fig:ch4-methods1">5.1</a>).</p>
<p>To motivate accurate estimates, we explained that these visual search tasks will be performed in the last part of the experiment, and that bonus points will be awarded for trials in which participants respond as fast or faster than their estimation. The number of points awarded for a successful search changed as a function of the search time estimate according to the rule
<span class="math inline">\(points=\frac{1}{\sqrt{secs}}\)</span>. This rule was chosen for being exponential with respect to the log response times, incentivizing participants to be consistent in their ratings across short and long search tasks. The report scale ranged from 0.1 to 4 seconds in Exp. 1 and to 2 seconds in Exp. 2.</p>
<div class="figure" style="text-align: center"><span id="fig:ch4-methods1"></span>
<img src="figure/ch4/methods1.png" alt="Experimental design. Participants first performed five similar visual search trials and received feedback about their speed and accuracy. Then, they were asked to estimate the duration of novel visual search tasks. Bonus points were awarded for accurate estimates, and more points were awarded for risky estimates. Finally, in the visual search part participants performed three consecutive trials of each visual search task for which they gave a search time estimates. Right panels: stimuli used for Experiments 1 and 2." width="100%" />
<p class="caption">
Figure 5.1: Experimental design. Participants first performed five similar visual search trials and received feedback about their speed and accuracy. Then, they were asked to estimate the duration of novel visual search tasks. Bonus points were awarded for accurate estimates, and more points were awarded for risky estimates. Finally, in the visual search part participants performed three consecutive trials of each visual search task for which they gave a search time estimates. Right panels: stimuli used for Experiments 1 and 2.
</p>
</div>
<p>After one practice trial (estimating search time for finding one <em>T</em> among 3 randomly positioned <em>L</em>s), we turned to our stimuli of interest. In Experiment 1, participants estimated how long it would take them to find a red (#FF5733) square among green (#16A085) squares (color condition), red circles (shape condition) and a mix of green squares, red circles, and green circles (shape-color conjunction condition), for set sizes 1, 5, 15 and 30. Together, participants estimated the expected search time of 12 different search tasks (see Figure <a href="5-ch-MVS.html#fig:ch4-methods1">5.1</a>, upper right panel). In Experiment 2, participants rated how long it would take them to find a red (#FF5733) tilted bar (20° off vertical) among green (#16A085) titled bars (color condition), red vertical bars (orientation condition) and a mix of green tilted and red vertical bars (oriention-color conjunction condition) for set sizes 2, 4, and 8. Together, participants estimated the expected search time of 9 different search tasks see Figure <a href="5-ch-MVS.html#fig:ch4-methods1">5.1</a>, lower right panel). In both experiments, the order of estimation trials was randomized between participants.</p>
</div>
<div id="visual-search" class="section level4 unnumbered">
<h4>Visual Search</h4>
<p>Participants performed three consecutive search tasks for each of the 12 (Exp. 1) or 9 (Exp. 2) search types. The order of presentation was randomized between participants. No feedback was delivered about speed. To motivate accurate responses, error trials were followed by a 5 second pause.</p>
</div>
</div>
<div id="results-5" class="section level3">
<h3><span class="header-section-number">5.2.3</span> Results</h3>
<p>Accuracy in the visual search task was reasonably high in both Experiments (Exp. 1: <span class="math inline">\(M = 0.93\)</span>, 95% CI <span class="math inline">\([0.90\)</span>, <span class="math inline">\(0.96]\)</span>; Exp. 2: <span class="math inline">\(M = 0.82\)</span>, 95% CI <span class="math inline">\([0.77\)</span>, <span class="math inline">\(0.87]\)</span>). Error trials and visual search trials that took shorter than 0.2 seconds or longer than 5 seconds were excluded from all further analysis. Participants were excluded if more than 30% of their trials were excluded based on the aforementioned criteria, leaving 89 and 74 participants for the main analysis of Experiments 1 and 2, respectively.</p>
<div id="search-times" class="section level4 unnumbered">
<h4>Search times</h4>
<p>For each participant and distractor type, we extracted the slope of the function relating RT to distractor set size. As expected, search slopes for color search were not significantly different than zero in Exp. 1 (-0.40 ms/item; <span class="math inline">\(t(88) = -0.45\)</span>, <span class="math inline">\(p = .652\)</span>, <span class="math inline">\(\mathrm{BF}_{\textrm{01}} = 7.74\)</span>) and Exp. 2 (0.51 ms/item; <span class="math inline">\(t(73) = 0.07\)</span>, <span class="math inline">\(p = .946\)</span>, <span class="math inline">\(\mathrm{BF}_{\textrm{01}} = 7.80\)</span>). This is consistent with color being a basic feature that is not dependent on serial attention for its extraction by the visual system <span class="citation">(Treisman, 1986; Treisman &amp; Sato, 1990)</span>. The slope for shape search was close, but significantly higher than zero (5.66 ms/item; <span class="math inline">\(t(88) = 4.35\)</span>, <span class="math inline">\(p &lt; .001\)</span>), and the slope for orientation was numerically higher than zero (11.05 ms/item) but not significantly so (<span class="math inline">\(t(73) = 1.50\)</span>, <span class="math inline">\(p = .139\)</span>, <span class="math inline">\(\mathrm{BF}_{\textrm{01}} = 2.70\)</span>). In both Experiments, conjunction search gave rise to search slopes significantly higher than zero (Exp. 1: 14.80 ms/item (<span class="math inline">\(t(88) = 9.16\)</span>, <span class="math inline">\(p &lt; .001\)</span>; Exp. 2: 72.14 ms/item (<span class="math inline">\(t(73) = 7.50\)</span>, <span class="math inline">\(p &lt; .001\)</span>; see Figure , upper panel). This is consistent with the FIT prediction that conjunction search demands serial attention.
<!-- A mixed effects linear regression model (fixed effects of search-type, set size, and the interaction between them, and random intercept and set size effects) yielded  significant coefficients for all three random effects.  --></p>
</div>
<div id="estimation-accuracy" class="section level4 unnumbered">
<h4>Estimation accuracy</h4>
<p>We next turned to analyze participants’ prospective search time estimates, and their alignment with actual search times. In both tasks, participants generally overestimated their search times. This was the case for all search types across the two Experiments (see Figure <a href="5-ch-MVS.html#fig:ch4-e1-e2-slopes">5.2</a>, left panels: all markers are above the dashed <span class="math inline">\(x=y\)</span> diagonal). Despite this bias, estimates were correlated with true search times, supporting a metacognitive insight into visual search behaviour (within subject Spearman correlations, Exp. 1: <span class="math inline">\(M = 0.28\)</span>, 95% CI <span class="math inline">\([0.21\)</span>, <span class="math inline">\(0.35]\)</span>, <span class="math inline">\(t(88) = 7.77\)</span>, <span class="math inline">\(p &lt; .001\)</span>; Exp 2: <span class="math inline">\(M = 0.16\)</span>, 95% CI <span class="math inline">\([0.07\)</span>, <span class="math inline">\(0.26]\)</span>, <span class="math inline">\(t(73) = 3.48\)</span>, <span class="math inline">\(p = .001\)</span>).</p>
<p>To test participants’ intuitive theory of visual search, we analyzed participants’ estimates as if they were search times, and extracted search slopes relating estimates to the number of distractors in the display. Estimation slopes (expected ms/item) were steeper than search slopes for all search types. In particular, although search time for a deviant color was unaffected by the number of distractors, participants estimated that color searches with more distractors should take longer (mean estimated slope in Exp. 1: 17.76 ms/item; <span class="math inline">\(t(88) = 6.35\)</span>, <span class="math inline">\(p &lt; .001\)</span>; in Exp 2: 29.43 ms/item; <span class="math inline">\(t(73) = 2.63\)</span>, <span class="math inline">\(p = .010\)</span>). In other words, at the group level, participants showed no metacognitive insight into the parallel nature of color search. Still, in both Experiments estimated slopes for color search were significantly shallower than for conjunction search (Exp. 1: <span class="math inline">\(t(88) = 4.08\)</span>, <span class="math inline">\(p &lt; .001\)</span>, Exp. 2: <span class="math inline">\(t(73) = 3.87\)</span>, <span class="math inline">\(p &lt; .001\)</span>). In contrast, although true search slopes were shallower for shape and orientation than for conjunction (p’s&lt;0.001), the difference in estimate slopes was not significant (difference between shape and conjuntion slopes: <span class="math inline">\(t(88) = 1.65\)</span>, <span class="math inline">\(p = .103\)</span>; difference between orientation and conjunction slopes: <span class="math inline">\(t(73) = 1.18\)</span>, <span class="math inline">\(p = .244\)</span>).</p>
<div class="figure" style="text-align: center"><span id="fig:ch4-e1-e2-slopes"></span>
<img src="thesis_files/figure-html/ch4-e1-e2-slopes-1.png" alt="Left panels: median estimated search times plotted against true search times for the different search types (coded by color), and set sizes (coded by circle size; from small to large), for Exp. 1 (upper panel) and 2 (lower panel). Error bars represent the standard error of the median. Right panels: distribution of search (top) and estimated (bottom) slopes for the three search types in Exp. 1 (upper panel) and 2 (lower panel). The dashed line indicates $y=x$ and the dotted line indicates $y=2x$." width="672" />
<p class="caption">
Figure 5.2: Left panels: median estimated search times plotted against true search times for the different search types (coded by color), and set sizes (coded by circle size; from small to large), for Exp. 1 (upper panel) and 2 (lower panel). Error bars represent the standard error of the median. Right panels: distribution of search (top) and estimated (bottom) slopes for the three search types in Exp. 1 (upper panel) and 2 (lower panel). The dashed line indicates <span class="math inline">\(y=x\)</span> and the dotted line indicates <span class="math inline">\(y=2x\)</span>.
</p>
</div>
<!-- #### A graded representation of search efficiency {-} -->
<!-- In FIT's simplest form, searches come in two flavours only: parallel and serial. If participants' intuitive theory of visual search shares this simplifying assumption, the results from the previous section indicate that their theories also wrongly specify that shape and orientation searches are serial just like conjunction search. In contrast, an intuitive theory of visual search may represent search efficiency along a continuum, with some searches being highly efficient, some highly inefficient, and others fall somewhere in between the two ends. This is more in line with contemporary theories of visual search such as Guided Search [REF]. -->
<!-- To further investigate which factors contribute to this correlation, we compared the correlation between true and estimated search times before and after regressing out the effects of set size and search type on search time. This analysis revealed that this correlation was largely based on the positive effect of distractor set size on both search times and estimates (difference between full and part correlation;  $M_d = 0.20$, 95\% CI $[0.15$, $0.24]$, $t(88) = 8.55$, $p < .001$) and on the effect of search type slope ($M_d = 0.13$, 95\% CI $[0.08$, $0.17]$, $t(88) = 5.76$, $p < .001$). When controlling for both factors the part correlation between estimated and actual search times was no longer significant ($M = -0.02$, 95\% CI $[-0.07$, $0.02]$, $t(88) = -0.95$, $p = .342$). In other words, participants' valid expectations about their own search time behaviour were fully captured by the linear model $RT\sim\beta_0+\beta_1N+\beta_2NT$, with *N* representing set size and *T* search type. Any expectations beyond this simple model were not in agreement with their actual search times. -->
<!-- In order to better distinguish between these two options, we focused on the slopes for shape and orientation. These searches were more efficient than conjunction search, but not as efficient as colour search. We tested if this efficiency gradient was represented in search time estimates of single individuals. To this end, we scaled both RT and estimate slopes with respect to subject-specific conjunction slopes $\beta_{scaled}=\frac{\beta}{\beta_{conjunction}}$. If representations of search efficiency are dichotomous, the distribution of scaled estimate slopes should peak at 1. Instead, scaled estimate slopes for both shape and orientation peaked at values lower than 1, and were significantly lower than 1 at the group level  -->
</div>
</div>
</div>
<div id="experiments-3-and-4-complex-unfamiliar-stimuli" class="section level2">
<h2><span class="header-section-number">5.3</span> Experiments 3 and 4: complex, unfamiliar stimuli</h2>
<p>In Experiments 1 and 2 participants’ intuitive theory of visual search allowed them to accurately estimate how long it would take them to find a target stimulus in arrays of distractor stimuli. Participants had insight into the set-size effect and into the fact that conjunction searches are more difficult than feature searches. Importantly, this knowledge could not have been acquired in the familiarization phase of the experiment, where we used ‘T’ and ‘L’s as our stimuli and all displays had the same number of distractors. We also found that participants’ intuitive theory of visual search was systematically biased to overestimate the set-size effect, even in feature searches in which the number of distractors had no effect on search time.</p>
<p>In Experiments 3 and 4 we asked how rich this intuitive theory is, by using displays of complex stimuli with which participants are unlikely to have had prior experience (letters from a medieval Alphabet and from the Futurama TV series). Here, insight into the set size effect and its absence in feature searches would not be useful for generating accurate search time estimates. Instead, participants’ intuitive theory of visual search must be capable of extracting relevant features from rich stimuli, and use these features to generate stimulus-specific predictions based on some intricate model of how visual search works. Using these more complex stimuli further allowed us to ask if search-time estimates rely on person-specific knowledge. Experiment 4 followed Experiment 3 and was pre-registered (pre-registration document: <a href="osf.io/dprtk">osf.io/dprtk</a>).</p>
<div id="participants-6" class="section level3">
<h3><span class="header-section-number">5.3.1</span> Participants</h3>
<p>For Exp. 3, 100 participants were recruited from the Prolific crowdsourcing web-service. The experiment took about 15
minutes to complete. Participants were paid £1.5. The highest performing 30% of participants received an additional bonus of £1. For Exp. 4, 200 participants were recruited from the Prolific crowdsourcing web-service. We recruited more participants for Exp. 4 in order to have sufficient statistical power for our inter-subject correlation analysis (section <a href="5-ch-MVS.html#selfself"><strong>??</strong></a>. The experiment took about 8 minutes to complete. Participants were paid $1.27. The highest performing 30% of participants received an additional bonus of $0.75.</p>
</div>
<div id="procedure-3" class="section level3">
<h3><span class="header-section-number">5.3.2</span> Procedure</h3>
<p>The procedure for Experiments 3 and 4 was similar to that of Exp. 1 with several changes.</p>
<p>Stimuli were letters drawn by Mechanical Turk workers <span class="citation">(Lake et al., 2011)</span>, instead of geometrical shapes. In Exp. 3, we used letters from the <em>Alphabet of the Magi</em>. In Exp. 4, we used letters from the <em>Futurama</em> television series as well as Latin letters. We explained to participants that they will search for a specific letter (the target letter) from among copies of another letter (the distractor letter). In Exp. 3, target and distractor letters were drawn from the Alphabet of the Magi, and distractors were drawn by different Mechanical Turk workers. In Exp. 4, the target and distractor letters were drawn from different Alphabets, with the target being a Latin letter on half of the trials and a Futurama letter on the other half. In this experiments, distractors were copies of the same letter drawn by the same Mechanical Turk worker. This was important for our visual search asymmetry analysis (section <a href="5-ch-MVS.html#asymmetry"><strong>??</strong></a>).</p>
<div class="figure" style="text-align: center"><span id="fig:ch4-methods2"></span>
<img src="figure/ch4/methods2.png" alt="Stimuli used for Experiments 3 and 4. In Exp. 3, stimuli were characters from the Alphabet of the Magi, and distractors were drawn by different Mechanical Turk Users. In Exp.4, stimuli were characters from the Latin and Futurama alphabets. Stimulus pairs 1-4 and 5-8 are identical except for the target assignment. In Exp. 4, all distractors in a display were drawn by the same Mechanical Turk user, and were presented on an invisible clockface." width="100%" />
<p class="caption">
Figure 5.3: Stimuli used for Experiments 3 and 4. In Exp. 3, stimuli were characters from the Alphabet of the Magi, and distractors were drawn by different Mechanical Turk Users. In Exp.4, stimuli were characters from the Latin and Futurama alphabets. Stimulus pairs 1-4 and 5-8 are identical except for the target assignment. In Exp. 4, all distractors in a display were drawn by the same Mechanical Turk user, and were presented on an invisible clockface.
</p>
</div>
<p>In the familiarization part, we used as target and distractors two letters from the Alphabet of the Magi in Exp. 3 and two letters from the Futurama alphabet in Exp. 4. Importantly, these letters were only used for training, and did not appear in the Estimation or Visual search parts. In the Estimation part participants gave search time estimates for 8 search tasks, all involving 10 distractors, and in the Visual Search part they performed these search tasks. To minimize random variation in spatial configurations, in Exp. 4 letters appeared on an invisible clockface surrounding the fixation cross. Finally, the report scale ranged from 0.1 to 4 seconds in Exp. 3 and to 2 seconds in Exp. 4.</p>
</div>
<div id="results-6" class="section level3">
<h3><span class="header-section-number">5.3.3</span> Results</h3>
<p>Accuracy in the visual search task was high in Exp. 3 (<span class="math inline">\(M = 0.89\)</span>, 95% CI <span class="math inline">\([0.86\)</span>, <span class="math inline">\(0.92]\)</span>) and at ceiling in Exp. 4 (<span class="math inline">\(M = 0.97\)</span>, 95% CI <span class="math inline">\([0.96\)</span>, <span class="math inline">\(0.98]\)</span>).
Error trials and visual search trials that took longer than 5 seconds were excluded from all further analysis. Participants were excluded if more than 30% of their trials were excluded based on the aforementioned criteria, leaving 88 and 200 participants for the main analysis of Experiments 3 and 4, respectively.</p>
<div id="estimation-accuracy-1" class="section level4 unnumbered">
<h4>Estimation accuracy</h4>
<p>In both experiments, search time estimates were positively correlated with true search times (within-subject Spearman correlations in Exp. 3: <span class="math inline">\(M = 0.44\)</span>, 95% CI <span class="math inline">\([0.37\)</span>, <span class="math inline">\(0.52]\)</span>, <span class="math inline">\(t(86) = 12.16\)</span>, <span class="math inline">\(p &lt; .001\)</span>; Exp. 4: <span class="math inline">\(M = 0.10\)</span>, 95% CI <span class="math inline">\([0.05\)</span>, <span class="math inline">\(0.15]\)</span>, <span class="math inline">\(t(191) = 3.67\)</span>, <span class="math inline">\(p &lt; .001\)</span>; see Figures <a href="5-ch-MVS.html#fig:ch-4-exp3-estimation-scatter">5.4</a> and <a href="5-ch-MVS.html#fig:ch-4-exp4-estimation-scatter">5.5</a>). The correlation between search time and search time estimates was significantly weaker in Experiment 4 (<span class="math inline">\(\Delta M = 0.35\)</span>, 95% CI <span class="math inline">\([0.26\)</span>, <span class="math inline">\(0.43]\)</span>, <span class="math inline">\(t(181.02) = 7.60\)</span>, <span class="math inline">\(p &lt; .001\)</span>)). This difference in correlation strength is likely the result of a more narrow range of search times in Exp. 4 (with median search times  - - ms, per display) than in Exp. 3 ( - - ms).</p>
<p>Importantly, in both experiments all searches involved exactly 10 distractors, so a positive correlation could not be driven by the effect of distractor set size. Furthermore, since participants had no prior experience with our stimuli, their estimates could not be informed by explicit knowledge about specific letters (‘The third letter in the <em>Alphabet of the Magi</em> pops out to attention when presented between instances of the fourth letter’, or ’the fifth letter in the <em>Futurama Alphabet</em> is difficult to find when presented among <em>d</em>s). These positive correlation reveal a more intricate theory of visual search. Our next two analyses were designed to test whether estimates were based on person-specific knowledge, and whether their generation involved a simulation of the search process.</p>
<div class="figure" style="text-align: center"><span id="fig:ch-4-exp3-estimation-scatter"></span>
<img src="thesis_files/figure-html/ch-4-exp3-estimation-scatter-1.png" alt="Estimated search times plotted against true search times in Experiment 2. The dashed line indicates $y=x$ and the dotted line indicates $y=2x$. Legend: each search task involved searching for one Omniglot character (top letter) among ten tokens of a second Omniglot character, drawn by 10 different MTurk workers (bottom letter)." width="672" />
<p class="caption">
Figure 5.4: Estimated search times plotted against true search times in Experiment 2. The dashed line indicates <span class="math inline">\(y=x\)</span> and the dotted line indicates <span class="math inline">\(y=2x\)</span>. Legend: each search task involved searching for one Omniglot character (top letter) among ten tokens of a second Omniglot character, drawn by 10 different MTurk workers (bottom letter).
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:ch-4-exp4-estimation-scatter"></span>
<img src="thesis_files/figure-html/ch-4-exp4-estimation-scatter-1.png" alt="Median estimated search times plotted against true search times in Experiment 4. The dashed line indicates $y=x$. Legend: each search task involved searching for one character (top letter) among ten tokens of a different character (bottom letter). In four searches, the target character was from the Latin alphabet (circles), and in the other four from the Futurama alphabet (squares). Search pairs that involved the same pair of stimuli with opposite roles are marked by the same color." width="672" />
<p class="caption">
Figure 5.5: Median estimated search times plotted against true search times in Experiment 4. The dashed line indicates <span class="math inline">\(y=x\)</span>. Legend: each search task involved searching for one character (top letter) among ten tokens of a different character (bottom letter). In four searches, the target character was from the Latin alphabet (circles), and in the other four from the Futurama alphabet (squares). Search pairs that involved the same pair of stimuli with opposite roles are marked by the same color.
</p>
</div>
</div>
<div id="selfself" class="section level4 unnumbered">
<h4>Cross-participant correlations</h4>
<p>In our choice of stimuli for Experiments 3 and 4 we were motivated to make heuristic-based estimation more difficult, and instead encourage an introspective estimation process. If participants were using idiosyncratic knowledge about their own attention, we would expect to find higher correlations between their search time estimates and their own search times (<em>self-self alignment</em>), compared to with the search times of a random participant (<em>self-other alignment</em>). To test this, we ran a non-parametric permutation test, comparing self-self and self-other alignment in prospective search time estimates. In Exp. 3, a numerical difference between self-seld (mean Spearman correlation <span class="math inline">\(M_r=\)</span> 0.44) and seld-other alignment (<span class="math inline">\(M_r=\)</span> 0.41) was marginally significant (<span class="math inline">\(p_{perm}=\)</span> 0.05). In Experiment 3, we found a significant advantage for self-self alignment compared with self-other alignment (mean Spearman correlations for self-self <span class="math inline">\(M_r=\)</span> 0.10 and self-other <span class="math inline">\(M_r=\)</span> 0.04, <span class="math inline">\(p_{perm}=\)</span> 0.01).</p>
<p>This advantage for self-self alignment is unlikely to reflect motivated slowing in searches that were rated as difficult. Our bonus scheme incentivized accurate search time estimates in the Estimation part, but in the search part points were awarded for speed. We interpret this result as indicating that at least some of participants’ intuitive theory of visual search builds on idiosyncratic knowledge about their own attention.</p>
</div>
<div id="estimation-time" class="section level4 unnumbered">
<h4>Estimation time</h4>
<p>We next looked at the time taken to give search time estimates in the Estimation part. We reasoned that if participants had to mentally simulate searching for the target in order to generate their search time estimates, they would take longer to estimate that a search task will terminate after 1500 compared to 1000 milliseconds. This is similar to how a linear alignment between the degree of rotation and response time in a mental rotation task was taken as support for an internal simulation that evolves over time <span class="citation">(Shepard &amp; Metzler, 1971)</span>. We see no evidence for within-subject correlation between estimates and the time taken to deliver them, not in Exp. 3 (<span class="math inline">\(t(86) = 0.40\)</span>, <span class="math inline">\(p = .692\)</span>) and not in Exp. 4 (<span class="math inline">\(t(191) = 0.74\)</span>, <span class="math inline">\(p = .458\)</span>).</p>
</div>
<div id="asymmetry" class="section level4 unnumbered">
<h4>Visual search asymmetry</h4>
<p>[PARAGRAPH ABOUT SEARCH ASYMMETRY]</p>
<p>To test if participants were sensitive to this asymmetry in their prospective visual search estimates, we extracted the estimated/true search time correlations after inverting the identity of the target and distractor stimuli in the estimates, but not in the actual search times. If estimates were affected by the assignment of stimuli to target and distractor, this inversion should reduce the correlation, but if visual search estimates reflected a symmetric notion of similarity the correlation should not be affected. Inverting the target/distractor assignment dropped the correlation between estimates and search time to zero (<span class="math inline">\(M = -0.01\)</span>, 95% CI <span class="math inline">\([-0.06\)</span>, <span class="math inline">\(0.04]\)</span>), significantly lower than the original correlation (<span class="math inline">\(M_d = 0.10\)</span>, 95% CI <span class="math inline">\([0.03\)</span>, <span class="math inline">\(0.18]\)</span>, <span class="math inline">\(t(191) = 2.63\)</span>, <span class="math inline">\(p = .009\)</span>).</p>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="6-app1-SDT.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["thesis.pdf", "PDF"], ["thesis.epub", "EPUB"], ["thesis.docx", "Word"]],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
