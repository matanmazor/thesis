<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>D Supp. materials for ch. 3 | Self-Modeling in Inference about Absence</title>
  <meta name="description" content="D Supp. materials for ch. 3 | Self-Modeling in Inference about Absence" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="D Supp. materials for ch. 3 | Self-Modeling in Inference about Absence" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="D Supp. materials for ch. 3 | Self-Modeling in Inference about Absence" />
  
  
  

<meta name="author" content="Matan Mazor" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="C-the-second-appendix-for-fun.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"></a></li>
<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> This will automatically install the {remotes} package and {thesisdown}</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#inference-about-absence"><i class="fa fa-check"></i><b>1.1</b> Inference about absence</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#formalabsence"><i class="fa fa-check"></i><b>1.2</b> Probabilistic reasoning, criterion setting, and self knowledge</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#symmetrical-definition"><i class="fa fa-check"></i><b>1.2.1</b> Symmetrical definition:</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#dissymmetrical-definition"><i class="fa fa-check"></i><b>1.2.2</b> Dissymmetrical definition:</a></li>
<li class="chapter" data-level="1.2.3" data-path="index.html"><a href="index.html#the-evolution-of-second-order-cognition"><i class="fa fa-check"></i><b>1.2.3</b> The evolution of second-order cognition</a></li>
<li class="chapter" data-level="1.2.4" data-path="index.html"><a href="index.html#computational-models-of-detection"><i class="fa fa-check"></i><b>1.2.4</b> Computational models of detection</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#detection-i-would-have-noticed-it"><i class="fa fa-check"></i><b>1.3</b> Detection: “I would have noticed it”</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#visual-search-i-would-have-found-it"><i class="fa fa-check"></i><b>1.4</b> Visual search: “I would have found it”</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#memory-i-would-have-remembered-it"><i class="fa fa-check"></i><b>1.5</b> Memory: “I would have remembered it”</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#the-development-of-a-self-model"><i class="fa fa-check"></i><b>1.6</b> The development of a self-model</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html"><a href="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html"><i class="fa fa-check"></i><b>2</b> Zero-shot search termination reveals a dissociation between implicit and explicit metacognitive knowledge</a><ul>
<li class="chapter" data-level="2.1" data-path="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html"><a href="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html"><a href="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html#experiment-1"><i class="fa fa-check"></i><b>2.2</b> Experiment 1</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html"><a href="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html#participants"><i class="fa fa-check"></i><b>2.2.1</b> Participants</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html"><a href="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html#procedure"><i class="fa fa-check"></i><b>2.2.2</b> Procedure</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html"><a href="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html#data-analysis"><i class="fa fa-check"></i><b>2.2.3</b> Data analysis</a></li>
<li class="chapter" data-level="2.2.4" data-path="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html"><a href="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html#results"><i class="fa fa-check"></i><b>2.2.4</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html"><a href="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html#experiment-2"><i class="fa fa-check"></i><b>2.3</b> Experiment 2</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html"><a href="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html#participants-1"><i class="fa fa-check"></i><b>2.3.1</b> Participants</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html"><a href="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html#procedure-1"><i class="fa fa-check"></i><b>2.3.2</b> Procedure</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html"><a href="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html#results-1"><i class="fa fa-check"></i><b>2.3.3</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html"><a href="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html#discussion"><i class="fa fa-check"></i><b>2.4</b> Discussion</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html"><a href="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html#is-implicit-metacognitive-knowledge-metacognitive"><i class="fa fa-check"></i><b>2.4.1</b> Is implicit metacognitive knowledge metacognitive?</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html"><a href="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html#inference-about-absence-as-a-tool-for-studying-implicit-self-knowledge"><i class="fa fa-check"></i><b>2.4.2</b> Inference about absence as a tool for studying implicit self knowledge</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html"><a href="2-zero-shot-search-termination-reveals-a-dissociation-between-implicit-and-explicit-metacognitive-knowledge.html#conclusion"><i class="fa fa-check"></i><b>2.4.3</b> Conclusion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-math-sci.html"><a href="3-math-sci.html"><i class="fa fa-check"></i><b>3</b> Mathematics and Science</a><ul>
<li class="chapter" data-level="3.1" data-path="3-math-sci.html"><a href="3-math-sci.html#math"><i class="fa fa-check"></i><b>3.1</b> Math</a></li>
<li class="chapter" data-level="3.2" data-path="3-math-sci.html"><a href="3-math-sci.html#chemistry-101-symbols"><i class="fa fa-check"></i><b>3.2</b> Chemistry 101: Symbols</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-math-sci.html"><a href="3-math-sci.html#typesetting-reactions"><i class="fa fa-check"></i><b>3.2.1</b> Typesetting reactions</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-math-sci.html"><a href="3-math-sci.html#other-examples-of-reactions"><i class="fa fa-check"></i><b>3.2.2</b> Other examples of reactions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-math-sci.html"><a href="3-math-sci.html#physics"><i class="fa fa-check"></i><b>3.3</b> Physics</a></li>
<li class="chapter" data-level="3.4" data-path="3-math-sci.html"><a href="3-math-sci.html#biology"><i class="fa fa-check"></i><b>3.4</b> Biology</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><i class="fa fa-check"></i><b>4</b> Distinct neural contributions to metacognition for detecting (but not discriminating) visual stimuli</a><ul>
<li class="chapter" data-level="4.1" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#introduction-1"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#methods-and-materials"><i class="fa fa-check"></i><b>4.2</b> Methods and Materials</a><ul>
<li class="chapter" data-level="4.2.1" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#participants-2"><i class="fa fa-check"></i><b>4.2.1</b> Participants</a></li>
<li class="chapter" data-level="4.2.2" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#design-and-procedure"><i class="fa fa-check"></i><b>4.2.2</b> Design and procedure</a></li>
<li class="chapter" data-level="4.2.3" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#scanning-parameters"><i class="fa fa-check"></i><b>4.2.3</b> Scanning parameters</a></li>
<li class="chapter" data-level="4.2.4" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#analysis"><i class="fa fa-check"></i><b>4.2.4</b> Analysis</a></li>
<li class="chapter" data-level="4.2.5" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#exclusion-criteria"><i class="fa fa-check"></i><b>4.2.5</b> Exclusion criteria</a></li>
<li class="chapter" data-level="4.2.6" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#response-conditional-type-ii-roc-curves"><i class="fa fa-check"></i><b>4.2.6</b> Response conditional type-II ROC curves</a></li>
<li class="chapter" data-level="4.2.7" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#imaging-analysis"><i class="fa fa-check"></i><b>4.2.7</b> Imaging analysis</a></li>
<li class="chapter" data-level="4.2.8" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#statistical-inference"><i class="fa fa-check"></i><b>4.2.8</b> Statistical inference</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#results-2"><i class="fa fa-check"></i><b>4.3</b> Results</a></li>
<li class="chapter" data-level="4.4" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#behavioural-results"><i class="fa fa-check"></i><b>4.4</b> Behavioural results</a><ul>
<li class="chapter" data-level="4.4.1" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#imaging-results"><i class="fa fa-check"></i><b>4.4.1</b> Imaging results</a></li>
<li class="chapter" data-level="4.4.2" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#computational-models"><i class="fa fa-check"></i><b>4.4.2</b> Computational models</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#discussion-1"><i class="fa fa-check"></i><b>4.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="conclusion-1.html"><a href="conclusion-1.html"><i class="fa fa-check"></i>Conclusion</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-app1-SDT.html"><a href="A-app1-SDT.html"><i class="fa fa-check"></i><b>A</b> Signal Detection Theory</a></li>
<li class="chapter" data-level="B" data-path="B-app1-ROC.html"><a href="B-app1-ROC.html"><i class="fa fa-check"></i><b>B</b> ROC and zROC curves</a></li>
<li class="chapter" data-level="C" data-path="C-the-second-appendix-for-fun.html"><a href="C-the-second-appendix-for-fun.html"><i class="fa fa-check"></i><b>C</b> The Second Appendix, for Fun</a></li>
<li class="chapter" data-level="D" data-path="D-supp-materials-for-ch-3.html"><a href="D-supp-materials-for-ch-3.html"><i class="fa fa-check"></i><b>D</b> Supp. materials for ch. 3</a><ul>
<li class="chapter" data-level="D.1" data-path="D-supp-materials-for-ch-3.html"><a href="D-supp-materials-for-ch-3.html#app3:buttonpresses"><i class="fa fa-check"></i><b>D.1</b> Confidence button presses</a></li>
<li class="chapter" data-level="D.2" data-path="D-supp-materials-for-ch-3.html"><a href="D-supp-materials-for-ch-3.html#app3:zROC"><i class="fa fa-check"></i><b>D.2</b> zROC curves</a></li>
<li class="chapter" data-level="D.3" data-path="D-supp-materials-for-ch-3.html"><a href="D-supp-materials-for-ch-3.html#app3:GC-DM"><i class="fa fa-check"></i><b>D.3</b> Global confidence design matrix</a></li>
<li class="chapter" data-level="D.4" data-path="D-supp-materials-for-ch-3.html"><a href="D-supp-materials-for-ch-3.html#app3:ROIconf"><i class="fa fa-check"></i><b>D.4</b> Effect of confidence in our pre-specified ROIs</a></li>
<li class="chapter" data-level="D.5" data-path="D-supp-materials-for-ch-3.html"><a href="D-supp-materials-for-ch-3.html#app3:varianceRat"><i class="fa fa-check"></i><b>D.5</b> SDT variance ratio correlation with the quadratic confidence effect</a></li>
<li class="chapter" data-level="D.6" data-path="D-supp-materials-for-ch-3.html"><a href="D-supp-materials-for-ch-3.html#app3:efficiency"><i class="fa fa-check"></i><b>D.6</b> Correlation of metacognitive efficiency with linear and quadratic confidence effects</a></li>
<li class="chapter" data-level="D.7" data-path="D-supp-materials-for-ch-3.html"><a href="D-supp-materials-for-ch-3.html#app3:cross"><i class="fa fa-check"></i><b>D.7</b> Confidence-decision cross classification</a></li>
<li class="chapter" data-level="D.8" data-path="D-supp-materials-for-ch-3.html"><a href="D-supp-materials-for-ch-3.html#app3:SDT"><i class="fa fa-check"></i><b>D.8</b> Static Signal Detection Theory</a><ul>
<li class="chapter" data-level="D.8.1" data-path="D-supp-materials-for-ch-3.html"><a href="D-supp-materials-for-ch-3.html#discrimination"><i class="fa fa-check"></i><b>D.8.1</b> Discrimination</a></li>
<li class="chapter" data-level="D.8.2" data-path="D-supp-materials-for-ch-3.html"><a href="D-supp-materials-for-ch-3.html#detection"><i class="fa fa-check"></i><b>D.8.2</b> Detection</a></li>
</ul></li>
<li class="chapter" data-level="D.9" data-path="D-supp-materials-for-ch-3.html"><a href="D-supp-materials-for-ch-3.html#app3:Dynamic"><i class="fa fa-check"></i><b>D.9</b> Dynamic Criterion</a><ul>
<li class="chapter" data-level="D.9.1" data-path="D-supp-materials-for-ch-3.html"><a href="D-supp-materials-for-ch-3.html#discrimination-1"><i class="fa fa-check"></i><b>D.9.1</b> Discrimination</a></li>
<li class="chapter" data-level="D.9.2" data-path="D-supp-materials-for-ch-3.html"><a href="D-supp-materials-for-ch-3.html#detection-1"><i class="fa fa-check"></i><b>D.9.2</b> Detection</a></li>
</ul></li>
<li class="chapter" data-level="D.10" data-path="D-supp-materials-for-ch-3.html"><a href="D-supp-materials-for-ch-3.html#app3:Monitoring"><i class="fa fa-check"></i><b>D.10</b> Attention Monitoring</a><ul>
<li class="chapter" data-level="D.10.1" data-path="D-supp-materials-for-ch-3.html"><a href="D-supp-materials-for-ch-3.html#discrimination-2"><i class="fa fa-check"></i><b>D.10.1</b> Discrimination</a></li>
<li class="chapter" data-level="D.10.2" data-path="D-supp-materials-for-ch-3.html"><a href="D-supp-materials-for-ch-3.html#detection-2"><i class="fa fa-check"></i><b>D.10.2</b> Detection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Self-Modeling in Inference about Absence</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="supp.-materials-for-ch.-3" class="section level1">
<h1><span class="header-section-number">D</span> Supp. materials for ch. 3</h1>
<div id="app3:buttonpresses" class="section level2">
<h2><span class="header-section-number">D.1</span> Confidence button presses</h2>
<div class="figure" style="text-align: center"><span id="fig:buttonpresses"></span>
<img src="figure/ch3/buttonPresses.png" alt="Average number of button presses for each confidence level, as a function of task. More button presses were needed on average to reach the extreme confidence ratings, hence the quadratic shape. No difference between the two tasks was observed in the mean number of button presses for any of the confidence levels. Error bars represent the standard error of the mean." width="\linewidth" />
<p class="caption">
Figure D.1: Average number of button presses for each confidence level, as a function of task. More button presses were needed on average to reach the extreme confidence ratings, hence the quadratic shape. No difference between the two tasks was observed in the mean number of button presses for any of the confidence levels. Error bars represent the standard error of the mean.
</p>
</div>
</div>
<div id="app3:zROC" class="section level2">
<h2><span class="header-section-number">D.2</span> zROC curves</h2>
<div class="figure" style="text-align: center"><span id="fig:zROC"></span>
<img src="figure/ch3/zROC.jpg" alt="mean zROC curves for the discrimination and detection tasks. As expected in a uv-SDT setting, the discrimination curve is approximately linear with a slope of 1, and the detection curve is approximately linear with a shallower slope. Error bars represent the standard error of the mean." width="\linewidth" />
<p class="caption">
Figure D.2: mean zROC curves for the discrimination and detection tasks. As expected in a uv-SDT setting, the discrimination curve is approximately linear with a slope of 1, and the detection curve is approximately linear with a shallower slope. Error bars represent the standard error of the mean.
</p>
</div>
</div>
<div id="app3:GC-DM" class="section level2">
<h2><span class="header-section-number">D.3</span> Global confidence design matrix</h2>
<div class="figure" style="text-align: center"><span id="fig:GC-DM"></span>
<img src="figure/ch3/GC-DM.jpg" alt="Effect of confidence in correct responses, from the global-confidence design matrix. Uncorrected, thresholded at p&lt;0.001. Left: glass brain visualization of the whole brain contrast. Right: yellow-red represent a positive correlation with subjective confidence ratings, and green-blue represent a negative correlation." width="\linewidth" />
<p class="caption">
Figure D.3: Effect of confidence in correct responses, from the global-confidence design matrix. Uncorrected, thresholded at p&lt;0.001. Left: glass brain visualization of the whole brain contrast. Right: yellow-red represent a positive correlation with subjective confidence ratings, and green-blue represent a negative correlation.
</p>
</div>
<p>From our pre-specified ROIs, only the vmPFC and BA46 ROIs showed a significant linear effect of confidence in correct responses, in the opposite direction to what we expected based on previous studies. This is likely to be due to the differences in confidence profiles between the detection and discrimination tasks:</p>
<table>
<thead>
<tr class="header">
<th>Average beta</th>
<th>T value</th>
<th>P value</th>
<th>Standard deviation</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>vmPFC</td>
<td>-0.35</td>
<td>-3.06</td>
<td>4 × 10-3</td>
<td>0.67</td>
</tr>
<tr class="even">
<td>pMFC</td>
<td>-0.31</td>
<td>-2.48</td>
<td>0.02</td>
<td>0.74</td>
</tr>
<tr class="odd">
<td>precuneus</td>
<td>0.25</td>
<td>2.30</td>
<td>0.03</td>
<td>0.64</td>
</tr>
<tr class="even">
<td>ventral striatum</td>
<td>-0.056</td>
<td>-1.51</td>
<td>0.14</td>
<td>0.22</td>
</tr>
<tr class="odd">
<td>FPl</td>
<td>0.16</td>
<td>1.52</td>
<td>0.14</td>
<td>0.64</td>
</tr>
<tr class="even">
<td>FPm</td>
<td>-0.12</td>
<td>-1.46</td>
<td>0.16</td>
<td>0.48</td>
</tr>
<tr class="odd">
<td>BA 46</td>
<td>0.37</td>
<td>3.77</td>
<td>6 × 10-4</td>
<td>0.57</td>
</tr>
</tbody>
</table>
</div>
<div id="app3:ROIconf" class="section level2">
<h2><span class="header-section-number">D.4</span> Effect of confidence in our pre-specified ROIs</h2>
<div class="figure" style="text-align: center"><span id="fig:ROIconf"></span>
<img src="figure/ch3/allROIs.png" alt="Effect of confidence in all 4 ROIs, as a function of task and response, as extracted from the categorical design matrix. No significant interaction between the linear or quadratic effects and task or response was observed in any of the ROIs." width="\linewidth" />
<p class="caption">
Figure D.4: Effect of confidence in all 4 ROIs, as a function of task and response, as extracted from the categorical design matrix. No significant interaction between the linear or quadratic effects and task or response was observed in any of the ROIs.
</p>
</div>
</div>
<div id="app3:varianceRat" class="section level2">
<h2><span class="header-section-number">D.5</span> SDT variance ratio correlation with the quadratic confidence effect</h2>
<div class="figure" style="text-align: center"><span id="fig:varianceRat"></span>
<img src="figure/ch3/varianceRatio.jpg" alt="Inter-subject correlation between the quadratic effect in the right hemisphere clusters and the ratio between the detection (top panel) and discrimination (lower panel) distribution variances, as estimated from the zROC curve slopes in the two tasks. Marker color indicates the goodness of fit of the second-order polynomial model to the BOLD data. All Spearman correlation coefficients are &lt;0.25." width="\linewidth" />
<p class="caption">
Figure D.5: Inter-subject correlation between the quadratic effect in the right hemisphere clusters and the ratio between the detection (top panel) and discrimination (lower panel) distribution variances, as estimated from the zROC curve slopes in the two tasks. Marker color indicates the goodness of fit of the second-order polynomial model to the BOLD data. All Spearman correlation coefficients are &lt;0.25.
</p>
</div>
</div>
<div id="app3:efficiency" class="section level2">
<h2><span class="header-section-number">D.6</span> Correlation of metacognitive efficiency with linear and quadratic confidence effects</h2>
<div class="figure" style="text-align: center"><span id="fig:varianceRatio"></span>
<img src="figure/ch3/efficiency.png" alt="Inter-subject correlation between the linear (upper panel) and quadratic (lower panel) effects in the right hemisphere clusters and metacognitive efficiency scores (measured as M ratio = meta-d'/d', Maniscalco and Lau, 2012)." width="\linewidth" />
<p class="caption">
Figure D.6: Inter-subject correlation between the linear (upper panel) and quadratic (lower panel) effects in the right hemisphere clusters and metacognitive efficiency scores (measured as M ratio = meta-d’/d’, Maniscalco and Lau, 2012).
</p>
</div>
</div>
<div id="app3:cross" class="section level2">
<h2><span class="header-section-number">D.7</span> Confidence-decision cross classification</h2>
<p>In order to dissociate between brain regions that encode stimulus visibility and brain regions that encode decision confidence, we performed a multivariate cross-classification analysis. We trained a linear classifier on detection decisions (‘yes’ and ‘no’), and tested it on discrimination confidence (high and low), and vice versa. Shared information content between detection responses and confidence in discrimination is expected in brain regions that encode stimulus visibility, rather than accuracy estimation. In detection, yes responses are associated with higher stimulus visibility compared to no responses (regardless of decision confidence), and in discrimination high confidence trials are associated with higher visibility than low confidence trials (regardless of subjective confidence).</p>
<p>Presented cross classification scores are the mean of cross classification accuracies in both directions. Detection-response and discrimination-confidence cross-classification was significantly above chance in in the pMFC (<span class="math inline">\(t(29)=2.76, p&lt;0.05\)</span>, corrected for family-wise error across the four ROIs), and in the BA46 anatomical subregion of the frontopolar ROI (<span class="math inline">\(t(29)=2.64, p&lt;0.05\)</span>, corrected).</p>
<div class="figure" style="text-align: center"><span id="fig:cross"></span>
<img src="figure/ch3/confidence_cross_resp.png" alt="Accuracy minus chance for classification of response in detection (yes vs. no; blue), and from a cross-classification between tasks: confidence in detection and confidence in discrimination (gray), and confidence in discrimination and decision in detection (pink). " width="\linewidth" />
<p class="caption">
Figure D.7: Accuracy minus chance for classification of response in detection (yes vs. no; blue), and from a cross-classification between tasks: confidence in detection and confidence in discrimination (gray), and confidence in discrimination and decision in detection (pink).
</p>
</div>
</div>
<div id="app3:SDT" class="section level2">
<h2><span class="header-section-number">D.8</span> Static Signal Detection Theory</h2>
<div id="discrimination" class="section level3">
<h3><span class="header-section-number">D.8.1</span> Discrimination</h3>
<div id="generative-model" class="section level4 unnumbered">
<h4>Generative model</h4>
<p>According to SDT, a decision variable <span class="math inline">\(x\)</span> is sampled from one of two distributions on each experimental trial.</p>
<p><span class="math display">\[\begin{equation}
  \mu_t=\begin{cases}
    0.5, &amp; \text{if cw}.\\
    -0.5, &amp; \text{if acw}.\\
  \end{cases}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
   x_t \sim \mathcal{N}(\mu_t,1)    
\end{equation}\]</span></p>
</div>
<div id="inference" class="section level4 unnumbered">
<h4>Inference</h4>
<p><span class="math inline">\(x\)</span> is compared against a criterion to generate a decision about which of the two distributions was most likely, given the sample. For a discrimination task with symmetric distributions around 0, the optimal placement for a criterion is at 0.</p>
<p><span class="math display">\[\begin{equation}
  decision_t=\begin{cases}
    \text{cw}, &amp; \text{if } x_t&gt;0.\\
   \text{acw}, &amp; \text{else}.
    
  \end{cases}
\end{equation}\]</span></p>
<p>In standard discrimination tasks, a common assumption is that the two distributions are Gaussian with equal variance. This assumption has a convenient computational consequence: the log-likelihood ratio (LLR), a quantity that reflects the degree to which the sample is more likely under one distribution or another, is linear with respect to <span class="math inline">\(x\)</span>. Confidence is then assumed to be proportional to the distance of <span class="math inline">\(x_t\)</span> from the decision criterion.</p>
<p>In what follows <span class="math inline">\(\phi(x,\mu,\sigma)\)</span> is the likelihood of observing x when sampling from a normal distribution with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>.</p>
<p><span class="math display">\[\begin{equation}
LLR = log(\phi(x_t,0.5,1))-log(\phi(x_t,-0.5,1))
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
confidence_t \propto |x_t|
\end{equation}\]</span></p>
</div>
</div>
<div id="detection" class="section level3">
<h3><span class="header-section-number">D.8.2</span> Detection</h3>
<div id="generative-model-1" class="section level4 unnumbered">
<h4>Generative model</h4>
<p>A common assumption is that in detection the signal distribution is wider than the noise distribution <span class="citation">(unequal-variance SDT; Wickens, 2002, p. 48)</span>.</p>
<p><span class="math display">\[\begin{equation}
  \mu_t=\begin{cases}
    1.3, &amp; \text{if P}.\\
    0, &amp; \text{if A}.\\
  \end{cases}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
  \sigma_t=\begin{cases}
    2, &amp; \text{if P}.\\
    1, &amp; \text{if A}.\\
  \end{cases}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
   x_t \sim \mathcal{N}(\mu_t,\sigma_t)    
\end{equation}\]</span></p>
</div>
<div id="inference-1" class="section level4 unnumbered">
<h4>Inference</h4>
<p>Here <span class="math inline">\(med(x)\)</span> represents the median sensory sample <span class="math inline">\(x\)</span>. This criterion was chosen to ensure that detection responses are balanced.</p>
<p><span class="math display">\[\begin{equation}
  decision=\begin{cases}
    \text{P}, &amp; \text{if } x_t&gt;med(x).\\
    \text{A}, &amp; \text{else}.
  \end{cases}
\end{equation}\]</span></p>
<p>Importantly, in uv-SDT, LLR is quadratic in x.
<span class="math display">\[\begin{equation}
LLR = log(\phi(x,1.3,2))-log(\phi(x,0,1))
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
confidence \propto |x_t-med(x)|
\end{equation}\]</span></p>
</div>
</div>
</div>
<div id="app3:Dynamic" class="section level2">
<h2><span class="header-section-number">D.9</span> Dynamic Criterion</h2>
<p>In SDT, task performance depends on the degree of overlap between the underlying distributions (d’) and on the positioning of the decision criterion (c). Participants may optimize criterion placement based on their changing beliefs about the underlying distributions <span class="citation">(Ko &amp; Lau, 2012)</span>. To model this dynamic process of criterion setting we simulated a model where beliefs about the underlying distributions are the Maximum Likelihood Estimates of the mean and standard deviation, based on the last 5 samples that were (correctly or not) categorized.</p>
<div id="discrimination-1" class="section level3">
<h3><span class="header-section-number">D.9.1</span> Discrimination</h3>
<div id="generative-model-2" class="section level4 unnumbered">
<h4>Generative model</h4>
<p>As in the Static Signal Detection model.</p>
</div>
<div id="inference-2" class="section level4 unnumbered">
<h4>Inference</h4>
<p>Means and standard deviations of the two distributions are estimated based on the last 5 samples in each category. To model prior beliefs about these parameters, each participant starts the task with 5 imaginary samples from the veridical distributions. Means and standard deviations are then extracted from these imaginary samples. In what follows, <span class="math inline">\(\vec{cw}\)</span> and <span class="math inline">\(\vec{acw}\)</span> are vectors with entries corresponding to the last 5 samples that were (correcly or not) labelled as ‘cw’ and ‘acw’, respectively. <span class="math inline">\(\bar{x}_{cw}\)</span> and <span class="math inline">\(\bar{x}_{acw}\)</span> correspond to the sample means of these vectors. <span class="math inline">\(\sigma_{cw}\)</span> and <span class="math inline">\(\sigma_{acw}\)</span> correspond to their standard deviations.</p>
<p><span class="math display">\[\begin{equation}
LLR = log(\phi(x,\bar{x}_{cw},\sigma_{cw}))-log(\phi(x,\bar{x}_{acw},\sigma_{acw}))
\end{equation}\]</span></p>
<p>Decisions and confidence are extracted from the <span class="math inline">\(LLR\)</span> as in the Static Signal Detection model.</p>
</div>
</div>
<div id="detection-1" class="section level3">
<h3><span class="header-section-number">D.9.2</span> Detection</h3>
<div id="generative-model-3" class="section level4 unnumbered">
<h4>Generative model</h4>
<p>As in the Static Signal Detection model.</p>
</div>
<div id="inference-3" class="section level4 unnumbered">
<h4>Inference</h4>
<p>As in discrimination. In what follows, <span class="math inline">\(\vec{a}\)</span> and <span class="math inline">\(\vec{p}\)</span> are vectors with entries corresponding to the last 5 samples that were (correcly or not) labelled as ‘signal absent’ and ‘signal present’, respectively. <span class="math inline">\(\bar{x}_{a}\)</span> and <span class="math inline">\(\bar{x}_{p}\)</span> correspond to the sample means of these vectors. <span class="math inline">\(\sigma_{a}\)</span> and <span class="math inline">\(\sigma_{p}\)</span> correspond to their standard deviations.</p>
<p><span class="math display">\[\begin{equation}
LLR = log(\phi(x,\bar{x}_{p},\sigma_{p}))-log(\phi(x,\bar{x}_{a},\sigma_{a}))
\end{equation}\]</span></p>
<p>In detection, <span class="math inline">\(LLR=0\)</span> at two points (see figure @ref{fig:models). The decision criterion <span class="math inline">\(c_t\)</span> is chosen to coincide with the rightmost point, which is positioned between the Signal and Noise distribution means.</p>
<p><span class="math display">\[\begin{equation}
  decision=\begin{cases}
    \text{p}, &amp; \text{if } x_t&gt;c_t.\\
    \text{a}, &amp; \text{else}.
  \end{cases}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
confidence \propto |LLR|
\end{equation}\]</span></p>
</div>
</div>
</div>
<div id="app3:Monitoring" class="section level2">
<h2><span class="header-section-number">D.10</span> Attention Monitoring</h2>
<p>Similar to the Dynamic Criterion model, in the Attention Monitoring model participants adjusts a decision criterion based on changing beliefs about the underlying distributions. However, unlike the Dynamic Criterion model, here beliefs change not as a function of recent perceptual samples, but as a function of access to an internal variable that represents the expected sensory precision (attention).</p>
<div id="discrimination-2" class="section level3">
<h3><span class="header-section-number">D.10.1</span> Discrimination</h3>
<div id="generative-model-4" class="section level4 unnumbered">
<h4>Generative model</h4>
<p>In our schematic formulation of this model, participants have a true attentional state, which for simplicity we treat as either being on (1) or off (0). When attending, participatns enjoy higher sensitivity than when they don’t.</p>
<p><span class="math display">\[\begin{equation}
    p(attended_t)= 0.5
\end{equation}\]</span></p>
<p>The attentional state determines the means of sensory distributions.</p>
<p><span class="math display">\[\begin{equation}
  \mu_t=\begin{cases}
    0.5, &amp; \text{if cw and $\neg attended_t$}.\\
    -0.5, &amp; \text{if acw and $\neg attended_t$}.\\
    2, &amp; \text{if cw and $attended_t$}.\\
    -2, &amp; \text{if acw and $attended_t$}.\\
  \end{cases}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
   x_t \sim \mathcal{N}(\mu_t,1)    
\end{equation}\]</span></p>
<p>However, they don’t have direct access to their attentional state, but only to a noisy approximation of the probability that they were attending.</p>
<span class="math display">\[\begin{equation}
onTask_t \sim \begin{cases}
    Beta(2, 1), &amp; \text{if $attended_t$}.\\
    Beta(1, 2), &amp; \text{if $\neg attended_t$}.\\
  \end{cases}
\end{equation}\]</span>

</div>
<div id="inference-4" class="section level4 unnumbered">
<h4>Inference</h4>
<p>Participants are then assumed to use their knowledge about the <span class="math inline">\(onTask\)</span> variable when making a decision and confidence estimate.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
    p(x_t | \text{cw}) = p(attended_t | onTask_t) \phi(x_t,2,1) + p(\neg attended_t | onTask_t) \phi(x_t,0.5,1)\\
    = onTask_t \phi(x_t,2,1) + (1-onTask_t) \phi(x_t,0.5,1)
    \end{aligned}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
    p(x_t | \text{acw}) = p(attended_T | onTask_t) \phi(x_t,-2,1) + p(\neg attended_t | onTask_t) \phi(x_t,-0.5,1)\\
    = onTask_t \phi(x_t,-2,1) + (1-onTask_t) \phi(x_t,-0.5,1)
    \end{aligned}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
LLR = log(p(x_t | \text{w}))-log(p(x_t | \text{acw})
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
  decision_t=\begin{cases}
    \text{cw}, &amp; \text{if } LLR&gt;0.\\
   \text{acw}, &amp; \text{else}.
    
  \end{cases}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
confidence_t \propto |LLR|
\end{equation}\]</span></p>
</div>
</div>
<div id="detection-2" class="section level3">
<h3><span class="header-section-number">D.10.2</span> Detection</h3>
<div id="generative-model-5" class="section level4 unnumbered">
<h4>Generative model</h4>
<p>In detection, attentional states only affect the signal distribution, as noise is always centred at 0.</p>
<p><span class="math display">\[\begin{equation}
  \mu_t=\begin{cases}
    0, &amp; \text{if a and $\neg attended_t$}.\\
    0.5, &amp; \text{if p and $\neg attended_t$}.\\
    0, &amp; \text{if a and $attended_t$}.\\
    2, &amp; \text{if p and $attended_t$}.\\
  \end{cases}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
   x_t \sim \mathcal{N}(\mu_t,1)    
\end{equation}\]</span></p>
</div>
<div id="inference-5" class="section level4 unnumbered">
<h4>Inference</h4>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
    p(x_t | \text{p}) = p(attended_t | onTask_t) \phi(x_t,2,1) + p(\neg attended_t|onTask_t) \phi(x_t,0.5,1)\\
    = onTask_t \phi(x_t,2,1) + (1-onTask_t) \phi(x_t,0.5,1)
    \end{aligned}
\end{equation}\]</span></p>
<p>The likelihood of observing <span class="math inline">\(x_t\)</span> if no stimulus was presented is independent of the attention state.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
    p(x_t | \text{a}) = p(attended_t | onTask_t) \phi(x_t,0,1)) + p(\neg attended_t | onTask_t) \phi(x_t,0,1)\\
    = \phi(x_t,0,1)
    \end{aligned}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
LLR = log(p(x_t | \text{p}))-log(p(x_t | \text{a})
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
  decision_t=\begin{cases}
    \text{p}, &amp; \text{if } LLR&gt;0.\\
   \text{a}, &amp; \text{else}.
    
  \end{cases}
\end{equation}\]</span></p>
<p>Nevertheless, confidence in judgments about stimulus absence is dependent on beliefs about the attentional state. This is mediated by the effect of attention on the likelihood of observing <span class="math inline">\(x_t\)</span> if a stimulus were present. This is the counterfactual part.</p>
<p><span class="math display">\[\begin{equation}
confidence_t \propto |LLR|
\end{equation}\]</span></p>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="C-the-second-appendix-for-fun.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["thesis.pdf", "PDF"], ["thesis.epub", "EPUB"], ["thesis.docx", "Word"]],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
