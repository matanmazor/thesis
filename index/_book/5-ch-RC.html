<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Prospective search time estimates for unseen displays reveal a rich intuitive theory of visual search | Self-Modeling in Inference about Absence</title>
  <meta name="description" content="Chapter 5 Prospective search time estimates for unseen displays reveal a rich intuitive theory of visual search | Self-Modeling in Inference about Absence" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Prospective search time estimates for unseen displays reveal a rich intuitive theory of visual search | Self-Modeling in Inference about Absence" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Prospective search time estimates for unseen displays reveal a rich intuitive theory of visual search | Self-Modeling in Inference about Absence" />
  
  
  

<meta name="author" content="Matan Mazor" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"/>
<link rel="next" href="6-app1-SDT.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"></a></li>
<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> This will automatically install the {remotes} package and {thesisdown}</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#inference-about-absence"><i class="fa fa-check"></i><b>1.1</b> Inference about absence</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#formalabsence"><i class="fa fa-check"></i><b>1.2</b> Probabilistic reasoning, criterion setting, and self knowledge</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#symmetrical-definition"><i class="fa fa-check"></i>Symmetrical definition:</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dissymmetrical-definition"><i class="fa fa-check"></i>Dissymmetrical definition:</a></li>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#second-order-cognition"><i class="fa fa-check"></i><b>1.2.1</b> Second-order cognition</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#detectionmodels"><i class="fa fa-check"></i><b>1.2.2</b> Computational models of detection</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#htm"><i class="fa fa-check"></i>The High-Threshold model</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#sdt"><i class="fa fa-check"></i>Signal Detection Theory</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#detection-i-would-have-noticed-it"><i class="fa fa-check"></i><b>1.3</b> Detection: “I would have noticed it”</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#intro:search"><i class="fa fa-check"></i><b>1.4</b> Visual search: “I would have found it”</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#memory-i-would-have-remembered-it"><i class="fa fa-check"></i><b>1.5</b> Memory: “I would have remembered it”</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#the-development-of-a-self-model"><i class="fa fa-check"></i><b>1.6</b> The development of a self-model</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#this-thesis"><i class="fa fa-check"></i><b>1.7</b> This thesis</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-ch-search.html"><a href="2-ch-search.html"><i class="fa fa-check"></i><b>2</b> Zero-shot search termination reveals a dissociation between implicit and explicit metacognitive knowledge</a><ul>
<li class="chapter" data-level="2.1" data-path="2-ch-search.html"><a href="2-ch-search.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="2-ch-search.html"><a href="2-ch-search.html#experiment-1"><i class="fa fa-check"></i><b>2.2</b> Experiment 1</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-ch-search.html"><a href="2-ch-search.html#participants"><i class="fa fa-check"></i><b>2.2.1</b> Participants</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-ch-search.html"><a href="2-ch-search.html#procedure"><i class="fa fa-check"></i><b>2.2.2</b> Procedure</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-ch-search.html"><a href="2-ch-search.html#data-analysis"><i class="fa fa-check"></i><b>2.2.3</b> Data analysis</a></li>
<li class="chapter" data-level="2.2.4" data-path="2-ch-search.html"><a href="2-ch-search.html#results"><i class="fa fa-check"></i><b>2.2.4</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-ch-search.html"><a href="2-ch-search.html#experiment-2"><i class="fa fa-check"></i><b>2.3</b> Experiment 2</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-ch-search.html"><a href="2-ch-search.html#participants-1"><i class="fa fa-check"></i><b>2.3.1</b> Participants</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-ch-search.html"><a href="2-ch-search.html#procedure-1"><i class="fa fa-check"></i><b>2.3.2</b> Procedure</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-ch-search.html"><a href="2-ch-search.html#results-1"><i class="fa fa-check"></i><b>2.3.3</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-ch-search.html"><a href="2-ch-search.html#discussion"><i class="fa fa-check"></i><b>2.4</b> Discussion</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-ch-search.html"><a href="2-ch-search.html#is-implicit-metacognitive-knowledge-metacognitive"><i class="fa fa-check"></i><b>2.4.1</b> Is implicit metacognitive knowledge metacognitive?</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-ch-search.html"><a href="2-ch-search.html#inference-about-absence-as-a-tool-for-studying-implicit-self-knowledge"><i class="fa fa-check"></i><b>2.4.2</b> Inference about absence as a tool for studying implicit self knowledge</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-ch-search.html"><a href="2-ch-search.html#conclusion"><i class="fa fa-check"></i><b>2.4.3</b> Conclusion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-ch-RC.html"><a href="3-ch-RC.html"><i class="fa fa-check"></i><b>3</b> Paradoxical evidence weightings in confidence judgments for detection and discrimination</a><ul>
<li class="chapter" data-level="3.1" data-path="3-ch-RC.html"><a href="3-ch-RC.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="3-ch-RC.html"><a href="3-ch-RC.html#experiment-1-1"><i class="fa fa-check"></i><b>3.2</b> Experiment 1</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-ch-RC.html"><a href="3-ch-RC.html#methods"><i class="fa fa-check"></i><b>3.2.1</b> Methods</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-ch-RC.html"><a href="3-ch-RC.html#analysis"><i class="fa fa-check"></i><b>3.2.2</b> Analysis</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-ch-RC.html"><a href="3-ch-RC.html#results-2"><i class="fa fa-check"></i><b>3.2.3</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-ch-RC.html"><a href="3-ch-RC.html#experiment-2-1"><i class="fa fa-check"></i><b>3.3</b> Experiment 2</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-ch-RC.html"><a href="3-ch-RC.html#methods-1"><i class="fa fa-check"></i><b>3.3.1</b> Methods</a></li>
<li class="chapter" data-level="3.3.2" data-path="3-ch-RC.html"><a href="3-ch-RC.html#results-3"><i class="fa fa-check"></i><b>3.3.2</b> Results</a></li>
<li class="chapter" data-level="3.3.3" data-path="3-ch-RC.html"><a href="3-ch-RC.html#detection-signal-trials-1"><i class="fa fa-check"></i><b>3.3.3</b> Detection signal trials</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-ch-RC.html"><a href="3-ch-RC.html#discussion-1"><i class="fa fa-check"></i><b>3.4</b> Discussion</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-ch-RC.html"><a href="3-ch-RC.html#model-1-a-rational-agent-symmetric-evidence-structure"><i class="fa fa-check"></i><b>3.4.1</b> Model 1: a rational agent + symmetric evidence structure</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-ch-RC.html"><a href="3-ch-RC.html#model-2-a-rational-agent-symmetric-evidence-structure"><i class="fa fa-check"></i><b>3.4.2</b> Model 2: a rational agent + symmetric evidence structure</a></li>
<li class="chapter" data-level="3.4.3" data-path="3-ch-RC.html"><a href="3-ch-RC.html#model-3-confidence-decision-cross"><i class="fa fa-check"></i><b>3.4.3</b> Model 3: confidence decision cross</a></li>
<li class="chapter" data-level="3.4.4" data-path="3-ch-RC.html"><a href="3-ch-RC.html#evidence-for-absence"><i class="fa fa-check"></i><b>3.4.4</b> Evidence for absence</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><i class="fa fa-check"></i><b>4</b> Distinct neural contributions to metacognition for detecting (but not discriminating) visual stimuli</a><ul>
<li class="chapter" data-level="4.1" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#introduction-2"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#methods-and-materials"><i class="fa fa-check"></i><b>4.2</b> Methods and Materials</a><ul>
<li class="chapter" data-level="4.2.1" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#participants-4"><i class="fa fa-check"></i><b>4.2.1</b> Participants</a></li>
<li class="chapter" data-level="4.2.2" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#design-and-procedure"><i class="fa fa-check"></i><b>4.2.2</b> Design and procedure</a></li>
<li class="chapter" data-level="4.2.3" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#scanning-parameters"><i class="fa fa-check"></i><b>4.2.3</b> Scanning parameters</a></li>
<li class="chapter" data-level="4.2.4" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#analysis-1"><i class="fa fa-check"></i><b>4.2.4</b> Analysis</a></li>
<li class="chapter" data-level="4.2.5" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#exclusion-criteria"><i class="fa fa-check"></i><b>4.2.5</b> Exclusion criteria</a></li>
<li class="chapter" data-level="4.2.6" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#response-conditional-type-ii-roc-curves"><i class="fa fa-check"></i><b>4.2.6</b> Response conditional type-II ROC curves</a></li>
<li class="chapter" data-level="4.2.7" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#imaging-analysis"><i class="fa fa-check"></i><b>4.2.7</b> Imaging analysis</a></li>
<li class="chapter" data-level="4.2.8" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#statistical-inference-1"><i class="fa fa-check"></i><b>4.2.8</b> Statistical inference</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#results-4"><i class="fa fa-check"></i><b>4.3</b> Results</a></li>
<li class="chapter" data-level="4.4" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#behavioural-results"><i class="fa fa-check"></i><b>4.4</b> Behavioural results</a><ul>
<li class="chapter" data-level="4.4.1" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#imaging-results"><i class="fa fa-check"></i><b>4.4.1</b> Imaging results</a></li>
<li class="chapter" data-level="4.4.2" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#computational-models"><i class="fa fa-check"></i><b>4.4.2</b> Computational models</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#discussion-2"><i class="fa fa-check"></i><b>4.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="3-ch-RC.html"><a href="3-ch-RC.html#ch:RC"><i class="fa fa-check"></i><b>5</b> Prospective search time estimates for unseen displays reveal a rich intuitive theory of visual search</a><ul>
<li class="chapter" data-level="5.1" data-path="5-ch-RC.html"><a href="5-ch-RC.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a><ul>
<li class="chapter" data-level="5.1.1" data-path="5-ch-RC.html"><a href="5-ch-RC.html#meta-visual-search"><i class="fa fa-check"></i><b>5.1.1</b> Meta visual search</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5-ch-RC.html"><a href="5-ch-RC.html#experiment-1-exploratory"><i class="fa fa-check"></i><b>5.2</b> Experiment 1 (Exploratory)</a><ul>
<li class="chapter" data-level="5.2.1" data-path="5-ch-RC.html"><a href="5-ch-RC.html#participants-5"><i class="fa fa-check"></i><b>5.2.1</b> Participants</a></li>
<li class="chapter" data-level="5.2.2" data-path="5-ch-RC.html"><a href="5-ch-RC.html#procedure-2"><i class="fa fa-check"></i><b>5.2.2</b> Procedure</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html"><i class="fa fa-check"></i><b>6</b> Signal Detection Theory</a><ul>
<li class="chapter" data-level="6.1" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app1:ROC"><i class="fa fa-check"></i><b>6.1</b> ROC and zROC curves</a></li>
<li class="chapter" data-level="6.2" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app1:uvSDT"><i class="fa fa-check"></i><b>6.2</b> Unequal-variance (uv) SDT</a></li>
<li class="chapter" data-level="6.3" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app1:mc"><i class="fa fa-check"></i><b>6.3</b> SDT Measures for Metacognition</a></li>
<li class="chapter" data-level="6.4" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app2:PDRC"><i class="fa fa-check"></i><b>6.4</b> Pseudo-discrimination analysis</a><ul>
<li class="chapter" data-level="6.4.1" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#exp.-1"><i class="fa fa-check"></i><b>6.4.1</b> Exp. 1</a></li>
<li class="chapter" data-level="6.4.2" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#exp.-2"><i class="fa fa-check"></i><b>6.4.2</b> Exp. 2</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app2:simulation"><i class="fa fa-check"></i><b>6.5</b> Unequal-variance model</a><ul>
<li class="chapter" data-level="6.5.1" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#discrimination"><i class="fa fa-check"></i><b>6.5.1</b> Discrimination</a></li>
<li class="chapter" data-level="6.5.2" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#detection-1"><i class="fa fa-check"></i><b>6.5.2</b> Detection</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app3:buttonpresses"><i class="fa fa-check"></i><b>6.6</b> Confidence button presses</a></li>
<li class="chapter" data-level="6.7" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app3:zROC"><i class="fa fa-check"></i><b>6.7</b> zROC curves</a></li>
<li class="chapter" data-level="6.8" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app3:GC-DM"><i class="fa fa-check"></i><b>6.8</b> Global confidence design matrix</a></li>
<li class="chapter" data-level="6.9" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app3:ROIconf"><i class="fa fa-check"></i><b>6.9</b> Effect of confidence in our pre-specified ROIs</a></li>
<li class="chapter" data-level="6.10" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app3:varianceRat"><i class="fa fa-check"></i><b>6.10</b> SDT variance ratio correlation with the quadratic confidence effect</a></li>
<li class="chapter" data-level="6.11" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app3:efficiency"><i class="fa fa-check"></i><b>6.11</b> Correlation of metacognitive efficiency with linear and quadratic confidence effects</a></li>
<li class="chapter" data-level="6.12" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app3:cross"><i class="fa fa-check"></i><b>6.12</b> Confidence-decision cross classification</a></li>
<li class="chapter" data-level="6.13" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app3:SDT"><i class="fa fa-check"></i><b>6.13</b> Static Signal Detection Theory</a><ul>
<li class="chapter" data-level="6.13.1" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#discrimination-1"><i class="fa fa-check"></i><b>6.13.1</b> Discrimination</a></li>
<li class="chapter" data-level="6.13.2" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#detection-2"><i class="fa fa-check"></i><b>6.13.2</b> Detection</a></li>
</ul></li>
<li class="chapter" data-level="6.14" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app3:Dynamic"><i class="fa fa-check"></i><b>6.14</b> Dynamic Criterion</a><ul>
<li class="chapter" data-level="6.14.1" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#discrimination-2"><i class="fa fa-check"></i><b>6.14.1</b> Discrimination</a></li>
<li class="chapter" data-level="6.14.2" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#detection-3"><i class="fa fa-check"></i><b>6.14.2</b> Detection</a></li>
</ul></li>
<li class="chapter" data-level="6.15" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#app3:Monitoring"><i class="fa fa-check"></i><b>6.15</b> Attention Monitoring</a><ul>
<li class="chapter" data-level="6.15.1" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#discrimination-3"><i class="fa fa-check"></i><b>6.15.1</b> Discrimination</a></li>
<li class="chapter" data-level="6.15.2" data-path="6-app1-SDT.html"><a href="6-app1-SDT.html#detection-4"><i class="fa fa-check"></i><b>6.15.2</b> Detection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html"><i class="fa fa-check"></i>General Discussion</a><ul>
<li class="chapter" data-level="6.16" data-path="general-discussion.html"><a href="general-discussion.html#summary-of-results"><i class="fa fa-check"></i><b>6.16</b> Summary of results</a></li>
<li class="chapter" data-level="6.17" data-path="general-discussion.html"><a href="general-discussion.html#inference-about-absence-without-self-knowledge"><i class="fa fa-check"></i><b>6.17</b> Inference about absence without self-knowledge?</a></li>
<li class="chapter" data-level="6.18" data-path="general-discussion.html"><a href="general-discussion.html#future-directions"><i class="fa fa-check"></i><b>6.18</b> Future directions</a><ul>
<li class="chapter" data-level="6.18.1" data-path="general-discussion.html"><a href="general-discussion.html#failures-of-a-self-model"><i class="fa fa-check"></i><b>6.18.1</b> Failures of a self-model</a></li>
</ul></li>
<li class="chapter" data-level="6.19" data-path="general-discussion.html"><a href="general-discussion.html#conclusion-1"><i class="fa fa-check"></i><b>6.19</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Self-Modeling in Inference about Absence</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch:RC" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Prospective search time estimates for unseen displays reveal a rich intuitive theory of visual search</h1>
<div id="matan-mazor-max-siegel-joshua-b.-tenenbaum" class="section level4 unnumbered">
<h4>Matan Mazor, Max Siegel &amp; Joshua B. Tenenbaum</h4>
<p>abstract</p>
</div>
<div id="introduction-3" class="section level2">
<h2><span class="header-section-number">5.1</span> Introduction</h2>
<p>The <em>Intuitive Theories</em> approach to cognitive science <span class="citation">(Gerstenberg &amp; Tenenbaum, 2017)</span> has been successful in accounting for human knowledge and reasoning in the domains of physics <span class="citation">(McCloskey, 1983)</span>, psychology <span class="citation">(Baker, Saxe, &amp; Tenenbaum, 2011)</span>, and semantic knowledge <span class="citation">(Gelman &amp; Legare, 2011)</span>, among others. In recent years, careful experimental and computational work has advanced our understanding of these models: their ontologies and causal laws, the abstractions that they make, and the consequences of these abstractions for faithfully and efficiently modeling the real world. For example, the computational specification of the intuitive physics model and its deviation from Newtonian physics was informed by people’s biased intuitions about the consequences of object collisions <span class="citation">(Sanborn, Mansinghka, &amp; Griffiths, 2013; Smith &amp; Vul, 2013)</span>.</p>
<p>Theoretically, there is no reason to believe that Intuitive Theories should be limited in their scope to modeling the external environment and other agents. Indeed, agents may benefit from having an intuitive theory or a simplified model of their own perceptual, cognitive and psychological states. For example, in the context of memory, it has been suggested that knowing which items are more subjectively memorable is useful for making negative recognition judgments <span class="citation">(“I would have remembered this object if I saw it”; Brown, Lewis, &amp; Monk, 1977)</span>. More relevant for our focus here, <span class="citation">Graziano &amp; Webb (2015)</span> argue that having a simplified <em>Attention Schema</em> (an intuitive theory of attention and its dynamics) may aid in monitoring and controlling one’s attention, similar to how a body-schema supports motor control.</p>
<p>Still, little experimental work has been devoted to characterizing the computational specifications of this intuitive theory of attention. Is it based on a simulation engine <span class="citation">(similar to the game engine proposal; Ullman, Spelke, Battaglia, &amp; Tenenbaum, 2017)</span>? Or instead formatted as a list of propositions (e.g., <em>‘My attention span is shorter when I am tired’</em>)? How accurate is it? To what extent is it learned from experience and what inductive biases guide its acquisition and tuning?</p>
<p>Here we take a first step in this direction, using visual search as our model test-case. We introduce a new task in which participants perform a series of visual search trials after predicting their prospective search times. Similar to using colliding balls and falling blocks to study intuitive physics, here we chose visual search for being thoroughly studied and for being amenable to relatively simple modeling, at least for simple stimuli.</p>
<div id="meta-visual-search" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Meta visual search</h3>
<p>We designed a paradigm to directly measure prior beliefs about the expected duration of various visual search tasks, and their alignment with actual search times. First, participants were familiarized with visual search and performed four trials of a visual search task (“Find the <em>T</em> among the 7 <em>L</em>s”). In this Familiarization part, participants received feedback about their accuracy and timing. After Familiarization, participants were asked to estimate how long it would take them to perform different visual search tasks, involving novel stimuli and set sizes (Estimation). In the final Visual Search part, participants performed three trials of each visual search task presented in the Estimation part. Bonus points were awarded for responding as fast or faster than the estimated time, and for giving low search time estimates. The bonus system was designed to motivate participants to give unbiased search-time estimates, and to later do their best in the visual search task.</p>
</div>
</div>
<div id="experiment-1-exploratory" class="section level2">
<h2><span class="header-section-number">5.2</span> Experiment 1 (Exploratory)</h2>
<div id="participants-5" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Participants</h3>
<p>100 participants were recruited from Amazon’s crowdsourcing web-service Mechanical Turk. The experiment took about 20
minutes to complete. Each participant was paid $2.50. The highest performing 30% of participants received an additional bonus of $1.50.</p>
</div>
<div id="procedure-2" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Procedure</h3>
<p>The study was built using the Lab.js platform <span class="citation">(Henninger, Shevchenko, Mertens, Kieslich, &amp; Hilbig, 2019)</span> and hosted on a JATOS server <span class="citation">(Lange, Kühn, &amp; Filevich, 2015)</span></p>
<div id="familiarization" class="section level4 unnumbered">
<h4>Familiarization</h4>
<p>First, participants were acquainted with the visual search task. The instructions for this part were as follows:</p>
<blockquote>
<p>In the first part, you will find a target hidden among distractors. First, a gray cross will appear on the screen. Look at the cross. Then, the target and distractors will appear. When you spot the target, press the spacebar as quickly as possible. Upon pressing the spacebar, the target and distractors will be replaced by up to 5 numbers. To move to the next trial, type in the number that replaced the target.</p>
</blockquote>
<p>The instructions were followed by four trials of an example visual search task (searching for a <em>T</em> among 7 <em>L</em>s). Feedback was delivered on speed and accuracy. The purpose of this part of the experiment was to familiarize participants with the task.</p>
</div>
<div id="estimation" class="section level4 unnumbered">
<h4>Estimation</h4>
<p>After familiarization, participants estimated how long it would take them to perform various visual search tasks involving novel stimuli and various set sizes. On each trial, they were presented with a target stimulus and a display of distractors and were asked to estimate how long it would take to find the target if it was hidden among the distractors. Specifically, participants were advised:</p>
<blockquote>
<p>Imagine that the square will be thoroughly shaken after we add the target to it, so the specific locations and orientations of the target and distractors are likely to change. Only the identity and number of distractors are relevant here.</p>
</blockquote>
<p>To motivate accurate estimates, we explained that these visual search tasks will be performed in the last part of the experiment, and that bonus points will be awarded for trials in which participants respond as fast or faster than their estimation. The number of points awarded for a successful search changed as a function of the search time estimate according to the rule
<span class="math inline">\(points=\frac{1}{\sqrt{secs}}\)</span>. This rule was chosen for being exponential with respect to the log response times, incentivizing participants to be consistent in their ratings across short and long search tasks.</p>
<div class="figure" style="text-align: center"><span id="fig:ch4-exp1-design"></span>
<img src="figure/ch4/methods1.png" alt="The meta visual search paradigm. Participants first performed five similar visual search trials and received feedback about their speed and accuracy. Then, they were asked to estimate the duration of novel visual search tasks. Bonus points were awarded for accurate estimates, and more points were awarded for risky estimates. Finally, in the visual search part participants performed three consecutive trials of each visual search task for which they gave a search time estimate." width="100%" />
<p class="caption">
Figure 5.1: The meta visual search paradigm. Participants first performed five similar visual search trials and received feedback about their speed and accuracy. Then, they were asked to estimate the duration of novel visual search tasks. Bonus points were awarded for accurate estimates, and more points were awarded for risky estimates. Finally, in the visual search part participants performed three consecutive trials of each visual search task for which they gave a search time estimate.
</p>
</div>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="6-app1-SDT.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["thesis.pdf", "PDF"], ["thesis.epub", "EPUB"], ["thesis.docx", "Word"]],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
