<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Zero-shot search termination reveals a dissociation between implicit and explicit metacognitive knowledge | Self-Modeling in Inference about Absence</title>
  <meta name="description" content="Chapter 2 Zero-shot search termination reveals a dissociation between implicit and explicit metacognitive knowledge | Self-Modeling in Inference about Absence" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Zero-shot search termination reveals a dissociation between implicit and explicit metacognitive knowledge | Self-Modeling in Inference about Absence" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Zero-shot search termination reveals a dissociation between implicit and explicit metacognitive knowledge | Self-Modeling in Inference about Absence" />
  
  
  

<meta name="author" content="Matan Mazor" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="3-math-sci.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"></a></li>
<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> This will automatically install the {remotes} package and {thesisdown}</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#inference-about-absence"><i class="fa fa-check"></i><b>1.1</b> Inference about absence</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#formalabsence"><i class="fa fa-check"></i><b>1.2</b> Probabilistic reasoning, criterion setting, and self knowledge</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#symmetrical-definition"><i class="fa fa-check"></i>Symmetrical definition:</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dissymmetrical-definition"><i class="fa fa-check"></i>Dissymmetrical definition:</a></li>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#second-order-cognition"><i class="fa fa-check"></i><b>1.2.1</b> Second-order cognition</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#detectionmodels"><i class="fa fa-check"></i><b>1.2.2</b> Computational models of detection</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#htm"><i class="fa fa-check"></i>The High-Threshold model</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#sdt"><i class="fa fa-check"></i>Signal Detection Theory</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#detection-i-would-have-noticed-it"><i class="fa fa-check"></i><b>1.3</b> Detection: “I would have noticed it”</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#visual-search-i-would-have-found-it"><i class="fa fa-check"></i><b>1.4</b> Visual search: “I would have found it”</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#memory-i-would-have-remembered-it"><i class="fa fa-check"></i><b>1.5</b> Memory: “I would have remembered it”</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#the-development-of-a-self-model"><i class="fa fa-check"></i><b>1.6</b> The development of a self-model</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#this-thesis"><i class="fa fa-check"></i><b>1.7</b> This thesis</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-ch-search.html"><a href="2-ch-search.html"><i class="fa fa-check"></i><b>2</b> Zero-shot search termination reveals a dissociation between implicit and explicit metacognitive knowledge</a><ul>
<li class="chapter" data-level="2.1" data-path="2-ch-search.html"><a href="2-ch-search.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="2-ch-search.html"><a href="2-ch-search.html#experiment-1"><i class="fa fa-check"></i><b>2.2</b> Experiment 1</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-ch-search.html"><a href="2-ch-search.html#participants"><i class="fa fa-check"></i><b>2.2.1</b> Participants</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-ch-search.html"><a href="2-ch-search.html#procedure"><i class="fa fa-check"></i><b>2.2.2</b> Procedure</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-ch-search.html"><a href="2-ch-search.html#data-analysis"><i class="fa fa-check"></i><b>2.2.3</b> Data analysis</a></li>
<li class="chapter" data-level="2.2.4" data-path="2-ch-search.html"><a href="2-ch-search.html#results"><i class="fa fa-check"></i><b>2.2.4</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-ch-search.html"><a href="2-ch-search.html#experiment-2"><i class="fa fa-check"></i><b>2.3</b> Experiment 2</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-ch-search.html"><a href="2-ch-search.html#participants-1"><i class="fa fa-check"></i><b>2.3.1</b> Participants</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-ch-search.html"><a href="2-ch-search.html#procedure-1"><i class="fa fa-check"></i><b>2.3.2</b> Procedure</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-ch-search.html"><a href="2-ch-search.html#results-1"><i class="fa fa-check"></i><b>2.3.3</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-ch-search.html"><a href="2-ch-search.html#discussion"><i class="fa fa-check"></i><b>2.4</b> Discussion</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-ch-search.html"><a href="2-ch-search.html#is-implicit-metacognitive-knowledge-metacognitive"><i class="fa fa-check"></i><b>2.4.1</b> Is implicit metacognitive knowledge metacognitive?</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-ch-search.html"><a href="2-ch-search.html#inference-about-absence-as-a-tool-for-studying-implicit-self-knowledge"><i class="fa fa-check"></i><b>2.4.2</b> Inference about absence as a tool for studying implicit self knowledge</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-ch-search.html"><a href="2-ch-search.html#conclusion"><i class="fa fa-check"></i><b>2.4.3</b> Conclusion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-math-sci.html"><a href="3-math-sci.html"><i class="fa fa-check"></i><b>3</b> Mathematics and Science</a><ul>
<li class="chapter" data-level="3.1" data-path="3-math-sci.html"><a href="3-math-sci.html#math"><i class="fa fa-check"></i><b>3.1</b> Math</a></li>
<li class="chapter" data-level="3.2" data-path="3-math-sci.html"><a href="3-math-sci.html#chemistry-101-symbols"><i class="fa fa-check"></i><b>3.2</b> Chemistry 101: Symbols</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-math-sci.html"><a href="3-math-sci.html#typesetting-reactions"><i class="fa fa-check"></i><b>3.2.1</b> Typesetting reactions</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-math-sci.html"><a href="3-math-sci.html#other-examples-of-reactions"><i class="fa fa-check"></i><b>3.2.2</b> Other examples of reactions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-math-sci.html"><a href="3-math-sci.html#physics"><i class="fa fa-check"></i><b>3.3</b> Physics</a></li>
<li class="chapter" data-level="3.4" data-path="3-math-sci.html"><a href="3-math-sci.html#biology"><i class="fa fa-check"></i><b>3.4</b> Biology</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><i class="fa fa-check"></i><b>4</b> Distinct neural contributions to metacognition for detecting (but not discriminating) visual stimuli</a><ul>
<li class="chapter" data-level="4.1" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#introduction-1"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#methods-and-materials"><i class="fa fa-check"></i><b>4.2</b> Methods and Materials</a><ul>
<li class="chapter" data-level="4.2.1" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#participants-2"><i class="fa fa-check"></i><b>4.2.1</b> Participants</a></li>
<li class="chapter" data-level="4.2.2" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#design-and-procedure"><i class="fa fa-check"></i><b>4.2.2</b> Design and procedure</a></li>
<li class="chapter" data-level="4.2.3" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#scanning-parameters"><i class="fa fa-check"></i><b>4.2.3</b> Scanning parameters</a></li>
<li class="chapter" data-level="4.2.4" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#analysis"><i class="fa fa-check"></i><b>4.2.4</b> Analysis</a></li>
<li class="chapter" data-level="4.2.5" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#exclusion-criteria"><i class="fa fa-check"></i><b>4.2.5</b> Exclusion criteria</a></li>
<li class="chapter" data-level="4.2.6" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#response-conditional-type-ii-roc-curves"><i class="fa fa-check"></i><b>4.2.6</b> Response conditional type-II ROC curves</a></li>
<li class="chapter" data-level="4.2.7" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#imaging-analysis"><i class="fa fa-check"></i><b>4.2.7</b> Imaging analysis</a></li>
<li class="chapter" data-level="4.2.8" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#statistical-inference"><i class="fa fa-check"></i><b>4.2.8</b> Statistical inference</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#results-2"><i class="fa fa-check"></i><b>4.3</b> Results</a></li>
<li class="chapter" data-level="4.4" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#behavioural-results"><i class="fa fa-check"></i><b>4.4</b> Behavioural results</a><ul>
<li class="chapter" data-level="4.4.1" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#imaging-results"><i class="fa fa-check"></i><b>4.4.1</b> Imaging results</a></li>
<li class="chapter" data-level="4.4.2" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#computational-models"><i class="fa fa-check"></i><b>4.4.2</b> Computational models</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#discussion-1"><i class="fa fa-check"></i><b>4.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="conclusion-1.html"><a href="conclusion-1.html"><i class="fa fa-check"></i>Conclusion</a></li>
<li class="chapter" data-level="5" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html"><i class="fa fa-check"></i><b>5</b> Signal Detection Theory</a><ul>
<li class="chapter" data-level="5.1" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app1:ROC"><i class="fa fa-check"></i><b>5.1</b> ROC and zROC curves</a></li>
<li class="chapter" data-level="5.2" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app1:uvSDT"><i class="fa fa-check"></i><b>5.2</b> Unequal-variance (uv) SDT</a></li>
<li class="chapter" data-level="5.3" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app1:mc"><i class="fa fa-check"></i><b>5.3</b> SDT Measures for Metacognition</a></li>
<li class="chapter" data-level="5.4" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app3:buttonpresses"><i class="fa fa-check"></i><b>5.4</b> Confidence button presses</a></li>
<li class="chapter" data-level="5.5" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app3:zROC"><i class="fa fa-check"></i><b>5.5</b> zROC curves</a></li>
<li class="chapter" data-level="5.6" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app3:GC-DM"><i class="fa fa-check"></i><b>5.6</b> Global confidence design matrix</a></li>
<li class="chapter" data-level="5.7" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app3:ROIconf"><i class="fa fa-check"></i><b>5.7</b> Effect of confidence in our pre-specified ROIs</a></li>
<li class="chapter" data-level="5.8" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app3:varianceRat"><i class="fa fa-check"></i><b>5.8</b> SDT variance ratio correlation with the quadratic confidence effect</a></li>
<li class="chapter" data-level="5.9" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app3:efficiency"><i class="fa fa-check"></i><b>5.9</b> Correlation of metacognitive efficiency with linear and quadratic confidence effects</a></li>
<li class="chapter" data-level="5.10" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app3:cross"><i class="fa fa-check"></i><b>5.10</b> Confidence-decision cross classification</a></li>
<li class="chapter" data-level="5.11" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app3:SDT"><i class="fa fa-check"></i><b>5.11</b> Static Signal Detection Theory</a><ul>
<li class="chapter" data-level="5.11.1" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#discrimination"><i class="fa fa-check"></i><b>5.11.1</b> Discrimination</a></li>
<li class="chapter" data-level="5.11.2" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#detection"><i class="fa fa-check"></i><b>5.11.2</b> Detection</a></li>
</ul></li>
<li class="chapter" data-level="5.12" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app3:Dynamic"><i class="fa fa-check"></i><b>5.12</b> Dynamic Criterion</a><ul>
<li class="chapter" data-level="5.12.1" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#discrimination-1"><i class="fa fa-check"></i><b>5.12.1</b> Discrimination</a></li>
<li class="chapter" data-level="5.12.2" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#detection-1"><i class="fa fa-check"></i><b>5.12.2</b> Detection</a></li>
</ul></li>
<li class="chapter" data-level="5.13" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#app3:Monitoring"><i class="fa fa-check"></i><b>5.13</b> Attention Monitoring</a><ul>
<li class="chapter" data-level="5.13.1" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#discrimination-2"><i class="fa fa-check"></i><b>5.13.1</b> Discrimination</a></li>
<li class="chapter" data-level="5.13.2" data-path="5-app1-SDT.html"><a href="5-app1-SDT.html#detection-2"><i class="fa fa-check"></i><b>5.13.2</b> Detection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Self-Modeling in Inference about Absence</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch:search" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Zero-shot search termination reveals a dissociation between implicit and explicit metacognitive knowledge</h1>
<div id="matan-mazor-stephen-m.-fleming" class="section level4 unnumbered">
<h4>Matan Mazor, Stephen M. Fleming</h4>
<p>In order to infer that a target item is missing from a display, subjects must know that they would have detected it if it was present. This form of counterfactual reasoning critically relies on metacognitive knowledge about spatial attention and visual search behaviour. Previous work on visual search established that this knowledge is constructed and expanded based on task experience. Here we show that some metacognitive knowledge is also available to participants in the first few trials of the task, and that this knowledge can be used to guide decisions about search termination even if it is not available for explicit report.</p>
</div>
<div id="introduction" class="section level2">
<h2><span class="header-section-number">2.1</span> Introduction</h2>
<p>Searching for the only blue letter in an array of yellow letters is easy, but searching for the only blue X among an array of yellow Xs and blue Ts is much harder <span class="citation">(Treisman &amp; Gelade, 1980)</span>. This difference manifests in the time taken to find the target letter, but also in the time taken to conclude that the target letter is missing. In other words, easier searches not only make it easier to detect the presence of a target, but also to infer its absence. Differences in the speed of detecting the presence of a target have been attributed to pre-attentional mechanisms <span class="citation">(Treisman &amp; Gelade, 1980)</span> and guiding signals <span class="citation">(Wolfe, 2021; Wolfe &amp; Gray, 2007)</span>, that can sometimes make the target item ‘pop out’ immediately, without any attentional effort. In target-absent trials, however, there is nothing in the display to pop-out. This reasis a fundamental question: what makes some decisions about target absence easier than others?</p>
<p>Metacognitive beliefs about the expected time taken to detect a target can draw on previous experience in the task. Indeed, search time in target-absent trials decreases following successful target-present trials, and sharply increases following target misses <span class="citation">(Chun &amp; Wolfe, 1996)</span>. This simple heuristic provided an excellent fit to data from a visual search task with hundreds of trials. However, in everyday life visual searches rarely come in a blocks of hundreds of similar trials, such that relying on previous repetitions of the same search to guide search termination is impossible <span class="citation">(Wolfe, 2021)</span>. Only the first trials of a visual search experiment, where participants meet the stimuli for the first time, are a good model of this <em>zero-shot search termination</em> behaviour. In these trials, search time should rely solely on metacognitive beliefs about search efficiency that are available to subjects prior to engaging with the task. This fact makes search time in the first few trials of a task a critical window into participants’ metacognitive knowledge about attention and visual search. Furthermore, participants’ ability to learn from positive examples (target-present trials), and their ability to generalize their knowledge across stimulus types and displays, offers an opportunity to study the structure of this simplified metacognitive knowledge, its building blocks, and the inductive biases that guide its acquisition. In this study, we use target-absent trials in visual search to ask what participants know about their spatial attention before engaging with the visual search task, and how this knowledge is built and expanded based on experience.</p>
<p>In two pre-registered experiments here we focus on feature search for colour and shape. Focusing on the first four trials in a visual search task, we ask whether prior experience with the task and stimuli is necessary for efficient search termination in feature searches. Unlike typical visual search experiments that comprise hundreds or thousands of trials, here we collect only a handful of trials from a large pool of online participants. This unusual design allows us to reliably identify search time patterns in the first trials of the experiment. Furthermore, by making sure that the first displays do not include the target stimulus, we are able for the first time to ask what knowledge is available to participants about their expected search efficiency prior to engaging with the task.</p>
<p>We dub this approach <em>zero-shot search termination</em> in a tribute to the study of ‘zero-shot learning’ in machine learning: the ability to classify unseen categories of stimuli, based on generalizable knowledge from other categories <span class="citation">(Xian, Schiele, &amp; Akata, 2017)</span>. Efficient (i.e., fast and accurate) quitting in target-absent trials prior to any target-present trials would indicate that knowledge about the salience of a divergent color or shape is available at some form in the cognitive system, and that this knowledge can flexibly be put to use for counterfactual reasoning in the process of inference about absence. Conversely, inefficient search in these first trials would mean that positive experience is necessary for this knowledge to be acquired, or to be expressed.</p>
</div>
<div id="experiment-1" class="section level2">
<h2><span class="header-section-number">2.2</span> Experiment 1</h2>
<p>In Experiment 1, we examined search termination in the case of colour search. When searching for a deviant colour, the number of distractors has virtually no effect on search time <span class="citation">(<em>colour pop-out</em>; e.g., D’Zmura, 1991)</span>, for both ‘target present’ and ‘target absent’ responses. Here we asked whether efficient quitting in colour search is dependent on task experience. A detailed pre-registration document for Experiment 1 can be accessed via the following link: <a href="https://osf.io/yh82v/">https://osf.io/yh82v/</a>.</p>
<div id="participants" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Participants</h3>
<p>The research complied with all relevant ethical regulations, and was approved by the Research Ethics Committee of University College London (study ID number 1260/003). 1187 Participants were recruited via Prolific, and gave their informed consent prior to their participation. They were selected based on their acceptance rate (&gt;95%) and for being native English speakers. Following our pre-registration, we collected data until we reached 320 included participants for each of our pre-registered hypotheses (after applying our pre-registered exclusion criteria). The entire experiment took around 3 minutes to complete (median completion time: 3.19 minutes). Participants were paid £0.38 for their participation, equivalent to an hourly wage of £ 7.14.</p>
</div>
<div id="procedure" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Procedure</h3>
<p>A static version of Experiment 1 can be accessed on <a href="matanmazor.github.io/termination/experiments/demos/exp1/">matanmazor.github.io/termination/experiments/demos/exp1/</a>. Participants were first instructed about the visual search task. Specifically, that their task is to report, as accurately and quickly as possible, whether a target stimulus was present (press ‘J’) or absent (press ‘F’). Then, practice trials were delivered, in which the target stimulus was a rotated <em>T</em>, and distractors are rotated <em>L</em>s. The purpose of the practice trials was to familiarize participants with the structure of the task. For these practice trials the number of items was always 3. Practice trials were delivered in small blocks of 6 trials each, and the main part of the experiment started only once participants responded correctly on at least five trials in a block (see Figure <a href="2-ch-search.html#fig:ch1-exp1-design">2.1</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:ch1-exp1-design"></span>
<img src="figure/ch1/designExp1.png" alt="Experimental design. Top panel: each visual search trial started with a screen indicating the target stimulus. The search display remained visible until a response is recorded. To motivate accurate responses, the feedback screen remained visible for one second following correct responses and for four seconds following errors. Middle panel: after reading the instructions, participants practiced the visual search task in blocks of 6 trials, until they had reached an accuracy level of 0.83 correct or higher (at most one error per block of 6 trials). Bottom panel: the main part of the experiment comprised 12 trials only, in which the target was a red dot. Unbeknown the subjects, only trials 5-8 (Block 2) were target-present trials, and the remaining trials were target-absent trials. Each 4-trial block followed a 2 by 2 design, with factors being set size (4 or 8) and distractor type (color or conjunction; blue dots only or blue dots and red squares, respectively)." width="\textwidth" />
<p class="caption">
Figure 2.1: Experimental design. Top panel: each visual search trial started with a screen indicating the target stimulus. The search display remained visible until a response is recorded. To motivate accurate responses, the feedback screen remained visible for one second following correct responses and for four seconds following errors. Middle panel: after reading the instructions, participants practiced the visual search task in blocks of 6 trials, until they had reached an accuracy level of 0.83 correct or higher (at most one error per block of 6 trials). Bottom panel: the main part of the experiment comprised 12 trials only, in which the target was a red dot. Unbeknown the subjects, only trials 5-8 (Block 2) were target-present trials, and the remaining trials were target-absent trials. Each 4-trial block followed a 2 by 2 design, with factors being set size (4 or 8) and distractor type (color or conjunction; blue dots only or blue dots and red squares, respectively).
</p>
</div>
<p>In the main part of the experiment, participants searched for a red dot among blue dots or a mixed array of blue dots and red squares. Set size was set to 4 or 8, resulting in a 2-by-2 design (search type: color or color<span class="math inline">\(\times\)</span>shape, by set size: 4 or 8). Critically, and unbeknown to subjects, the first four trials were always target-absent trials (one of each set-size <span class="math inline">\(\times\)</span> search-type combination), presented in randomized order. These trials were followed by the four corresponding target-present trials, presented in randomized order. The final four trials were again target-absent trials, presented in randomized order.</p>
<div id="randomization" class="section level4 unnumbered">
<h4>Randomization</h4>
<p>The order and timing of experimental events was determined pseudo-randomly by the Mersenne Twister pseudorandom number generator, initialized in a way that ensures registration time-locking <span class="citation">(Mazor, Mazor, &amp; Mukamel, 2019)</span>.</p>
</div>
</div>
<div id="data-analysis" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Data analysis</h3>
<div id="rejection-criteria" class="section level4 unnumbered">
<h4>Rejection criteria</h4>
<p>Participants were excluded for making more than one error in the main part of the experiment, or for having extremely fast or slow reaction times in one or more of the tasks (below 250 milliseconds or above 5 seconds in more than 25% of the trials).</p>
<p>Error trials, and trials with response times below 250 milliseconds or above 1 second were excluded from the response-time analysis.</p>
</div>
<div id="data-preprocessing" class="section level4 unnumbered">
<h4>Data preprocessing</h4>
<p>To control for within-block trial order effects, a separate linear regression model was fitted to the data of each block, predicting search time as a function of trial serial order (<span class="math inline">\(RT \sim \beta_0+\beta_1i\)</span>, with <span class="math inline">\(i\)</span> denoting the mean-centered serial position within a block). Search times were corrected by subtracting the product of the slope and the mean-centered serial position, in a block-wise manner.</p>
<p>Subject-wise search slopes were then extracted for each combination of search type (color or conjunction) and block number by fitting a linear regression model to the reaction time data with one intercept and one set-size term.</p>
</div>
<div id="hypotheses-and-analysis-plan" class="section level4 unnumbered">
<h4>Hypotheses and analysis plan</h4>
<p>Experiment 1 was designed to test several hypotheses about the contribution of metacognitive knowledge to search termination, the state of this knowledge prior to engaging with the task, and the effect of experience trials on this metacognitive knowledge. The specifics of our pre-registered analysis can be accessed in the following link: <a href="https://osf.io/ea385">https://osf.io/ea385</a>. We outline some possible search time patterns and their pre-registered interpretation in Fig. <a href="2-ch-search.html#fig:ch1-models">2.2</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:ch1-models"></span>
<img src="figure/ch1/models.png" alt="Visualization of Hypotheses. Top left: typical search time results in visual search experiments with many trials (where TP =  Target Present responses; TA = Target Absent responses). Set size (x axis) affects search time in conjunction search, but much less so in color search. However, it is unclear whether this pattern of target-absent search also holds in the first trials in an experiment. Different models make different predictions about target-absent serach times in the first block of the experiment. Top right: one possible pattern is that the same qualitative pattern will be observed in our design, with an overall decrease in response time as a function of trial number. This would suggest that the metacognitive knowledge necessary to support efficient inference about absence was already in place before engaging with the task. Bottom left: an alternative pattern is that the same qualitative pattern will be observed for blocks 2 and 3, but not in block 1. This would suggest that for inference about absence to be efficient, participants had to first experience some target-present trials. Bottom right: alternatively, some degree of metacognitive knowledge may be available prior to engaging with the task, with some being acquired by subsequent exposure to target-present trials. This would manifest as different slopes for conjunction and color searches in blocks 1 and a learning effect for color search between blocks 1 and 3." width="\textwidth" />
<p class="caption">
Figure 2.2: Visualization of Hypotheses. Top left: typical search time results in visual search experiments with many trials (where TP = Target Present responses; TA = Target Absent responses). Set size (x axis) affects search time in conjunction search, but much less so in color search. However, it is unclear whether this pattern of target-absent search also holds in the first trials in an experiment. Different models make different predictions about target-absent serach times in the first block of the experiment. Top right: one possible pattern is that the same qualitative pattern will be observed in our design, with an overall decrease in response time as a function of trial number. This would suggest that the metacognitive knowledge necessary to support efficient inference about absence was already in place before engaging with the task. Bottom left: an alternative pattern is that the same qualitative pattern will be observed for blocks 2 and 3, but not in block 1. This would suggest that for inference about absence to be efficient, participants had to first experience some target-present trials. Bottom right: alternatively, some degree of metacognitive knowledge may be available prior to engaging with the task, with some being acquired by subsequent exposure to target-present trials. This would manifest as different slopes for conjunction and color searches in blocks 1 and a learning effect for color search between blocks 1 and 3.
</p>
</div>
<p>Analysis comprised a positive control based on target-present trials, a test of the presence of a pop-out effect for target-absent color search in block 1, and a test for the change in slope for target-absent color search between blocks 1 and 3. All hypotheses were tested using a within-subject t-test, with a significance level of 0.05.
Given the fact that we only have one trial per cell, one excluded trial is sufficient to make some hypotheses impossible to test on a given participant. For this reason, for each hypothesis separately, participants were included only if all necessary trials met our inclusion criteria. This meant that some hypotheses were tested on different subsets of participants.</p>
<p>We used R <span class="citation">(Version 3.6.0; R Core Team, 2019)</span> and the R-packages <em>BayesFactor</em> <span class="citation">(Version 0.9.12.4.2; Morey &amp; Rouder, 2018)</span>, <em>cowplot</em> <span class="citation">(Version 1.0.0; Wilke, 2019)</span>, <em>dplyr</em> <span class="citation">(Version 1.0.4; Wickham, François, Henry, &amp; Müller, 2020)</span>, <em>ggplot2</em> <span class="citation">(Version 3.3.1; Wickham, 2016)</span>, <em>jsonlite</em> <span class="citation">(Version 1.7.1; Ooms, 2014)</span>, <em>lsr</em> <span class="citation">(Version 0.5; Navarro, 2015)</span>, <em>MESS</em> <span class="citation">(Version 0.5.6; Ekstrøm, 2019)</span>, <em>papaja</em> <span class="citation">(Version 0.1.0.9942; Aust &amp; Barth, 2020)</span>, <em>pwr</em> <span class="citation">(Version 1.3.0; Champely, 2020)</span>, <em>reticulate</em> <span class="citation">(Version 1.16; Ushey, Allaire, &amp; Tang, 2020)</span>, and <em>tidyr</em> <span class="citation">(Version 1.1.0; Wickham &amp; Henry, n.d.)</span> for all our analyses.</p>
</div>
</div>
<div id="results" class="section level3">
<h3><span class="header-section-number">2.2.4</span> Results</h3>
<p>Overall mean accuracy was 0.95 (standard deviation =0.06). Median reaction time was 623.98 ms (median absolute deviation = 127.37). In all further analyses, only correct trials with response times between 250 and 1000 ms are included.</p>
<p><em>Hypothesis 1 (positive control)</em>: Search times in block 2 (target-present) followed the expected pattern, with a steep slope for conjunction search (<span class="math inline">\(M = 12.52\)</span>, 95% CI <span class="math inline">\([10.08\)</span>, <span class="math inline">\(14.95]\)</span>) and a shallow slope for conjunction search (<span class="math inline">\(M = 3.91\)</span>, 95% CI <span class="math inline">\([2.13\)</span>, <span class="math inline">\(5.70]\)</span>; see middle panel in Fig. <a href="2-ch-search.html#fig:ch1-exp1-results">2.3</a>). The slope for color search was significantly lower than 10 ms/item and thus met our criterion for being considered ‘pop-out’ (<span class="math inline">\(t(961) = -6.69\)</span>, <span class="math inline">\(p &lt; .001\)</span>). Furthermore, the difference between the slopes was significant (<span class="math inline">\(t(749) = 6.50\)</span>, <span class="math inline">\(p &lt; .001\)</span>). This positive control served to validate our method of using two trials per participant for obtaining reliable group-level estimates of search slopes.</p>
<p><em>Hypothesis 2</em>: Our central focus was on results from block 1 (target-absent). Here participants didn’t yet have experience with searching for the red dot. Similar to the second block, the slope for the conjunction search was steep (<span class="math inline">\(M = 18.41\)</span>, 95% CI <span class="math inline">\([14.95\)</span>, <span class="math inline">\(21.87]\)</span>). A clear ‘pop-out’ effect for color search was also evident (<span class="math inline">\(M = 0.15\)</span>, 95% CI <span class="math inline">\([-\infty\)</span>, <span class="math inline">\(2.31]\)</span>, <span class="math inline">\(t(886) = -7.51\)</span>, <span class="math inline">\(p &lt; .001\)</span>). Furthermore, the average search slope for color search in this first block was significantly different from that of the conjunction search (<span class="math inline">\(t(413) = 6.55\)</span>, <span class="math inline">\(p &lt; .001\)</span>; see leftmost panel in Fig. <a href="2-ch-search.html#fig:ch1-exp1-results">2.3</a>), indicating that a color-absence pop-out is already in place prior to direct task experience. This result is in line with the <em>prior-knowledge only</em> model (see Fig. <a href="2-ch-search.html#fig:ch1-models">2.2</a>), in which participants have valid expectations for efficient color search, prior to engaging with a task.</p>
<p>Pre-registered hypotheses 3-5 were designed to test for a learning effect between blocks 1 and 3, before and after experience with observing a red target among blue distractors. Given the overwhelming pop-out effect for target-absent trials in block 1, not much room for additional learning remained. Indeed, results from these tests support a prior-knowledge only model.</p>
<p><em>Hypothesis 3</em>: Like in the first block, in the third block color search complied with our criterion for ‘pop-out’ (<span class="math inline">\(M = 2.27\)</span>, 95% CI <span class="math inline">\([-\infty\)</span>, <span class="math inline">\(3.86]\)</span>, <span class="math inline">\(t(979) = -7.98\)</span>, <span class="math inline">\(p &lt; .001\)</span>), and was significantly different from the conjunction search slope (<span class="math inline">\(t(745) = 11.16\)</span>, <span class="math inline">\(p &lt; .001\)</span>; see rightmost panel in Fig. <a href="2-ch-search.html#fig:ch1-exp1-results">2.3</a>). This result is not surprising, given that a pop-out effect was already observed in block 1.</p>
<p><em>Hypothesis 4</em>: To quantify the learning effect for color search, we directly contrasted the search slope for color search in blocks 1 and 3. We find no evidence for a learning effect (<span class="math inline">\(t(799) = -1.15\)</span>, <span class="math inline">\(p = .250\)</span>). Furthermore, a Bayesian t-test with a scaled Cauchy prior for effect sizes (r=0.707) provided strong evidence in favour of the absence of a learning effect (<span class="math inline">\(\mathrm{BF}_{\textrm{01}} = 12.98\)</span>).</p>
<p><em>Hypothesis 5</em>: In case of a learning effect for pop-out search, Hypothesis 5 was designed to test the specificity of this effect to color pop-out by computing an interaction between block number and search type. Given that no learning effect was observed, this test makes little sense. For completeness, we report that the change in slope between blocks 1 and 3 was similar for color and conjunction search (<span class="math inline">\(M = -3.58\)</span>, 95% CI <span class="math inline">\([-10.52\)</span>, <span class="math inline">\(3.36]\)</span>, <span class="math inline">\(t(320) = -1.01\)</span>, <span class="math inline">\(p = .311\)</span>).</p>
<div class="figure" style="text-align: center"><span id="fig:ch1-exp1-results"></span>
<img src="thesis_files/figure-html/ch1-exp1-results-1.png" alt="Upper panel: median search time by distractor set size for the two search tasks across the three blocks (12 trials per participant). Correct responses only. Lower panel: accuracy as a function of block, set size and search type. Error bars represent the standard error of the median." width="\textwidth" />
<p class="caption">
Figure 2.3: Upper panel: median search time by distractor set size for the two search tasks across the three blocks (12 trials per participant). Correct responses only. Lower panel: accuracy as a function of block, set size and search type. Error bars represent the standard error of the median.
</p>
</div>
<div id="additional-analyses" class="section level4 unnumbered">
<h4>Additional analyses</h4>
<p>In Experiment 1, we found a clear pop-out effect for color absence in the first trials of the experiment, before participants experienced color pop-out in target-present trials. As per our analysis, this reflects prior metacognitive knowledge about the expected efficiency of color search. In order to terminate the search immediately, participants must have known, implicitly or explicitly, that a red item would have popped out immediately. In the setting of this experiment, this knowledge could not be acquired in previous trials. However, an alternative account is that participants noticed the pop-out of the red distractors in the conjunction trials of block 1, and based their expectation for color pop out on those trials. This account can be directly tested by zeroing in on the subset of participants who performed the two color trials before the two conjunction trials in block 1 (the order of trials within each block was determined pseudorandomly, such that half of the participants had color-search for the first trial, and of those a third had color-search for the second trial as well). This subset of participants showed a clear pop-out effect (<span class="math inline">\(M = -5.07\)</span>, 95% CI <span class="math inline">\([-\infty\)</span>, <span class="math inline">\(2.25]\)</span>, <span class="math inline">\(t(138) = -3.41\)</span>, <span class="math inline">\(p &lt; .001\)</span>), indicating that the highly efficient search termination in these first trials was not based on prior experience with red distractors.</p>
</div>
</div>
</div>
<div id="experiment-2" class="section level2">
<h2><span class="header-section-number">2.3</span> Experiment 2</h2>
<p>Experiment 1 provided evidence that color-absence pop-out occurs prior to experiencing color pop-out in the context of the same task. We interpret this as indicating that task-naive adults had valid implicit or explicit metacognitive expectations about color pop-out. This metacognitive knowledge may be innate (acquired in the course of evolution, for example driven by the utility of color search for foraging), learned from previous visual experience (for example, first-person experience of attention being immediately drawn to distinct colors), or culturally acquired (for example, through language). Experiment 2 was designed to extend these findings to another stimulus feature that is found to also efficiently guide attention: shape. The time cost of additional distractors in shape search was under 10 ms in our pilot data, rendering it another case of parallel, efficient search. It is possible however that unlike in the case of color, the metacognitive knowledge that gives rise to the pop-out effect for shape-absence is acquired through experience with the task. Unlike the colour space, that spans three dimensions only, the space of possible shapes is relatively unconstrained such that having prior knowledge of the expected effect of different shapes on attention requires a richer mental model of attentional processes. Furthrmore, colour is agreed to be a ‘guiding attribute of attention’, while it is unclear which shape features guide attention <span class="citation">(Wolfe &amp; Horowitz, 2017)</span>. In this experiment we also include an additional control for prior experience with visual search tasks, and ask whether the implicit metacognitive knowledge about pop-out is available for explicit report.</p>
<div id="participants-1" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Participants</h3>
<p>The research complied with all relevant ethical regulations, and was approved by the Research Ethics Committee of University College London (study ID number 1260/003). 887 Participants were recruited via Prolific, and gave their informed consent prior to their participation. They were selected based on their acceptance rate (&gt;95%) and for being native English speakers. We collected data until we reached 320 included participants for hypotheses 1-4 (after applying our pre-registered exclusion criteria). The entire experiment took around 4 minutes to complete (median completion time in our pilot data: 3.93 minutes). Participants were paid £0.51 for their participation, equivalent to an hourly wage of £7.78.</p>
</div>
<div id="procedure-1" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Procedure</h3>
<p>A static version of Experiment 2 can be accessed on <a href="matanmazor.github.io/termination/experiments/demos/exp1/">matanmazor.github.io/termination/experiments/demos/exp2/</a>. Experiment 2 was identical to Experiment 1 with the following exceptions. First, instead of color search trials, we included shape search trials, where the red dot target is present or absent in an array of red squares. Second, to minimize the similarity between conjunction and shape searches, conjunction trials included blue dots and red triangles as distractors. Third, to test participants’ explicit metacognition about their visual search behaviour, upon completing the main part of the task participants were presented with the four target-absent displays (shape and conjunction displays with 4 or 8 items), and were asked to sort them from fastest to slowest. Finally, participants reported whether they had participated in a similar experiment before, where they were asked to search for shapes on the screen. Participants who responded ‘yes’ were asked to tell us more about this previous experiment. This question was included in order to examine whether efficient target-absent search in trial 1 reflects prior experience with similar visual search experiments.</p>
<p>Our pre-registered analysis plan for Experiment 2, including rejection criteria and data preprocessing, was identical to our analysis plan for Experiment 1, and can be accessed in the following link: <a href="https://osf.io/v6mnb">https://osf.io/v6mnb</a>.</p>
</div>
<div id="results-1" class="section level3">
<h3><span class="header-section-number">2.3.3</span> Results</h3>
<p>Overall mean accuracy was 0.96 (standard deviation =0.06). Median reaction time was 644.60 ms (median absolute deviation = 123.89). In all further analyses, only correct trials with response times between 250 and 1000 ms are included.</p>
<p><em>Hypothesis 1 (positive control)</em>: Search times in block 2 (target-present) followed the expected pattern, with a steep slope for conjunction search (<span class="math inline">\(M = 15.08\)</span>, 95% CI <span class="math inline">\([12.34\)</span>, <span class="math inline">\(17.83]\)</span>) and a shallow slope for shape search (<span class="math inline">\(M = 5.84\)</span>, 95% CI <span class="math inline">\([3.90\)</span>, <span class="math inline">\(7.78]\)</span>; see middle panel of Fig. <a href="2-ch-search.html#fig:ch1-exp2-results">2.4</a>). The slope for shape search was significantly lower than 10 ms/item and thus met our criterion for being considered ‘pop-out’ (<span class="math inline">\(t(754) = -4.21\)</span>, <span class="math inline">\(p &lt; .001\)</span>). Furthermore, the difference between the slopes was significant (<span class="math inline">\(t(584) = 4.98\)</span>, <span class="math inline">\(p &lt; .001\)</span>).</p>
<p><em>Hypothesis 2</em>: Our central focus was on results from block 1 (target-absent). Here participants didn’t yet have experience with finding the red dot. Similar to the second block, the slope for the conjunction search was steep (<span class="math inline">\(M = 19.53\)</span>, 95% CI <span class="math inline">\([16.03\)</span>, <span class="math inline">\(23.04]\)</span>). The slope for shape search was numerically lower than 10 ms/item, but not significantly so (<span class="math inline">\(M = 8.03\)</span>, 95% CI <span class="math inline">\([-\infty\)</span>, <span class="math inline">\(10.50]\)</span>, <span class="math inline">\(t(608) = -1.31\)</span>, <span class="math inline">\(p = .095\)</span>). Still, the average search slope for shape search in this first block was significantly different from that of the conjunction search (<span class="math inline">\(t(326) = 2.77\)</span>, <span class="math inline">\(p = .006\)</span>; see leftmost panel of Fig. <a href="2-ch-search.html#fig:ch1-exp2-results">2.4</a>), indicating that a processing advantage for the detecting the absence of a shape compared to the absence of shape-color conjunction was already in place before experience with target presence.</p>
<p><em>Hypothesis 3</em>: As in the first block, in the third block the slope for shape search was numerically lower than 10 ms/item, but not significantly so (<span class="math inline">\(M = 8.85\)</span>, 95% CI <span class="math inline">\([-\infty\)</span>, <span class="math inline">\(10.68]\)</span>, <span class="math inline">\(t(723) = -1.03\)</span>, <span class="math inline">\(p = .151\)</span>). Importantly, the slope for shape search in block 3 was significantly different from the the slope for conjunction search (<span class="math inline">\(t(565) = 6.02\)</span>, <span class="math inline">\(p &lt; .001\)</span>; see rightmost panel of Fig. <a href="2-ch-search.html#fig:ch1-exp2-results">2.4</a>).</p>
<p><em>Hypothesis 4</em>: To quantify a potential learning effect for shape search between blocks 1 and 3, we directly contrasted the search slope for shape search in these two ‘target-absent’ blocks. We find no evidence for a learning effect (<span class="math inline">\(t(542) = -0.03\)</span>, <span class="math inline">\(p = .974\)</span>). Furthermore, a Bayesian t-test with a scaled Cauchy prior for effect sizes (r=0.707) provided strong evidence against a learning effect (<span class="math inline">\(\mathrm{BF}_{\textrm{01}} = 20.72\)</span>). Like in Experiment 1, these results are most consistent with a <em>prior-knowledge only</em> model (see Fig. <a href="2-ch-search.html#fig:ch1-exp2-results">2.4</a>), in which participants already know to expect that shape search should be easier than conjunction search, prior to having direct experience with target-present trials.</p>
<div class="figure" style="text-align: center"><span id="fig:ch1-exp2-results"></span>
<img src="thesis_files/figure-html/ch1-exp2-results-1.png" alt="Upper panel: median search time by distractor set size for the two search tasks across the three blocks. Correct responses only. Lower panel: accuracy as a function of block, set size and search type. Error bars represent the standard error of the median." width="\textwidth" />
<p class="caption">
Figure 2.4: Upper panel: median search time by distractor set size for the two search tasks across the three blocks. Correct responses only. Lower panel: accuracy as a function of block, set size and search type. Error bars represent the standard error of the median.
</p>
</div>
<div id="additional-analyses-1" class="section level4 unnumbered">
<h4>Additional Analyses</h4>
<div id="exploratory-analysis-task-experience" class="section level5 unnumbered">
<h5>Exploratory analysis: task experience</h5>
<p>At the end of the experiment, participants were asked if they have ever participated in a similar experiment before, where they were asked to search for a target item. 796 participants answered ‘no’ to this question. For those participants, a highly efficient search for a distinct shape in the first trials of the experiment, if found, cannot be due to prior experience performing a visual search task with similar stimuli. Participants that reported having no prior experience with a visual search task still showed efficient search termination for shape distractors (<span class="math inline">\(M = 7.32\)</span>, 95% CI <span class="math inline">\([4.21\)</span>, <span class="math inline">\(10.43]\)</span>), and were significantly more efficient in terminating shape search than conjunction search in the first 4 target-absent trials (<span class="math inline">\(t(296) = 2.68\)</span>, <span class="math inline">\(p = .008\)</span>). Efficient search termination for shape search is therefore not dependent on prior visual search trials, neither within the same experiment nor in previous ones.</p>
</div>
</div>
<div id="exploratory-analysis-search-time-estimates" class="section level4 unnumbered">
<h4>Exploratory analysis: search time estimates</h4>
<p>Upon completing the main part of Experiment 2, participants placed the four search arrays (shape and conjunction searches with 4 or 8 distractors) on a perceived difficulty axis. We used these ratings to ask whether the advantage for detecting the absence of a distinct shape over the absence of a shape/color conjunction depended on explicit access to metacognitive knowledge about search difficulty. The decision to quit early in target-absent shape search trials may depend on an internal belief that the target shape would have drawn attention immediately, but this belief may inaccessible to introspection. If introspective access is not a necessary condition for efficient quitting in visual search, some participants may not be able to reliably introspect about the difficulty of different searches but still be able to quit efficiently in shape search.</p>
<p>For this analysis, we only considered the ratings of participants who engaged with the array-sorting trial, and moved some of the arrays before continuing to the next trial (N=789). Searches with 8 distractors were rated as more difficult than searches with 4 distractors, in line with the set-size effect (<span class="math inline">\(t(788) = 31.62\)</span>, <span class="math inline">\(p &lt; .001\)</span>). Furthermore, conjunction searches were rates as more difficult than shape searches (<span class="math inline">\(t(788) = 5.11\)</span>, <span class="math inline">\(p &lt; .001\)</span>). Finally, we fitted single-subject linear regression models to the two search types, predicting search-time estimates as a function of set size. Similar to actual search slopes, these slopes derived from subjective estimates were also shallower for shape than for conjunction search, reflecting a belief that the effect of set size in shape search is not as strong as the effect of set size in conjunction search (<span class="math inline">\(M = 6.45\)</span>, 95% CI <span class="math inline">\([2.81\)</span>, <span class="math inline">\(10.08]\)</span>, <span class="math inline">\(t(788) = 3.48\)</span>, <span class="math inline">\(p = .001\)</span>).</p>
<p>Subjective search time estimates revealed that by the end of the experiment, the average participant considered the slope of shape search to be shallower than that of conjunction search. This suggests that at least some participants had introspective access to their visual search behaviour. But were those participants whose estimates reflected a shallow slope for shape search the same ones that were more efficient in detecting the absence of a shape in the display? The slopes of retrospective estimates for shape search were not reliably correlated with actual search slopes for shape absence in block 1 (<span class="math inline">\(r = .08\)</span>, 95% CI <span class="math inline">\([-.06\)</span>, <span class="math inline">\(.22]\)</span>) or 2 (<span class="math inline">\(r = .02\)</span>, 95% CI <span class="math inline">\([-.12\)</span>, <span class="math inline">\(.16]\)</span>). However, this result should be interpreted carefully in light of the low reliability of single subject estimated that are derived from one trial per cell. Indeed, search slopes for shape absence in blocks 1 and 3 were not reliably correlated themselves (<span class="math inline">\(r = .05\)</span>, 95% CI <span class="math inline">\([-.10\)</span>, <span class="math inline">\(.19]\)</span>).</p>
<p>To answer this question using a more severe test <span class="citation">(Mayo, 2018)</span>, we focused on the subset of participants whose difficulty orderings reflected the erroneous belief that shape search was more difficult than conjunction search (<span class="math inline">\(N=\)</span> 83). If efficient search termination depends on accurate explicit metacognitive knowledge about search efficiency, search termination in this subset of participants is not expected to be more efficient in shape compared to conjunction search, and is even expected to show the opposite pattern. In contrast with this prediction, and in support of a functional dissociation between explicit and implicit metacognitive knowledge, search slopes for shape-absence trials were shallower than for conjunction-absence trials (<span class="math inline">\(M_d = 12.45\)</span>, 95% CI <span class="math inline">\([5.21\)</span>, <span class="math inline">\(19.69]\)</span>, <span class="math inline">\(t(82) = 3.42\)</span>, <span class="math inline">\(p = .001\)</span>).</p>
</div>
</div>
</div>
<div id="discussion" class="section level2">
<h2><span class="header-section-number">2.4</span> Discussion</h2>
<p>Deciding that an item is absent requires counterfactual thinking, in the form of ‘I would have seen it if it was present’. In some cases, it is immediately clear that an hypothetical target would have been detected (such as when searching for a red item, but seeing only blue items), and in other cases more deliberate searching is needed until this belief can be held with confidence (such as when searching for a conjunction of features, for example colour and shape). Here we sought to determine the origins of this metacognitive knowledge that allows participants to conclude that a target would be found immediately in the first case, but not in the second. Specifically, we asked if this knowledge depends on task experience (such that with time, participants learn that some searches are easier than others), or alternatively, whether it is available already in the first trials of the experiment.</p>
<p>Previous studies of search termination have focused on the calibration of a quitting strategy over long chains of similar trials. For example, in a seminal study by <span class="citation">Chun &amp; Wolfe (1996)</span>, participants decreased their activation threshold (the necessary activation for an item to be scanned) following misses, but increased the threshold following correct rejections. This calibration mechanism critically depended on two features of the experimental design: a large number of similar trials, and explicit feedback about accuracy. Similarly, in a multi-session perceptual training study by Ellison and Walsh <span class="citation">(1998)</span>, response times became faster over sessions, and search slopes for conjunction search became shallower. In more recent studies, participants were able to learn statistical regularities in spatial position <span class="citation">(Moorselaar &amp; Slagter, 2019)</span> and visual features <span class="citation">(Moorselaar, Lampers, Cordesius, &amp; Slagter, 2020)</span> of distractor stimuli in a visual search task, and to use this information for making faster responses. These studies revealed important mechanisms by which task experience can affect visual search behaviour, but they left open the question of what guides search termination in the absence of any task experience. Our zero-shot search termination paradigm revealed that some knowledge about search efficiency is available to participants already in the first trials of the experiment, before engaging with the task or knowing what distractors to expect.</p>
<p>In two experiments, no prior experience with color or shape pop-out in previous trials was needed for participants to be able to terminate the search early when a target was absent. Participants were sensitive to the counterfactual likelihood of detecting a hypothetical target even in the first trials of the experiment, suggesting that metacognitive knowledge about visual attention (e.g., ‘red pops out’, or ‘a dot would catch my attention’) is available to guide zero-shot search termination. In Experiment 2, we find that some of this knowledge is represented explicitly, as expressed in participants’ ordering of visual search arrays by difficulty. However, focusing on participants with erroneous metacognitive beliefs about search efficiency, we find that explicit metacognitive knowledge is not a necessary condition for efficient search termination. More broadly, this finding indicates a functional dissociation between explicit and implicit metacognitive knowledge.</p>
<div id="is-implicit-metacognitive-knowledge-metacognitive" class="section level3">
<h3><span class="header-section-number">2.4.1</span> Is implicit metacognitive knowledge metacognitive?</h3>
<p>In this study we assumed that efficient search termination is impossible without accurate metacognitive knowledge about search difficulty. We base this conjecture on our conceptual analysis of inference about absence: in order to represent something as absent, one must know that they would have detected it had it been present <span class="citation">(Mazor &amp; Fleming, 2020)</span>. Alternative approaches to visual search assume that the absence of a stimulus can sometimes be perceived directly, without alluding to any metacognitive beliefs or counterfactual thinking. For example, ensemble perception allows observers to extract summary statistical information from sets of similar stimuli, without directly perceiving every single stimulus <span class="citation">(Whitney &amp; Yamanashi Leib, 2018)</span>. According to one alternative explanation of our results, if participants immediately perceive that the search array is all blue, they might not need to rely on any counterfactual thinking or self-knowledge to conclude that no red item was present. Similarly, when searching for a red dot, there is no need to serially scan a search array if it is immediately perceived as comprising only squares.</p>
<p>When contrasting this alternative account with our counterfactual model, it is useful to ask how does the visual system extract ensemble properties from sets of objects. For the global statistical property ‘the array comprises only squares’ to be extracted from a display without representing individual squares, the visual system must represent, explicitly or implicitly, that a non-square item would have been detected if present. This representation can be implemented, for example, as a threshold on curvature-sensitive neurons (‘a round object would have induced a higher firing rate in this neuron population’), or more generally as a likelihood function going from polygons to firing patterns (‘The perceived input is most likely under a world state where the display includes polygons only’). Even within the ensemble perception framework, inference about the absence of items must be based on some form of meta-level knowledge about the cognitive and perceptual systems. The fact that attention may not be required for ensemble perception <span class="citation">(Hochstein, Pavlovskaya, Bonneh, &amp; Soroker, 2015)</span> can inform and constrain our theories of where this meta-level knowledge is represented in the cognitive hierarchy, but it does not, by itself, weigh on the question of whether this is indeed metacognitive knowledge.</p>
<p>We note here that it not a prerequisite that metacognitive knowledge be accessible to consciousness. Metacognitive knowledge was originally assumed by Flavell <span class="citation">(1979)</span> to mostly affect cognition without accessing consciousness at all (i.e. without inducing a ‘metacognitive experience’). Different aspects of metacognition monitoring, including an immediate <em>Feeling of Knowing</em> when presented with a problem, have been attributed to implicit metacognitive mechanisms that share a conceptual similarity with the ones described in the previous paragraph <span class="citation">(Reder &amp; Schunn, 1996)</span>. More relevant to visual search, a schematic model of attention has been suggested to be implemented in the brains of many animal species, including all mammals and birds, and to facilitate attention control and monitoring <span class="citation">(Graziano, 2013)</span>. This <em>Attention Schema</em> is metacognitive in the sense that it reflects self knowledge about one’s own attention. This kind of implicit metacognitive knowledge may be crucial for extracting ensemble statistics from displays, and for representing the absence of objects.</p>
</div>
<div id="inference-about-absence-as-a-tool-for-studying-implicit-self-knowledge" class="section level3">
<h3><span class="header-section-number">2.4.2</span> Inference about absence as a tool for studying implicit self knowledge</h3>
<p>Participants’ early quitting in target-absence feature searches taught us something about implicit self-knowledge. This is not a coincidence, but an example of a general principle: inference about absence critically relies on self-knowledge not only in visual search (‘If a target was present, I would have found it’) but also in near-threshold detection (‘If a stimulus was present, I would have noticed it’), recognition memory (‘If this item was in the study list, I would have remembered it’), and problem-solving (‘If a solution to this problem was present, I would have come up with it’). This makes inference about absence an important tool for studying implicit self-knowledge in a range of domains without relying on explicit metacognitive reports. For example, in the context of recognition memory, items that are most likely to be remembered are also the ones that are most likely to be correctly rejected as foils when new. This ‘mirror effect’ <span class="citation">(Brown, Lewis, &amp; Monk, 1977)</span> conceptually resembles the alignment of feature-present and feature-absent search times across items and visual dimensions in visual search: if a target is found easily within a set of distractors <em>S</em>, it would also be easy to conclude that a target is absent if <em>S</em> is presented without the target in it. Just as in the study of visual search, previous studies of the mirror effect adopted a typical many subjects/few trials designs <span class="citation">(e.g., Brown et al., 1977; Glanzer &amp; Adams, 1985; Greene &amp; Thapar, 1994)</span>. By generalizing the approach we have taken here to implicit metacognitive knowledge of memory, future <em>Zero-shot negative recognition</em> experiments could ask whether the self knowledge that gives rise to the mirror effect is also available prior to engaging with the task.</p>
</div>
<div id="conclusion" class="section level3">
<h3><span class="header-section-number">2.4.3</span> Conclusion</h3>
<p>Search termination in the first few trials of an experiment (zero shot search termination) showed the same qualitative response time pattern as that commonly found in typical (few subjects/many trials) visual search experiments. Given that no target was present in these trials, participants must have been sensitive to the counterfactual likelihood of them finding the target, had it been present. In Experiment 2 we showed that this metacognitive knowledge about search difficulty was often accessible to report, but that this was not a necessary condition for efficient search termination. We interpret our results as indicating a dissociation between implicit and explicit metacognitive knowledge, with the former having a particularly influential role in inference about absence.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="3-math-sci.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["thesis.pdf", "PDF"], ["thesis.epub", "EPUB"], ["thesis.docx", "Word"]],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
