<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Experiment 1 | Self-Modeling in Inference about Absence</title>
  <meta name="description" content="Chapter 3 Experiment 1 | Self-Modeling in Inference about Absence" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Experiment 1 | Self-Modeling in Inference about Absence" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Experiment 1 | Self-Modeling in Inference about Absence" />
  
  
  

<meta name="author" content="Matan Mazor" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="2-behavioural-markers-of-perceptual-detection.html"/>
<link rel="next" href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"></a></li>
<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preliminary Content</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#preface"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#abstract"><i class="fa fa-check"></i>Abstract</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a><ul>
<li class="chapter" data-level="0.1" data-path="introduction.html"><a href="introduction.html#inference-about-absence"><i class="fa fa-check"></i><b>0.1</b> Inference about absence</a></li>
<li class="chapter" data-level="0.2" data-path="introduction.html"><a href="introduction.html#formalabsence"><i class="fa fa-check"></i><b>0.2</b> Probabilistic reasoning, criterion setting, and self knowledge</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#symmetrical-definition"><i class="fa fa-check"></i>Symmetrical definition:</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#dissymmetrical-definition"><i class="fa fa-check"></i>Dissymmetrical definition:</a></li>
<li class="chapter" data-level="0.2.1" data-path="introduction.html"><a href="introduction.html#second-order-cognition"><i class="fa fa-check"></i><b>0.2.1</b> Second-order cognition</a></li>
<li class="chapter" data-level="0.2.2" data-path="introduction.html"><a href="introduction.html#detectionmodels"><i class="fa fa-check"></i><b>0.2.2</b> Computational models of detection</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#htm"><i class="fa fa-check"></i>The High-Threshold model</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#sdt"><i class="fa fa-check"></i>Signal Detection Theory</a></li>
</ul></li>
<li class="chapter" data-level="0.3" data-path="introduction.html"><a href="introduction.html#detection-i-would-have-noticed-it"><i class="fa fa-check"></i><b>0.3</b> Detection: “I would have noticed it”</a></li>
<li class="chapter" data-level="0.4" data-path="introduction.html"><a href="introduction.html#intro:search"><i class="fa fa-check"></i><b>0.4</b> Visual search: “I would have found it”</a></li>
<li class="chapter" data-level="0.5" data-path="introduction.html"><a href="introduction.html#memory-i-would-have-remembered-it"><i class="fa fa-check"></i><b>0.5</b> Memory: “I would have remembered it”</a></li>
<li class="chapter" data-level="0.6" data-path="introduction.html"><a href="introduction.html#the-development-of-a-self-model"><i class="fa fa-check"></i><b>0.6</b> The development of a self-model</a></li>
<li class="chapter" data-level="0.7" data-path="introduction.html"><a href="introduction.html#this-thesis"><i class="fa fa-check"></i><b>0.7</b> This thesis</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-ch-search.html"><a href="1-ch-search.html"><i class="fa fa-check"></i><b>1</b> Zero-shot search termination reveals a dissociation between implicit and explicit metacognitive knowledge</a><ul>
<li class="chapter" data-level="1.1" data-path="1-ch-search.html"><a href="1-ch-search.html#introduction-1"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="1-ch-search.html"><a href="1-ch-search.html#experiment-1"><i class="fa fa-check"></i><b>1.2</b> Experiment 1</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-ch-search.html"><a href="1-ch-search.html#participants"><i class="fa fa-check"></i><b>1.2.1</b> Participants</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-ch-search.html"><a href="1-ch-search.html#procedure"><i class="fa fa-check"></i><b>1.2.2</b> Procedure</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-ch-search.html"><a href="1-ch-search.html#data-analysis"><i class="fa fa-check"></i><b>1.2.3</b> Data analysis</a></li>
<li class="chapter" data-level="1.2.4" data-path="1-ch-search.html"><a href="1-ch-search.html#results"><i class="fa fa-check"></i><b>1.2.4</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-ch-search.html"><a href="1-ch-search.html#experiment-2"><i class="fa fa-check"></i><b>1.3</b> Experiment 2</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-ch-search.html"><a href="1-ch-search.html#participants-1"><i class="fa fa-check"></i><b>1.3.1</b> Participants</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-ch-search.html"><a href="1-ch-search.html#procedure-1"><i class="fa fa-check"></i><b>1.3.2</b> Procedure</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-ch-search.html"><a href="1-ch-search.html#results-1"><i class="fa fa-check"></i><b>1.3.3</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-ch-search.html"><a href="1-ch-search.html#discussion"><i class="fa fa-check"></i><b>1.4</b> Discussion</a><ul>
<li class="chapter" data-level="1.4.1" data-path="1-ch-search.html"><a href="1-ch-search.html#is-implicit-metacognitive-knowledge-metacognitive"><i class="fa fa-check"></i><b>1.4.1</b> Is implicit metacognitive knowledge metacognitive?</a></li>
<li class="chapter" data-level="1.4.2" data-path="1-ch-search.html"><a href="1-ch-search.html#inference-about-absence-as-a-tool-for-studying-implicit-self-knowledge"><i class="fa fa-check"></i><b>1.4.2</b> Inference about absence as a tool for studying implicit self knowledge</a></li>
<li class="chapter" data-level="1.4.3" data-path="1-ch-search.html"><a href="1-ch-search.html#conclusion"><i class="fa fa-check"></i><b>1.4.3</b> Conclusion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-behavioural-markers-of-perceptual-detection.html"><a href="2-behavioural-markers-of-perceptual-detection.html"><i class="fa fa-check"></i><b>2</b> Behavioural markers of perceptual detection</a></li>
<li class="chapter" data-level="3" data-path="3-experiment-1-1.html"><a href="3-experiment-1-1.html"><i class="fa fa-check"></i><b>3</b> Experiment 1</a><ul>
<li class="chapter" data-level="3.1" data-path="3-experiment-1-1.html"><a href="3-experiment-1-1.html#results-2"><i class="fa fa-check"></i><b>3.1</b> Results</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-experiment-1-1.html"><a href="3-experiment-1-1.html#response-accuracy"><i class="fa fa-check"></i><b>3.1.1</b> Response accuracy</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-experiment-1-1.html"><a href="3-experiment-1-1.html#overall-properties-of-response-and-confidence-distributions"><i class="fa fa-check"></i><b>3.1.2</b> Overall properties of response and confidence distributions</a></li>
<li class="chapter" data-level="3.1.3" data-path="3-experiment-1-1.html"><a href="3-experiment-1-1.html#response-conditional-roc-curves"><i class="fa fa-check"></i><b>3.1.3</b> Response conditional ROC curves</a></li>
<li class="chapter" data-level="3.1.4" data-path="3-experiment-1-1.html"><a href="3-experiment-1-1.html#zroc-curves"><i class="fa fa-check"></i><b>3.1.4</b> zROC curves</a></li>
<li class="chapter" data-level="3.1.5" data-path="3-experiment-1-1.html"><a href="3-experiment-1-1.html#reverse-correlation"><i class="fa fa-check"></i><b>3.1.5</b> Reverse Correlation</a></li>
<li class="chapter" data-level="3.1.6" data-path="3-experiment-1-1.html"><a href="3-experiment-1-1.html#discrimination-decisions"><i class="fa fa-check"></i><b>3.1.6</b> Discrimination decisions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><i class="fa fa-check"></i><b>4</b> Distinct neural contributions to metacognition for detecting (but not discriminating) visual stimuli</a><ul>
<li class="chapter" data-level="4.1" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#introduction-2"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#methods-and-materials"><i class="fa fa-check"></i><b>4.2</b> Methods and Materials</a><ul>
<li class="chapter" data-level="4.2.1" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#participants-2"><i class="fa fa-check"></i><b>4.2.1</b> Participants</a></li>
<li class="chapter" data-level="4.2.2" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#design-and-procedure"><i class="fa fa-check"></i><b>4.2.2</b> Design and procedure</a></li>
<li class="chapter" data-level="4.2.3" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#scanning-parameters"><i class="fa fa-check"></i><b>4.2.3</b> Scanning parameters</a></li>
<li class="chapter" data-level="4.2.4" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#analysis"><i class="fa fa-check"></i><b>4.2.4</b> Analysis</a></li>
<li class="chapter" data-level="4.2.5" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#exclusion-criteria"><i class="fa fa-check"></i><b>4.2.5</b> Exclusion criteria</a></li>
<li class="chapter" data-level="4.2.6" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#response-conditional-type-ii-roc-curves"><i class="fa fa-check"></i><b>4.2.6</b> Response conditional type-II ROC curves</a></li>
<li class="chapter" data-level="4.2.7" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#imaging-analysis"><i class="fa fa-check"></i><b>4.2.7</b> Imaging analysis</a></li>
<li class="chapter" data-level="4.2.8" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#statistical-inference"><i class="fa fa-check"></i><b>4.2.8</b> Statistical inference</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#results-3"><i class="fa fa-check"></i><b>4.3</b> Results</a></li>
<li class="chapter" data-level="4.4" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#behavioural-results"><i class="fa fa-check"></i><b>4.4</b> Behavioural results</a><ul>
<li class="chapter" data-level="4.4.1" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#imaging-results"><i class="fa fa-check"></i><b>4.4.1</b> Imaging results</a></li>
<li class="chapter" data-level="4.4.2" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#computational-models"><i class="fa fa-check"></i><b>4.4.2</b> Computational models</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html"><a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html#discussion-1"><i class="fa fa-check"></i><b>4.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="conclusion-1.html"><a href="conclusion-1.html"><i class="fa fa-check"></i>Conclusion</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-app1-SDT.html"><a href="A-app1-SDT.html"><i class="fa fa-check"></i><b>A</b> Signal Detection Theory</a><ul>
<li class="chapter" data-level="A.1" data-path="A-app1-SDT.html"><a href="A-app1-SDT.html#app1:ROC"><i class="fa fa-check"></i><b>A.1</b> ROC and zROC curves</a></li>
<li class="chapter" data-level="A.2" data-path="A-app1-SDT.html"><a href="A-app1-SDT.html#app1:uvSDT"><i class="fa fa-check"></i><b>A.2</b> Unequal-variance (uv) SDT</a></li>
<li class="chapter" data-level="A.3" data-path="A-app1-SDT.html"><a href="A-app1-SDT.html#app1:mc"><i class="fa fa-check"></i><b>A.3</b> SDT Measures for Metacognition</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="B-the-second-appendix-for-fun.html"><a href="B-the-second-appendix-for-fun.html"><i class="fa fa-check"></i><b>B</b> The Second Appendix, for Fun</a></li>
<li class="chapter" data-level="C" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html"><i class="fa fa-check"></i><b>C</b> Supp. materials for ch. 3</a><ul>
<li class="chapter" data-level="C.1" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#app3:buttonpresses"><i class="fa fa-check"></i><b>C.1</b> Confidence button presses</a></li>
<li class="chapter" data-level="C.2" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#app3:zROC"><i class="fa fa-check"></i><b>C.2</b> zROC curves</a></li>
<li class="chapter" data-level="C.3" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#app3:GC-DM"><i class="fa fa-check"></i><b>C.3</b> Global confidence design matrix</a></li>
<li class="chapter" data-level="C.4" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#app3:ROIconf"><i class="fa fa-check"></i><b>C.4</b> Effect of confidence in our pre-specified ROIs</a></li>
<li class="chapter" data-level="C.5" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#app3:varianceRat"><i class="fa fa-check"></i><b>C.5</b> SDT variance ratio correlation with the quadratic confidence effect</a></li>
<li class="chapter" data-level="C.6" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#app3:efficiency"><i class="fa fa-check"></i><b>C.6</b> Correlation of metacognitive efficiency with linear and quadratic confidence effects</a></li>
<li class="chapter" data-level="C.7" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#app3:cross"><i class="fa fa-check"></i><b>C.7</b> Confidence-decision cross classification</a></li>
<li class="chapter" data-level="C.8" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#app3:SDT"><i class="fa fa-check"></i><b>C.8</b> Static Signal Detection Theory</a><ul>
<li class="chapter" data-level="C.8.1" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#discrimination"><i class="fa fa-check"></i><b>C.8.1</b> Discrimination</a></li>
<li class="chapter" data-level="C.8.2" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#detection"><i class="fa fa-check"></i><b>C.8.2</b> Detection</a></li>
</ul></li>
<li class="chapter" data-level="C.9" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#app3:Dynamic"><i class="fa fa-check"></i><b>C.9</b> Dynamic Criterion</a><ul>
<li class="chapter" data-level="C.9.1" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#discrimination-1"><i class="fa fa-check"></i><b>C.9.1</b> Discrimination</a></li>
<li class="chapter" data-level="C.9.2" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#detection-1"><i class="fa fa-check"></i><b>C.9.2</b> Detection</a></li>
</ul></li>
<li class="chapter" data-level="C.10" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#app3:Monitoring"><i class="fa fa-check"></i><b>C.10</b> Attention Monitoring</a><ul>
<li class="chapter" data-level="C.10.1" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#discrimination-2"><i class="fa fa-check"></i><b>C.10.1</b> Discrimination</a></li>
<li class="chapter" data-level="C.10.2" data-path="C-supp-materials-for-ch-3.html"><a href="C-supp-materials-for-ch-3.html#detection-2"><i class="fa fa-check"></i><b>C.10.2</b> Detection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Self-Modeling in Inference about Absence</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="experiment-1-1" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Experiment 1</h1>
<div id="results-2" class="section level2">
<h2><span class="header-section-number">3.1</span> Results</h2>
<div id="response-accuracy" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Response accuracy</h3>
<p>Overall accuracy level was 0.74 in the discrimination and 0.72 in the detection task. Performance for discrimination was significantly higher than for detection (<span class="math inline">\(M_d = 0.02\)</span>, 95% CI <span class="math inline">\([0.00\)</span>, <span class="math inline">\(0.04]\)</span>, <span class="math inline">\(t(9) = 2.43\)</span>, <span class="math inline">\(p = .038\)</span>). This difference in task performance reflected a slower convergence of the staircasing procedure for the discrimination task during the first session. When discarding all data from the first session and analyzing only data from the last three sessions (1800 trials per participant), task performance was equated between the two tasks at the group level (<span class="math inline">\(M_d = 0.00\)</span>, 95% CI <span class="math inline">\([-0.02\)</span>, <span class="math inline">\(0.02]\)</span>, <span class="math inline">\(t(9) = -0.05\)</span>, <span class="math inline">\(p = .962\)</span>; <span class="math inline">\(\mathrm{BF}_{\textrm{01}} = 3.24\)</span>). In order to avoid conflating true differences between discrimination and detection with more general difficulty effects, the first session was excluded from all subsequent analyses.</p>
</div>
<div id="overall-properties-of-response-and-confidence-distributions" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Overall properties of response and confidence distributions</h3>
<p>Participants were more likely to respond ‘yes’ than ‘no’ (mean proportion of ‘yes’ responses: <span class="math inline">\(M = 0.59\)</span>, 95% CI <span class="math inline">\([0.53\)</span>, <span class="math inline">\(0.64]\)</span>, <span class="math inline">\(t(9) = 3.45\)</span>, <span class="math inline">\(p = .007\)</span>). As a result, accuracy for ‘no’ responses was higher on average (mean accuracy in ‘yes’ responses: 0.69, mean accuracy in ‘no’ responses: 0.77, <span class="math inline">\(t(9) = -3.17\)</span>, <span class="math inline">\(p = .011\)</span>). We did not observe a consistent response bias for the discrimination data (mean proportion of ‘rightward’ or ‘upward’ responses: <span class="math inline">\(M = 0.52\)</span>, 95% CI <span class="math inline">\([0.47\)</span>, <span class="math inline">\(0.57]\)</span>, <span class="math inline">\(t(9) = 1.00\)</span>, <span class="math inline">\(p = .344\)</span>).</p>
<p>In detection, participants were generally slower to deliver ‘no’ responses compared to ‘yes’ responses (median difference: 85.37 ms, <span class="math inline">\(t(9) = -3.46\)</span>, <span class="math inline">\(p = .007\)</span> for a t-test on the log-transformed response times; see Fig. <a href="3-experiment-1-1.html#fig:ch2e1hists">3.1</a>, upper panel). No significant difference in response times was observed for the discrimination task (median difference: 6.16 ms, <span class="math inline">\(t(9) = -0.43\)</span>, <span class="math inline">\(p = .676\)</span>).</p>
<p>Confidence in detection was generally higher than in discrimination (<span class="math inline">\(M_d = 0.06\)</span>, 95% CI <span class="math inline">\([0.01\)</span>, <span class="math inline">\(0.12]\)</span>, <span class="math inline">\(t(9) = 2.49\)</span>, <span class="math inline">\(p = .035\)</span>; see Fig. <a href="3-experiment-1-1.html#fig:ch2e1hists">3.1</a>, lower panel). Within detection, confidence in ‘yes’ responses was generally higher than confidence in ‘no’ responses (<span class="math inline">\(M = 0.08\)</span>, 95% CI <span class="math inline">\([0.03\)</span>, <span class="math inline">\(0.13]\)</span>, <span class="math inline">\(t(9) = 3.49\)</span>, <span class="math inline">\(p = .007\)</span>). No difference in average confidence levels was found between the two discrimination responses (<span class="math inline">\(M = 0.02\)</span>, 95% CI <span class="math inline">\([-0.03\)</span>, <span class="math inline">\(0.06]\)</span>, <span class="math inline">\(t(9) = 0.91\)</span>, <span class="math inline">\(p = .384\)</span>).</p>
<div class="figure" style="text-align: center"><span id="fig:ch2e1hists"></span>
<img src="thesis_files/figure-html/ch2e1hists-1.png" alt="Response time (upper panel) and confidence (lower panel) histograms for the detection (left) and discrimination (right) tasks in Experiment 1. Vertical lines represent the median response time and the mean confidence rating for each response." width="672" />
<p class="caption">
Figure 3.1: Response time (upper panel) and confidence (lower panel) histograms for the detection (left) and discrimination (right) tasks in Experiment 1. Vertical lines represent the median response time and the mean confidence rating for each response.
</p>
</div>
</div>
<div id="response-conditional-roc-curves" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Response conditional ROC curves</h3>
<p>Following <span class="citation">Meuwese et al. (2014)</span>, we extracted response-conditional type-2 ROC (rc-ROC) curves for the two tasks. Unlike traditional type-I ROC curves that provide a visual representation of subjects’ ability to distinguish between two external world states, type 2 ROC curves represent their ability to track the accuracy of their own responses. The area under the response-conditional ROC curve (auROC2) is a measure of metacognitive sensitivity, with higher values corresponding to more accurate metacognitive monitoring.</p>
<p>Mean response-conditional ROC curves for the two responses in the discrimination task closely matched (<span class="math inline">\(M = 0.00\)</span>, 95% CI <span class="math inline">\([-0.05\)</span>, <span class="math inline">\(0.05]\)</span>, <span class="math inline">\(t(9) = 0.13\)</span>, <span class="math inline">\(p = .900\)</span>), indicating that on average, participants had similar metacognitive insight into the accuracy of the two discrimination responses. In contrast, response-conditional ROC curves for ‘yes’ and ‘no’ responses diverged significantly, indicating a metacognitive advantage for ‘yes’ responses (group difference in area under the response-conditional curve, AUROC2: <span class="math inline">\(M = 0.11\)</span>, 95% CI <span class="math inline">\([0.03\)</span>, <span class="math inline">\(0.18]\)</span>, <span class="math inline">\(t(9) = 3.28\)</span>, <span class="math inline">\(p = .010\)</span>).</p>
<!-- This effect was still significant after subtracting the mean confidence for each experimental block and response from the ratings (t(9)=4.39, p=0.001), suggesting that this pattern originates from trial-specific differences in confidence levels, and not from more global confidence level adjustments occurring at the block level (see figure S1).  -->
<p>To better understand the origin of this difference between ‘yes’ and ‘no’ curves, we compared the detection auROC2 values with the average discrimination auUROC2. We found both a significant increase in auROC2 for ‘yes’ responses (<span class="math inline">\(M = 0.06\)</span>, 95% CI <span class="math inline">\([0.01\)</span>, <span class="math inline">\(0.11]\)</span>, <span class="math inline">\(t(9) = 2.80\)</span>, <span class="math inline">\(p = .021\)</span>) and a marginally significant decrease in auROC2 for ‘no’ responses relative to discrimination (<span class="math inline">\(M = -0.05\)</span>, 95% CI <span class="math inline">\([-0.10\)</span>, <span class="math inline">\(0.00]\)</span>, <span class="math inline">\(t(9) = -2.16\)</span>, <span class="math inline">\(p = .059\)</span>). In other words, relative to our discrimination benchmark, metacognitive asymmetry in detection was driven by improved metacognitive insight into the accuracy of ‘yes’ responses, and degraded metacognitive insight into the accuracy of ‘no’ responses.</p>
<div class="figure" style="text-align: center"><span id="fig:ch2e1ROC"></span>
<img src="thesis_files/figure-html/ch2e1ROC-1.png" alt="Response conditional ROC curves for the two tasks and four responses in Exp. 1. The area under the curve is a measure of metacognitive sensitivity. Lower panel: distributions of the area under the curve for the four responses, across participants. Error bars stand for the standard error of the mean." width="672" />
<p class="caption">
Figure 3.2: Response conditional ROC curves for the two tasks and four responses in Exp. 1. The area under the curve is a measure of metacognitive sensitivity. Lower panel: distributions of the area under the curve for the four responses, across participants. Error bars stand for the standard error of the mean.
</p>
</div>
<p>A difference in response-conditional auROC estimates can emerge from higher-order differences in metacognitive monitoring for the two responses or in lower-level differences in the representations of signal and noise <span class="citation">(such as in first-order signal detection models where the signal variance is higher; Maniscalco &amp; Lau, 2014)</span>. Importantly, a difference can also emerge in first-order signal-detection models that assume equal variance, in the presence of a response bias or insufficient variance in confidence ratings. To test if this was the case here, we simulated data that was identical to our empirical data except for confidence ratings in correct responses, that were chosen to perfectly agree with the assumptions of an equal-variance SDT model, given participants’ decision criterion, sensitivity, and their confidence in incorrect responses. We then compared subject-wise differences between the response-conditional auROCs with the differences in this simulated dataset <span class="citation">(Mazor, Moran, &amp; Fleming, 2021)</span>. The difference in differences was significant, indicating that the observed metacognitive asymmetry could not be accounted for by a first-order equal-variance SDT model (<span class="math inline">\(M = 0.08\)</span>, 95% CI <span class="math inline">\([0.02\)</span>, <span class="math inline">\(0.14]\)</span>, <span class="math inline">\(t(9) = 2.96\)</span>, <span class="math inline">\(p = .016\)</span>).</p>
</div>
<div id="zroc-curves" class="section level3">
<h3><span class="header-section-number">3.1.4</span> zROC curves</h3>
<p>An asymmetry in metacognitive sensitivity for ‘yes’ and ‘no’ responses is predicted by unequal-variance Signal Detection Theory (<em>uvSDT</em>). Specifically, if the ‘signal’ distribution is wider than the ‘noise’ distribution, the overlap between the distributions will be more pronounced for misses and correct rejections than for hits and false alarms, making metacognitive judgments for ‘no’ responses objectively more difficult. Furthermore, uvSDT predicts that plotting the type-1 ROC curve in z-space (taking the inverse cumulative distribution of the confidence rating histogram) will reveal a straight line with a slope equal to <span class="math inline">\(\frac{\sigma_{noise}}{\sigma_{signal}}\)</span>.</p>
<p>We used linear regression to estimate the slope of the zROC curve. However, linear regression systematically underestimates the regression slope in case of any deviation from a perfect fit, due to regression to the mean <span class="citation">(Wickens, 2002, p. 56)</span>. To control for this, we fitted two regression models for the task data of each participant: one predicting <span class="math inline">\(Z(h)\)</span> based on <span class="math inline">\(Z(f)\)</span> (yielding slope <span class="math inline">\(s_1\)</span>) and one predicting <span class="math inline">\(Z(f)\)</span> based on <span class="math inline">\(Z(h)\)</span> (yielding slope <span class="math inline">\(s_2\)</span>). We then used <span class="math inline">\(\frac{log(s_1)-log(s2)}{2}\)</span> as a bias-free measure of the zROC slope. In equal-variance SDT, this value is predicted to be 0, corresponding to a slope of 1.</p>
<p>Indeed, slopes were generally shallow for detection zROC curves (as predicted by an unequal-variance SDT model; <span class="math inline">\(M = -0.15\)</span>, 95% CI <span class="math inline">\([-0.27\)</span>, <span class="math inline">\(-0.04]\)</span>, <span class="math inline">\(t(9) = -2.95\)</span>, <span class="math inline">\(p = .016\)</span>), and not significantly different from 1 for discrimination zROC curves (as predicted by equal-variance SDT; <span class="math inline">\(M = 0.00\)</span>, 95% CI <span class="math inline">\([-0.10\)</span>, <span class="math inline">\(0.09]\)</span>, <span class="math inline">\(t(9) = -0.07\)</span>, <span class="math inline">\(p = .946\)</span>).</p>
<p>These results support a difference in the variance-structure of the representation of signal and noise, such that the representation of signal is more varied across trials. However, it is still possible that some of the metacognitive asymmetry in detection (the difference in auROC between ‘yes’ and ‘no’ responses) reflects additional higher-order processes that cannot be captured by a first-order signal-detection model. If this was the case, zROC curves for detection should not only be more shallow, but also less linear than for discrimination, reflecting poorer fit of the signal-detection model to detection. In order to test if this was the case, we compared the subject-wise <span class="math inline">\(R^2\)</span> values for the detection and discrimination zROC slopes. <span class="math inline">\(R^2\)</span> values reflect the goodness of fit of a linear model to the data. These values were similar for the two tasks (<span class="math inline">\(M_d = -0.01\)</span>, 95% CI <span class="math inline">\([-0.03\)</span>, <span class="math inline">\(0.01]\)</span>, <span class="math inline">\(t(9) = -0.91\)</span>, <span class="math inline">\(p = .385\)</span>). This suggests that a first-order SDT model can account equally well for behaviour in the two tasks.</p>
</div>
<div id="reverse-correlation" class="section level3">
<h3><span class="header-section-number">3.1.5</span> Reverse Correlation</h3>
<p>Stimuli in Exp. 1 comprised set of randomly moving dots. Different trials randomly varied in the proportion of dots that moved coherently and in the overall direction of the randomly moving dots, both between stimuli of the same category and between timepoints of the same stimulus. To test whether participants incorporated this random variation into their decision-making process, we applied reverse correlation analysis to identify stimulus features that affect decisions and confidence estimates in detection and discrimination <span class="citation">(Zylberberg, Barttfeld, &amp; Sigman, 2012)</span>.</p>
</div>
<div id="discrimination-decisions" class="section level3">
<h3><span class="header-section-number">3.1.6</span> Discrimination decisions</h3>
<p>For each participant, we averaged the motion energy in the chosen and unchosen directions, and these mean vectors were then fed into a group-level analysis. This analysis was designed to reveal whether random fluctuations in motion energy affect the probability of responding ‘right’ and ‘left’ (or ‘up’ or ‘down’), and at which time points these random fluctuations have the strongest effects on choice. Similar to the results obtained by Zylberberg et. al., participants’ decisions were more sensitive to motion energy fluctuations during the first 300 milliseconds of the trial.</p>
<pre><code>Warning: Removed 8 row(s) containing missing values (geom_path).</code></pre>
<pre><code>Warning: Removed 4 row(s) containing missing values (geom_path).</code></pre>
<div class="figure" style="text-align: center"><span id="fig:rcDecision"></span>
<img src="thesis_files/figure-html/rcDecision-1.png" alt="Decision kernels in discrimination in Experiment 1. Upper panel: motion energy in the chosen (green) and unchosen (purple) direction as a function of time. Lower panel: a subtraction between energy in the chosen and unchosen directions" width="672" />
<p class="caption">
Figure 3.3: Decision kernels in discrimination in Experiment 1. Upper panel: motion energy in the chosen (green) and unchosen (purple) direction as a function of time. Lower panel: a subtraction between energy in the chosen and unchosen directions
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:e2hists"></span>
<img src="thesis_files/figure-html/e2hists-1.png" alt="Response conditional ROC curves for the two discrimination responses in Exp. 1. The area under the curve is a measure of metacognitive sensitivity. Bottom right inset: distributions of the area under the curve for the two responses, across participants. Overall, participants had lower metacognitive insight into the accuracy of their 'O' responses. Error bars stand for the standard error of the mean. For illustration, the response-conditional ROC curves of the first 20 participants are included." width="672" />
<p class="caption">
Figure 3.4: Response conditional ROC curves for the two discrimination responses in Exp. 1. The area under the curve is a measure of metacognitive sensitivity. Bottom right inset: distributions of the area under the curve for the two responses, across participants. Overall, participants had lower metacognitive insight into the accuracy of their ‘O’ responses. Error bars stand for the standard error of the mean. For illustration, the response-conditional ROC curves of the first 20 participants are included.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:e2ROC"></span>
<img src="thesis_files/figure-html/e2ROC-1.png" alt="Response conditional ROC curves for the two discrimination responses in Exp. 1. The area under the curve is a measure of metacognitive sensitivity. Bottom right inset: distributions of the area under the curve for the two responses, across participants. Overall, participants had lower metacognitive insight into the accuracy of their 'O' responses. Error bars stand for the standard error of the mean. For illustration, the response-conditional ROC curves of the first 20 participants are included." width="672" />
<p class="caption">
Figure 3.5: Response conditional ROC curves for the two discrimination responses in Exp. 1. The area under the curve is a measure of metacognitive sensitivity. Bottom right inset: distributions of the area under the curve for the two responses, across participants. Overall, participants had lower metacognitive insight into the accuracy of their ‘O’ responses. Error bars stand for the standard error of the mean. For illustration, the response-conditional ROC curves of the first 20 participants are included.
</p>
</div>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="2-behavioural-markers-of-perceptual-detection.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="4-distinct-neural-contributions-to-metacognition-for-detecting-but-not-discriminating-visual-stimuli.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["thesis.pdf", "PDF"], ["thesis.epub", "EPUB"], ["thesis.docx", "Word"]],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
