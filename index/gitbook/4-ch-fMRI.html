<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Distinct neural contributions to metacognition for detecting (but not discriminating) visual stimuli | Self-Modelling in Inference about Absence</title>
  <meta name="description" content="Chapter 4 Distinct neural contributions to metacognition for detecting (but not discriminating) visual stimuli | Self-Modelling in Inference about Absence" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Distinct neural contributions to metacognition for detecting (but not discriminating) visual stimuli | Self-Modelling in Inference about Absence" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Distinct neural contributions to metacognition for detecting (but not discriminating) visual stimuli | Self-Modelling in Inference about Absence" />
  
  
  

<meta name="author" content="Matan Mazor" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="3-ch-RC.html"/>
<link rel="next" href="5-ch-asymmetry.html"/>
<style type="text/css">
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
</style>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"></a></li>
<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preliminary Content</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#impact-statement"><i class="fa fa-check"></i>Impact Statement</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="0.1" data-path="introduction.html"><a href="introduction.html#inference-about-absence"><i class="fa fa-check"></i><b>0.1</b> Inference about absence</a></li>
<li class="chapter" data-level="0.2" data-path="introduction.html"><a href="introduction.html#formalabsence"><i class="fa fa-check"></i><b>0.2</b> Probabilistic reasoning, criterion setting, and self knowledge</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#symmetrical-definition"><i class="fa fa-check"></i>Symmetrical definition:</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#dissymmetrical-definition"><i class="fa fa-check"></i>Dissymmetrical definition:</a></li>
<li class="chapter" data-level="0.2.1" data-path="introduction.html"><a href="introduction.html#intro-2nd-order"><i class="fa fa-check"></i><b>0.2.1</b> Second-order cognition</a></li>
<li class="chapter" data-level="0.2.2" data-path="introduction.html"><a href="introduction.html#detectionmodels"><i class="fa fa-check"></i><b>0.2.2</b> Computational models of detection</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#htm"><i class="fa fa-check"></i>The High-Threshold model</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#sdt"><i class="fa fa-check"></i>Signal Detection Theory</a></li>
</ul></li>
<li class="chapter" data-level="0.3" data-path="introduction.html"><a href="introduction.html#detection-i-would-have-noticed-it"><i class="fa fa-check"></i><b>0.3</b> Detection: “I would have noticed it”</a></li>
<li class="chapter" data-level="0.4" data-path="introduction.html"><a href="introduction.html#intro:search"><i class="fa fa-check"></i><b>0.4</b> Visual search: “I would have found it”</a></li>
<li class="chapter" data-level="0.5" data-path="introduction.html"><a href="introduction.html#memory-i-would-have-remembered-it"><i class="fa fa-check"></i><b>0.5</b> Memory: “I would have remembered it”</a></li>
<li class="chapter" data-level="0.6" data-path="introduction.html"><a href="introduction.html#the-development-of-a-self-model"><i class="fa fa-check"></i><b>0.6</b> The development of a self-model</a></li>
<li class="chapter" data-level="0.7" data-path="introduction.html"><a href="introduction.html#this-thesis"><i class="fa fa-check"></i><b>0.7</b> This thesis</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-ch-termination.html"><a href="1-ch-termination.html"><i class="fa fa-check"></i><b>1</b> Efficient search termination without task experience: the role of second-order knowledge about visual search</a>
<ul>
<li class="chapter" data-level="1.1" data-path="1-ch-termination.html"><a href="1-ch-termination.html#introduction-1"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="1-ch-termination.html"><a href="1-ch-termination.html#experiment-1"><i class="fa fa-check"></i><b>1.2</b> Experiment 1</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="1-ch-termination.html"><a href="1-ch-termination.html#participants"><i class="fa fa-check"></i><b>1.2.1</b> Participants</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-ch-termination.html"><a href="1-ch-termination.html#procedure"><i class="fa fa-check"></i><b>1.2.2</b> Procedure</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-ch-termination.html"><a href="1-ch-termination.html#randomization"><i class="fa fa-check"></i><b>1.2.3</b> Randomization</a></li>
<li class="chapter" data-level="1.2.4" data-path="1-ch-termination.html"><a href="1-ch-termination.html#data-analysis"><i class="fa fa-check"></i><b>1.2.4</b> Data analysis</a></li>
<li class="chapter" data-level="1.2.5" data-path="1-ch-termination.html"><a href="1-ch-termination.html#results"><i class="fa fa-check"></i><b>1.2.5</b> Results</a></li>
<li class="chapter" data-level="1.2.6" data-path="1-ch-termination.html"><a href="1-ch-termination.html#additional-analysis-first-trial-only"><i class="fa fa-check"></i><b>1.2.6</b> Additional analysis: first trial only</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-ch-termination.html"><a href="1-ch-termination.html#experiment-2"><i class="fa fa-check"></i><b>1.3</b> Experiment 2</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="1-ch-termination.html"><a href="1-ch-termination.html#participants-1"><i class="fa fa-check"></i><b>1.3.1</b> Participants</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-ch-termination.html"><a href="1-ch-termination.html#procedure-1"><i class="fa fa-check"></i><b>1.3.2</b> Procedure</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-ch-termination.html"><a href="1-ch-termination.html#results-1"><i class="fa fa-check"></i><b>1.3.3</b> Results</a></li>
<li class="chapter" data-level="1.3.4" data-path="1-ch-termination.html"><a href="1-ch-termination.html#additional-analyses"><i class="fa fa-check"></i><b>1.3.4</b> Additional Analyses</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-ch-termination.html"><a href="1-ch-termination.html#discussion"><i class="fa fa-check"></i><b>1.4</b> Discussion</a></li>
<li class="chapter" data-level="1.5" data-path="1-ch-termination.html"><a href="1-ch-termination.html#conclusion"><i class="fa fa-check"></i><b>1.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html"><i class="fa fa-check"></i><b>2</b> Internal models of visual search are rich, person-specific, and mostly accurate</a>
<ul>
<li class="chapter" data-level="2.1" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#introduction-2"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#experiments-1-and-2-shape-orientation-and-color"><i class="fa fa-check"></i><b>2.2</b> Experiments 1 and 2: shape, orientation, and color</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#participants-2"><i class="fa fa-check"></i><b>2.2.1</b> Participants</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#procedure-2"><i class="fa fa-check"></i><b>2.2.2</b> Procedure</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#results-2"><i class="fa fa-check"></i><b>2.2.3</b> Results</a></li>
<li class="chapter" data-level="" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#estimation-accuracy"><i class="fa fa-check"></i>Estimation accuracy</a></li>
<li class="chapter" data-level="" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#a-graded-representation-of-search-efficiency"><i class="fa fa-check"></i>A graded representation of search efficiency</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#experiments-3-and-4-complex-unfamiliar-stimuli"><i class="fa fa-check"></i><b>2.3</b> Experiments 3 and 4: complex, unfamiliar stimuli</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#participants-3"><i class="fa fa-check"></i><b>2.3.1</b> Participants</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#procedure-3"><i class="fa fa-check"></i><b>2.3.2</b> Procedure</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#results-3"><i class="fa fa-check"></i><b>2.3.3</b> Results</a></li>
<li class="chapter" data-level="" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#estimation-time"><i class="fa fa-check"></i>Estimation time</a></li>
<li class="chapter" data-level="" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#asymmetry"><i class="fa fa-check"></i>Visual search asymmetry</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#discussion-1"><i class="fa fa-check"></i><b>2.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-ch-RC.html"><a href="3-ch-RC.html"><i class="fa fa-check"></i><b>3</b> Evidence weightings in confidence judgments for detection and discrimination</a>
<ul>
<li class="chapter" data-level="3.1" data-path="3-ch-RC.html"><a href="3-ch-RC.html#introduction-3"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="3-ch-RC.html"><a href="3-ch-RC.html#experiment-1-1"><i class="fa fa-check"></i><b>3.2</b> Experiment 1</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="3-ch-RC.html"><a href="3-ch-RC.html#methods"><i class="fa fa-check"></i><b>3.2.1</b> Methods</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-ch-RC.html"><a href="3-ch-RC.html#randomization-1"><i class="fa fa-check"></i><b>3.2.2</b> Randomization</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-ch-RC.html"><a href="3-ch-RC.html#analysis"><i class="fa fa-check"></i><b>3.2.3</b> Analysis</a></li>
<li class="chapter" data-level="3.2.4" data-path="3-ch-RC.html"><a href="3-ch-RC.html#results-4"><i class="fa fa-check"></i><b>3.2.4</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-ch-RC.html"><a href="3-ch-RC.html#experiment-2-1"><i class="fa fa-check"></i><b>3.3</b> Experiment 2</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="3-ch-RC.html"><a href="3-ch-RC.html#methods-1"><i class="fa fa-check"></i><b>3.3.1</b> Methods</a></li>
<li class="chapter" data-level="3.3.2" data-path="3-ch-RC.html"><a href="3-ch-RC.html#randomization-2"><i class="fa fa-check"></i><b>3.3.2</b> Randomization</a></li>
<li class="chapter" data-level="3.3.3" data-path="3-ch-RC.html"><a href="3-ch-RC.html#results-5"><i class="fa fa-check"></i><b>3.3.3</b> Results</a></li>
<li class="chapter" data-level="3.3.4" data-path="3-ch-RC.html"><a href="3-ch-RC.html#detection-signal-trials-1"><i class="fa fa-check"></i><b>3.3.4</b> Detection signal trials</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-ch-RC.html"><a href="3-ch-RC.html#experiment-3"><i class="fa fa-check"></i><b>3.4</b> Experiment 3</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="3-ch-RC.html"><a href="3-ch-RC.html#methods-2"><i class="fa fa-check"></i><b>3.4.1</b> Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-ch-RC.html"><a href="3-ch-RC.html#results-6"><i class="fa fa-check"></i><b>3.4.2</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-ch-RC.html"><a href="3-ch-RC.html#discussion-2"><i class="fa fa-check"></i><b>3.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-ch-fMRI.html"><a href="4-ch-fMRI.html"><i class="fa fa-check"></i><b>4</b> Distinct neural contributions to metacognition for detecting (but not discriminating) visual stimuli</a>
<ul>
<li class="chapter" data-level="4.1" data-path="4-ch-fMRI.html"><a href="4-ch-fMRI.html#introduction-4"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="4-ch-fMRI.html"><a href="4-ch-fMRI.html#methods-and-materials"><i class="fa fa-check"></i><b>4.2</b> Methods and Materials</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="4-ch-fMRI.html"><a href="4-ch-fMRI.html#participants-7"><i class="fa fa-check"></i><b>4.2.1</b> Participants</a></li>
<li class="chapter" data-level="4.2.2" data-path="4-ch-fMRI.html"><a href="4-ch-fMRI.html#design-and-procedure"><i class="fa fa-check"></i><b>4.2.2</b> Design and procedure</a></li>
<li class="chapter" data-level="4.2.3" data-path="4-ch-fMRI.html"><a href="4-ch-fMRI.html#scanning-parameters"><i class="fa fa-check"></i><b>4.2.3</b> Scanning parameters</a></li>
<li class="chapter" data-level="4.2.4" data-path="4-ch-fMRI.html"><a href="4-ch-fMRI.html#analysis-1"><i class="fa fa-check"></i><b>4.2.4</b> Analysis</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-ch-fMRI.html"><a href="4-ch-fMRI.html#results-7"><i class="fa fa-check"></i><b>4.3</b> Results</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="4-ch-fMRI.html"><a href="4-ch-fMRI.html#behavioural-results"><i class="fa fa-check"></i><b>4.3.1</b> Behavioural results</a></li>
<li class="chapter" data-level="4.3.2" data-path="4-ch-fMRI.html"><a href="4-ch-fMRI.html#imaging-results"><i class="fa fa-check"></i><b>4.3.2</b> Imaging results</a></li>
<li class="chapter" data-level="4.3.3" data-path="4-ch-fMRI.html"><a href="4-ch-fMRI.html#computational-models"><i class="fa fa-check"></i><b>4.3.3</b> Computational models</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-ch-fMRI.html"><a href="4-ch-fMRI.html#discussion-3"><i class="fa fa-check"></i><b>4.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html"><i class="fa fa-check"></i><b>5</b> Metacognitive asymmetries in visual perception</a>
<ul>
<li class="chapter" data-level="5.1" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#introduction-5"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#methods-3"><i class="fa fa-check"></i><b>5.2</b> Methods</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#participants-8"><i class="fa fa-check"></i><b>5.2.1</b> Participants</a></li>
<li class="chapter" data-level="5.2.2" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#procedure-4"><i class="fa fa-check"></i><b>5.2.2</b> Procedure</a></li>
<li class="chapter" data-level="5.2.3" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#data-analysis-1"><i class="fa fa-check"></i><b>5.2.3</b> Data analysis</a></li>
<li class="chapter" data-level="5.2.4" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#analysis-plan"><i class="fa fa-check"></i><b>5.2.4</b> Dependent variables and analysis plan</a></li>
<li class="chapter" data-level="5.2.5" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#statistical-power"><i class="fa fa-check"></i><b>5.2.5</b> Statistical power</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#data-availability"><i class="fa fa-check"></i><b>5.3</b> Data availability</a></li>
<li class="chapter" data-level="5.4" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#code-availability"><i class="fa fa-check"></i><b>5.4</b> Code availability</a></li>
<li class="chapter" data-level="5.5" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#deviations"><i class="fa fa-check"></i><b>5.5</b> Deviations from pre-registration</a></li>
<li class="chapter" data-level="5.6" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#results-8"><i class="fa fa-check"></i><b>5.6</b> Results</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#experiment-1-q-vs.-o"><i class="fa fa-check"></i><b>5.6.1</b> Experiment 1: <em>Q</em> vs. <em>O</em></a></li>
<li class="chapter" data-level="5.6.2" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#experiment-2-c-vs.-o"><i class="fa fa-check"></i><b>5.6.2</b> Experiment 2: C vs. O</a></li>
<li class="chapter" data-level="5.6.3" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#experiment-3-tilted-vs.-vertical-lines"><i class="fa fa-check"></i><b>5.6.3</b> Experiment 3: tilted vs. vertical lines</a></li>
<li class="chapter" data-level="5.6.4" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#experiment-4-curved-vs.-straight-lines"><i class="fa fa-check"></i><b>5.6.4</b> Experiment 4: curved vs. straight lines</a></li>
<li class="chapter" data-level="5.6.5" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#experiment-5-upward-tilted-vs.-downward-tilted-cubes"><i class="fa fa-check"></i><b>5.6.5</b> Experiment 5: upward-tilted vs. downward-tilted cubes</a></li>
<li class="chapter" data-level="5.6.6" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#experiment-6-flipped-vs.-normal-letters"><i class="fa fa-check"></i><b>5.6.6</b> Experiment 6: flipped vs. normal letters</a></li>
<li class="chapter" data-level="5.6.7" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#asymmetry-summary"><i class="fa fa-check"></i><b>5.6.7</b> Experiments 1-6: summary</a></li>
<li class="chapter" data-level="5.6.8" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#experiment-7-exploratory-grating-vs.-noise"><i class="fa fa-check"></i><b>5.6.8</b> Experiment 7 (exploratory): grating vs. noise</a></li>
<li class="chapter" data-level="5.6.9" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#exploratory-analysis"><i class="fa fa-check"></i><b>5.6.9</b> Exploratory analysis</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#discussion-4"><i class="fa fa-check"></i><b>5.7</b> Discussion</a></li>
<li class="chapter" data-level="5.8" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#conclusion-1"><i class="fa fa-check"></i><b>5.8</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html"><i class="fa fa-check"></i>General Discussion</a>
<ul>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html#didnotfind"><i class="fa fa-check"></i>What I didn’t find</a>
<ul>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html#chapter-1-no-correlation-with-explicit-metacognition"><i class="fa fa-check"></i>Chapter 1: no correlation with explicit metacognition</a></li>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html#chapter-3-no-effect-of-confidence-in-signal-presence"><i class="fa fa-check"></i>Chapter 3: no effect of confidence in signal presence</a></li>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html#chapter-4-only-minor-differences-in-brain-activity-between-inference-about-absence-and-presence"><i class="fa fa-check"></i>Chapter 4: only minor differences in brain activity between inference about absence and presence</a></li>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html#chapter-5-no-metacognitive-asymmetry-between-default-complying-and-default-violating-signals"><i class="fa fa-check"></i>Chapter 5: no metacognitive asymmetry between default-complying and default-violating signals</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html#withoutselfmodel"><i class="fa fa-check"></i>Inference about absence without self-modelling</a>
<ul>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html#patch"><i class="fa fa-check"></i>Patch-leaving in foraging</a></li>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html#absenceperception"><i class="fa fa-check"></i>Direct perception</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html#future-directions"><i class="fa fa-check"></i>Future directions</a>
<ul>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html#failures"><i class="fa fa-check"></i>Failures of a self-model</a></li>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html#inference-about-asbence-in-multi-dimensional-and-hierarchical-representational-spaces"><i class="fa fa-check"></i>Inference about asbence in multi-dimensional and hierarchical representational spaces</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html#conclusion-2"><i class="fa fa-check"></i>Conclusion</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-app1-SDT.html"><a href="A-app1-SDT.html"><i class="fa fa-check"></i><b>A</b> Signal Detection Theory</a>
<ul>
<li class="chapter" data-level="A.1" data-path="A-app1-SDT.html"><a href="A-app1-SDT.html#app1-ROC"><i class="fa fa-check"></i><b>A.1</b> ROC and zROC curves</a></li>
<li class="chapter" data-level="A.2" data-path="A-app1-SDT.html"><a href="A-app1-SDT.html#app1-uvSDT"><i class="fa fa-check"></i><b>A.2</b> Unequal-variance (uv) SDT</a></li>
<li class="chapter" data-level="A.3" data-path="A-app1-SDT.html"><a href="A-app1-SDT.html#app1-mc"><i class="fa fa-check"></i><b>A.3</b> SDT Measures for Metacognition</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="B-app1-RT.html"><a href="B-app1-RT.html"><i class="fa fa-check"></i><b>B</b> Supp. materials for ch. 1</a>
<ul>
<li class="chapter" data-level="B.1" data-path="B-app1-RT.html"><a href="B-app1-RT.html#effect-of-rt-based-trial-exclusion"><i class="fa fa-check"></i><b>B.1</b> Effect of RT-based trial exclusion</a>
<ul>
<li class="chapter" data-level="B.1.1" data-path="B-app1-RT.html"><a href="B-app1-RT.html#experiment-1-2"><i class="fa fa-check"></i><b>B.1.1</b> Experiment 1</a></li>
<li class="chapter" data-level="B.1.2" data-path="B-app1-RT.html"><a href="B-app1-RT.html#experiment-2-2"><i class="fa fa-check"></i><b>B.1.2</b> Experiment 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="C-supp.-materials-for-ch.-2.html"><a href="C-supp.-materials-for-ch.-2.html"><i class="fa fa-check"></i><b>C</b> Supp. materials for ch. 2</a>
<ul>
<li class="chapter" data-level="C.1" data-path="C-supp.-materials-for-ch.-2.html"><a href="C-supp.-materials-for-ch.-2.html#app2-bonus"><i class="fa fa-check"></i><b>C.1</b> Bonus structure</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html"><i class="fa fa-check"></i><b>D</b> Supp. materials for ch. 3</a>
<ul>
<li class="chapter" data-level="D.1" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#additional-analyses-exp.-1"><i class="fa fa-check"></i><b>D.1</b> Additional analyses: Exp. 1</a>
<ul>
<li class="chapter" data-level="D.1.1" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#appRC-asymmetries1"><i class="fa fa-check"></i><b>D.1.1</b> Response time, confidence, and metacognitive sensitivity differences</a></li>
<li class="chapter" data-level="D.1.2" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#zroc-curves"><i class="fa fa-check"></i><b>D.1.2</b> zROC curves</a></li>
<li class="chapter" data-level="D.1.3" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#confidence-response-time-alignment"><i class="fa fa-check"></i><b>D.1.3</b> Confidence response-time alignment</a></li>
<li class="chapter" data-level="D.1.4" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#global-metacognitive-estimates"><i class="fa fa-check"></i><b>D.1.4</b> Global metacognitive estimates</a></li>
</ul></li>
<li class="chapter" data-level="D.2" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#additional-analyses-exp.-2"><i class="fa fa-check"></i><b>D.2</b> Additional analyses: Exp. 2</a>
<ul>
<li class="chapter" data-level="D.2.1" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#appRC-asymmetries2"><i class="fa fa-check"></i><b>D.2.1</b> Response time, confidence, and metacognitive sensitivity differences</a></li>
<li class="chapter" data-level="D.2.2" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#zroc-curves-1"><i class="fa fa-check"></i><b>D.2.2</b> zROC curves</a></li>
</ul></li>
<li class="chapter" data-level="D.3" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#additional-analyses-exp.-3"><i class="fa fa-check"></i><b>D.3</b> Additional analyses: Exp. 3</a>
<ul>
<li class="chapter" data-level="D.3.1" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#appRC-asymmetries3"><i class="fa fa-check"></i><b>D.3.1</b> Response time, confidence, and metacognitive sensitivity differences</a></li>
<li class="chapter" data-level="D.3.2" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#appRC-standardonly"><i class="fa fa-check"></i><b>D.3.2</b> Reverse correlation analysis of standard trials only</a></li>
</ul></li>
<li class="chapter" data-level="D.4" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#appRC-PDRC"><i class="fa fa-check"></i><b>D.4</b> Pseudo-discrimination analysis</a>
<ul>
<li class="chapter" data-level="D.4.1" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#exp.-1"><i class="fa fa-check"></i><b>D.4.1</b> Exp. 1</a></li>
<li class="chapter" data-level="D.4.2" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#exp.-2"><i class="fa fa-check"></i><b>D.4.2</b> Exp. 2</a></li>
</ul></li>
<li class="chapter" data-level="D.5" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#app2-simulation"><i class="fa fa-check"></i><b>D.5</b> Stimulus-dependent noise model</a>
<ul>
<li class="chapter" data-level="D.5.1" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#discrimination"><i class="fa fa-check"></i><b>D.5.1</b> Discrimination</a></li>
<li class="chapter" data-level="D.5.2" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#detection-2"><i class="fa fa-check"></i><b>D.5.2</b> Detection</a></li>
<li class="chapter" data-level="D.5.3" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#effects-of-evidence-on-decision-and-confidence-exp.-2-and-3"><i class="fa fa-check"></i><b>D.5.3</b> Effects of evidence on decision and confidence: Exp. 2 and 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="E" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html"><i class="fa fa-check"></i><b>E</b> Supp. materials for ch. 4</a>
<ul>
<li class="chapter" data-level="E.1" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#app3-buttonpresses"><i class="fa fa-check"></i><b>E.1</b> Confidence button presses</a></li>
<li class="chapter" data-level="E.2" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#app3-zROC"><i class="fa fa-check"></i><b>E.2</b> zROC curves</a></li>
<li class="chapter" data-level="E.3" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#app3-GC-DM"><i class="fa fa-check"></i><b>E.3</b> Global confidence design matrix</a></li>
<li class="chapter" data-level="E.4" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#app3-ROIconf"><i class="fa fa-check"></i><b>E.4</b> Effect of confidence in our pre-specified ROIs</a></li>
<li class="chapter" data-level="E.5" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#app3-varianceRat"><i class="fa fa-check"></i><b>E.5</b> SDT variance ratio correlation with the quadratic confidence effect</a></li>
<li class="chapter" data-level="E.6" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#app3-efficiency"><i class="fa fa-check"></i><b>E.6</b> Correlation of metacognitive efficiency with linear and quadratic confidence effects</a></li>
<li class="chapter" data-level="E.7" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#app3-cross"><i class="fa fa-check"></i><b>E.7</b> Confidence-decision cross classification</a></li>
<li class="chapter" data-level="E.8" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#app3-SDT"><i class="fa fa-check"></i><b>E.8</b> Static Signal Detection Theory</a>
<ul>
<li class="chapter" data-level="E.8.1" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#discrimination-1"><i class="fa fa-check"></i><b>E.8.1</b> Discrimination</a></li>
<li class="chapter" data-level="E.8.2" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#detection-3"><i class="fa fa-check"></i><b>E.8.2</b> Detection</a></li>
</ul></li>
<li class="chapter" data-level="E.9" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#app3-Dynamic"><i class="fa fa-check"></i><b>E.9</b> Dynamic Criterion</a>
<ul>
<li class="chapter" data-level="E.9.1" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#discrimination-2"><i class="fa fa-check"></i><b>E.9.1</b> Discrimination</a></li>
<li class="chapter" data-level="E.9.2" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#detection-4"><i class="fa fa-check"></i><b>E.9.2</b> Detection</a></li>
</ul></li>
<li class="chapter" data-level="E.10" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#app3-Monitoring"><i class="fa fa-check"></i><b>E.10</b> Attention Monitoring</a>
<ul>
<li class="chapter" data-level="E.10.1" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#discrimination-3"><i class="fa fa-check"></i><b>E.10.1</b> Discrimination</a></li>
<li class="chapter" data-level="E.10.2" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#detection-5"><i class="fa fa-check"></i><b>E.10.2</b> Detection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="F" data-path="F-supp.-materials-for-ch.-5.html"><a href="F-supp.-materials-for-ch.-5.html"><i class="fa fa-check"></i><b>F</b> Supp. materials for ch. 5</a>
<ul>
<li class="chapter" data-level="F.1" data-path="F-supp.-materials-for-ch.-5.html"><a href="F-supp.-materials-for-ch.-5.html#robustness-region"><i class="fa fa-check"></i><b>F.1</b> Robustness Region</a></li>
</ul></li>
<li class="chapter" data-level="G" data-path="G-reproducibility-receipt.html"><a href="G-reproducibility-receipt.html"><i class="fa fa-check"></i><b>G</b> Reproducibility receipt</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Self-Modelling in Inference about Absence</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-fMRI" class="section level1" number="4">
<h1><span class="header-section-number">Chapter 4</span> Distinct neural contributions to metacognition for detecting (but not discriminating) visual stimuli</h1>
<div id="matan-mazor-karl-j.-friston-stephen-m.-fleming" class="section level4 unnumbered">
<h4>Matan Mazor, Karl J. Friston &amp; Stephen M. Fleming</h4>
<p>Being confident in whether a stimulus is present or absent (a detection judgment) is qualitatively distinct from being confident in the identity of that stimulus (a discrimination judgment). In particular, in detection, evidence can only be available for the presence, not the absence, of a target object. In accordance with this observation, in Chapter <a href="3-ch-RC.html#ch-RC">3</a> we observed slower reaction times, as well as lower and less reliable subjective confidence ratings for decisions about absence. Here we asked if these conceptual and behavioural differences between decisions about presence and absence are also reflected in brain activation patterns, specifically in parietal and prefrontal brain regions that are typically implicated in higher-order thought. In a within-subject, pre-registered and performance-matched fMRI design, we observed quadratic confidence effects in frontopolar cortex for detection but not discrimination. Furthermore, in the right temporoparietal junction, confidence effects were enhanced for judgments of target absence compared to judgments of target presence. We interpret these findings as reflecting qualitative differences between the neural basis of metacognitive evaluation of detection and discrimination, potentially in line with counterfactual or higher-order models of confidence formation in detection.</p>
</div>
<div id="introduction-4" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Introduction</h2>
<p>When foraging for berries, one first needs to decide whether a certain bush bears fruit or not. Only if berries are detected, can one proceed to examine and classify them into a category - are these raspberries or blackberries? The first is a <em>detection</em> task: a decision about whether something is there or not, and the second is a <em>discrimination</em> task: a decision about which item is there. For these types of decisions, it is important not only to understand the decision process that leads to deciding present or absent, or raspberries or blackberries, but also our ability to reflect on and estimate the quality of the decision, known as metacognition. For instance, two foragers working together may want to share their confidence in deciding which bush to tackle next <span class="citation">(Bahrami et al., 2010; C. D. Frith, 2012)</span>.</p>
<p>There is an increasing understanding of the neural basis of confidence in simple decisions, with a network of prefrontal and parietal regions being identified as important for tracking metacognitive beliefs about the accuracy of both perceptual and value-based decisions <span class="citation">(Domenech &amp; Koechlin, 2015; for reviews, see Fleming &amp; Dolan, 2012; Meyniel, Sigman, &amp; Mainen, 2015)</span>. Accordingly, neuropsychological data in humans suggests that damage or impairment of prefrontal function can lead to metacognitive impairments such as noisy or inappropriate confidence judgments <span class="citation">(for a review, see Rouault, Seow, Gillan, &amp; Fleming, 2018)</span>. However, in a majority of these cases, the study of confidence has been restricted to discrimination, or deciding whether a stimulus is from category A or B. Despite their ubiquity and importance in decision-making, much less is known about how confidence is formed in detection settings, in which subjects are asked to make a judgment about whether a target stimulus is present or not.</p>
<p>Computational considerations and behavioural findings suggest that computing confidence in detection judgments may differ from computing confidence in the more commonly studied discrimination tasks. In particular, detection is unique in the landscape of perceptual tasks in that evidence can only be available to support the presence, not the absence, of a target object. This makes confidence ratings in judgments about absence a unique case, where confidence is decoupled from the amount of supporting perceptual evidence. Accordingly, behavioural evidence indicates that metacognitive sensitivity, or the alignment between subjective confidence and objective performance, for judgments about absence is typically impaired compared to metacognitive sensitivity for judgments about presence <span class="citation">(Kanai, Walsh, &amp; Tseng, 2010; Meuwese, Loon, Lamme, &amp; Fahrenfort, 2014)</span>.</p>
<p>Under one family of models (<em>first-order models</em>), confidence in detection judgments is formed in the same way as confidence in discrimination judgments. For example, in evidence-accumulation models, confidence can be evaluated as the distance of the losing accumulator from the threshold at the time of decision <span class="citation">(Merkle &amp; Van Zandt, 2006)</span>. Similarly, in models of discrimination confidence based on <em>Signal Detection Theory</em> (SDT), decision confidence is assumed to be proportional to the strength of the available evidence supporting the decision, which is modeled as the distance of the perceptual sample from the decision criterion on a strength-of-evidence axis <span class="citation">(Wickens, 2002, p. 85)</span>. While first-order models are traditionally symmetric, they can be adapted to account for the asymmetry between judgments about presence and absence. For example, <em>unequal-variance</em> and <em>multi-dimensional SDT models</em> account for the inherent difference between presence and absence by making the signal distribution wider than the noise distribution <span class="citation">(Wickens, 2002, p. 48)</span>, or by assuming a high-dimensional stimulus space, in which the absence of a signal is represented as a distribution centered around the origin <span class="citation">(King &amp; Dehaene, 2014; Wickens, 2002, p. 118)</span>. Importantly, first-order models treat the process of metacogntive evaluation of detection and discrimination as qualitatively similar, with any differences between detection and discrimination emerging from differences in the underlying distributions (uv-SDT), or the mapping between stimulus features and responses (two-dimensional SDT).</p>
<p>In contrast with first-order models of detection confidence, <em>higher-order models</em> treat confidence in judgments about target absence as emerging from a distinct, higher-order cognitive process. For instance, in one version of the higher-order approach, confidence in judgments about absence is assumed to be based on counterfactual estimation of the likelihood of a hypothetical stimulus to be detected, if presented. In other words, subjects may be more confident in the absence of a target object when they believe they would not have missed it, based on their global estimation of task difficulty, or on their current level of attention. A similar type of modeling has been successfully employed in studies of memory, to explain how participants form judgments that an item was not presented during the preceding learning phase, based on their counterfactual expectations about remembering an item <span class="citation">(for example, Glanzer &amp; Adams, 1990)</span>. When applied to the comparison of detection and discrimination, this approach predicts that qualitatively distinct cognitive and neural resources will be recruited when judging confidence in detection responses, due to the additional demand on counterfactual and self-monitoring processes, and that this recruitment will be most pronounced for confidence about absence. In particular, the counterfactual account predicts that responses in the frontopolar cortex, a region which has been shown to track counterfactual world states <span class="citation">(Boorman, Behrens, Woolrich, &amp; Rushworth, 2009)</span>, will show specificity for confidence judgements when inferring the absence of a target.</p>
<p>To test for such qualitative differences, here we set out to directly compare the neural basis of metacognitive evaluation of detection and discrimination responses within two similar low-level perceptual tasks, while controlling for differences in task performance. In a pre-registered design, we asked whether parametric relationships between subjective confidence ratings and the blood-oxygenation-level-dependent (BOLD) signal in a set of predefined prefrontal and parietal regions of interests (ROIs) would show systematic interaction with task (detection/discrimination) and, within detection, type of response (present/absent). To anticipate our results, we observed a quadratic effect of confidence on regional responses in frontopolar cortex for detection, but not for discrimination judgments. In further whole-brain exploratory analyses, we found stronger confidence-related effecs for judgments of absence compared to presence in right temporoparietal junction.</p>
</div>
<div id="methods-and-materials" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Methods and Materials</h2>
<p>All design and analysis details were pre-registered before data acquisition and time-locked using pre-RNG randomization <span class="citation">(Mazor, Mazor, &amp; Mukamel, 2019)</span>. The time-locked protocol folder is available <a href="https://www.github.com/matanmazor/detectionVsDiscrimination_fMRI">in the following GitHub repository</a>. The entire set of preregistered analysis is available <a href="https://www.osf.io/98mv4">in the following OSF Project</a>. Whole-brain imaging results are available in <a href="https://neurovault.org/collections/VVLPQBWK/">NeuroValut</a>.</p>
<div id="participants-7" class="section level3" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Participants</h3>
<p>46 participants took part in the study (ages 18-36, mean = 24 <span class="math inline">\(\pm\)</span> 4; 29 females). 35 participants met our pre-specified inclusion criteria (ages 18-36, mean=24 <span class="math inline">\(\pm\)</span> 4; 20 females). After applying our run-wise exclusion criteria to the data of the remaining 35 participants, our dataset consisted of 5 usable experimental runs from 15 participants, 4 usable experimental runs from 14 participants, 3 usable experimental runs from 5 participants, and 2 usable experimental runs from one participant.</p>
</div>
<div id="design-and-procedure" class="section level3" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Design and procedure</h3>
<p>After a temporally jittered rest period of 500-4000 milliseconds, each trial started with a fixation cross (500 milliseconds), followed by a presentation of a target for 33 milliseconds. In discrimination trials, the target was a circle of diameter 3° containing randomly generated white noise, merged with a sinusoidal grating (2 cycles per degree; oriented 45° or -45°). In half of the detection trials, targets did not contain a sinusoidal grating and consisted of random noise only. After stimulus offset, participants used their right-hand index and middle fingers to make a perceptual decision about the orientation of the grating (discrimination blocks), or about the presence or absence of a grating (detection blocks). The response mapping was counterbalanced between blocks, such that an index finger press was used to indicate a clockwise tilt on half of the trials, and an anticlockwise tilt on the other half. Similarly, in half of the detection trials the index finger was mapped to a ‘yes’ (‘target present’) response, and on the other half to a ‘no’ (‘target absent’) response.</p>
<p>Immediately after making a decision, participants rated their confidence on a 6-point scale by using two keys to increase and decrease their reported confidence level with their left-hand thumb. Confidence levels were indicated by the size and color of a circle presented at the center of the screen. The initial size and color of the circle was determined randomly at the beginning of the confidence rating phase, to decorrelate the number of button presses and the final confidence rating. The mapping between color and size to confidence was counterbalanced between participants: for half of the participants high confidence was mapped to small, red circles, and for the other half high confidence was mapped to large, blue circles. This counterbalancing was employed to isolate confidence-related activations from activations that originate from the perceptual properties of the confidence scale or from differences in the motor requirement to press the upper and lower buttons. The perceptual decision and the confidence rating phases were restricted to 1500 and 2500 milliseconds, respectively. No feedback was delivered to subjects about their performance.</p>
<p>Participants were acquainted with the task in a preceding behavioural session. During this session, task difficulty was adjusted independently for detection and for discrimination, targeting around 70% accuracy on both tasks. We achieved this by adaptively controlling the stimulus signal-to-noise ratio (SNR) once in every 10 trials: increasing the SNR when accuracy fell below 60%, and decreasing it when accuracy exceeded 80%. Performance on the detection and discrimination task was further calibrated to the scanner environment at the beginning of the scanning session, during the acquisition of anatomical (MP-RAGE and fieldmap) images. After completing the calibration phase, participants underwent five ten-minute functional scanner runs, each comprising one detection and one discrimination block of 40 trials each, presented in random order.</p>
<p>To avoid stimulus-driven fluctuations in confidence, grating SNR was fixed within each experimental block. Nevertheless, following experimental blocks with markedly bad (<span class="math inline">\(\leq52.5\%\)</span> ) or good (<span class="math inline">\(\geq85\%\)</span>) accuracy, grating SNR was adjusted for the next block of the same task (SNR level was divided or multiplied by a factor of 0.9 for bad and good performance, respectively). Finally, grating SNR was adjusted for both tasks following runs in which the difference in performance between the two tasks exceeded 16.25% (SNR level was multiplied by the square root of 0.9 for the easier task and divided by the square root of 0.9 for the more difficult task).</p>
<p>To incentivize participants to do their best at the task and rate their confidence accurately, we offered a bonus payment according to the following payment schedule:
bonus = £<span class="math inline">\(\frac{\overrightarrow{accuracy} \cdot \overrightarrow{confidence}}{200}\)</span>
Where <span class="math inline">\(\overrightarrow{accuracy}\)</span> is a vector of 1 and -1 for correct and incorrect responses, and <span class="math inline">\(\overrightarrow{confidence}\)</span> is a vector of integers in the range of 1 to 6, representing confidence reports for all trials. We explained the payment structure to participants in the preceding behavioural session. Specifically, we advised participants that to maximize their bonus they should do their best at the main task, rate the confidence higher when they believe they are correct, and rate their confidence lower when they believe they might be wrong.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fMRI-exp1-design"></span>
<img src="figure/fMRI/design.png" alt="Experimental design for discrimination and for detection trials . Perceptual decisions were reported using the right index and middle fingers, and confidence ratings were reported using the left thumb. A) In discrimination blocks, participants indicated the orientation of a visual grating ('clockwise' or 'counterclockwise'). B) In detection blocks, participants indicated whether a grating was embedded in random noise, or not ('yes' or 'no'). Confidence ratings were made by varying the size and color of a circle, with 6 options ranging from small and red to big and blue. For half of the subjects, high confidence was mapped to a small, red circle. For the other half, high confidence was mapped to a big, blue circle. The initial size and color of the circle was determined randomly at the beginning of the confidence rating phase. Participants performed 10 interleaved 40-trial detection and discrimination blocks inside a 3T MRI scanner." width="\linewidth" />
<p class="caption">
Figure 4.1: Experimental design for discrimination and for detection trials . Perceptual decisions were reported using the right index and middle fingers, and confidence ratings were reported using the left thumb. A) In discrimination blocks, participants indicated the orientation of a visual grating (‘clockwise’ or ‘counterclockwise’). B) In detection blocks, participants indicated whether a grating was embedded in random noise, or not (‘yes’ or ‘no’). Confidence ratings were made by varying the size and color of a circle, with 6 options ranging from small and red to big and blue. For half of the subjects, high confidence was mapped to a small, red circle. For the other half, high confidence was mapped to a big, blue circle. The initial size and color of the circle was determined randomly at the beginning of the confidence rating phase. Participants performed 10 interleaved 40-trial detection and discrimination blocks inside a 3T MRI scanner.
</p>
</div>
</div>
<div id="scanning-parameters" class="section level3" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> Scanning parameters</h3>
<p>Scanning took place at the Wellcome Centre for Human Neuroimaging, London, using a 3 Tesla Siemens Prisma MRI scanner with a 64-channel head coil.
We acquired structural images using an MPRAGE sequence (1x1x1mm voxels, 176 slices, in plane FoV = 256x256 mm<span class="math inline">\(^2\)</span>), followed by a double-echo FLASH (gradient echo) sequence with TE1=10ms and TE2=12.46ms (64 slices, slice thickness = 2mm, gap = 1mm, in plane FoV= 192×192 mm<span class="math inline">\(^2\)</span>, resolution = 3×3 mm<span class="math inline">\(^2\)</span>) that was later used for field inhomogeneity correction. Functional scans were acquired using a 2D EPI sequence, optimized for regions near the orbito-frontal cortex (3.0x3.0x3.0mm voxels, TR=3.36 seconds, TE = 30 ms, 48 slices tilted by -30 degrees with respect to the T&gt;C axis, matrix size = 64x72, Z-shim=-1.4).</p>
</div>
<div id="analysis-1" class="section level3" number="4.2.4">
<h3><span class="header-section-number">4.2.4</span> Analysis</h3>
<p>The preregistered objectives of this study were to:</p>
<ul>
<li><p>Replicate findings of a generic (task-invariant) confidence signal in the activity of medial prefrontal cortex <span class="citation">(Fleming &amp; Dolan, 2012; Morales, Lau, &amp; Fleming, 2018)</span>.</p></li>
<li><p>Test for an interaction between the parametric effect of confidence level and task (detection/discrimination) in the BOLD response in prefrontal cortex ROIs.</p></li>
<li><p>Within detection trials, test for an interaction between the parametric effect of confidence level and response (‘yes’/‘no’) in the BOLD response, specifically in the prefrontal cortex and in frontopolar regions that have previously been associated with counterfactual reasoning <span class="citation">(Boorman, Behrens, Woolrich, &amp; Rushworth, 2009; Donoso, Collins, &amp; Koechlin, 2014)</span>.</p></li>
<li><p>Test for relationships between fluctuations in metacognitive adequacy [a trial-by-trial measure of metacognitive sensitivity; <span class="citation">Wokke, Cleeremans, &amp; Ridderinkhof (2017)</span>], and the BOLD signal separately for detection and for discrimination, and for ‘yes’ and ‘no’ responses within detection.</p></li>
<li><p>Replicate previous findings of between-subject correlations between lateral prefrontal cortex (lPFC) function and metacognitive efficiency [meta-d’/d’; <span class="citation">Fleming &amp; Lau (2014)</span>] in discrimination <span class="citation">(Yokoyama et al., 2010)</span>.</p></li>
<li><p>Identify between-subject functional correlates of metacognitive efficiency in detection. Specifically, ask if metacognitive efficiency in detection is predicted by activity in distinct networks compared to metacognitive efficiency in discrimination.</p></li>
</ul>
<div id="exclusion-criteria" class="section level4" number="4.2.4.1">
<h4><span class="header-section-number">4.2.4.1</span> Exclusion criteria</h4>
<p>Subjects were excluded from all analyses for any of the following pre-specified reasons: missing more than 20% of the trials, performing one of the tasks with accuracy below 60%, exceeding the 4 mm affine motion cutoff criterion in more than 2 experimental runs, and showing a consistent response bias (i.e. using the same response in more than 75% of the trials) in at least one task. Individual scan runs were excluded from all analyses if the participant exceeded the affine motion cutoff, if more than 20% of trials were missed, if mean accuracy was below 60% or if the response bias for one of the tasks exceeded 80%.</p>
<p>In addition, we applied a confidence-related exclusion criterion: participants were excluded if they used the same confidence level in more than 80% of all trials globally or for a particular response, and individual scan runs were excluded if the same confidence level was used in more than 95% of the trials, either globally or for particular response types. Our preregistration document specified that the confidence exclusion criterion will be used to exclude participants from confidence-related analyses only, but we subsequently revised this plan in order to use identical design matrices for all participants.</p>
</div>
<div id="response-conditional-type-ii-roc-curves" class="section level4" number="4.2.4.2">
<h4><span class="header-section-number">4.2.4.2</span> Response conditional type-II ROC curves</h4>
<p>Response conditional ROC (Receiver Operating Characteristic) curves were extracted for the two discrimination and two detection responses. This was done by plotting the cumulative distribution of confidence levels in correct responses against the cumulative distribution of confidence levels in incorrect responses. As a measure of response-specific metacognitive sensitivity, we extracted the area under these curves (<em>AUROC2</em>). The expected AUROC2 for no metacognitive insight (i.e., the confidence distributions are identical for correct and incorrect responses) is 0.5. Perfect metacognitive insight (i.e., confidence in all correct responses is higher than confidence in all incorrect responses) will result in an AUROC2 of 1.</p>
</div>
<div id="imaging-analysis" class="section level4" number="4.2.4.3">
<h4><span class="header-section-number">4.2.4.3</span> Imaging analysis</h4>
<div id="fmri-data-preprocessing" class="section level5 unnumbered">
<h5>fMRI data preprocessing</h5>
<p>Data preprocessing followed the procedure described in <span class="citation">Morales, Lau, &amp; Fleming (2018)</span>:</p>
<blockquote>
<p>“Imaging analysis was performed using SPM12 (Statistical Parametric Mapping; www.fil.ion.ucl.ac.uk/spm). The first five volumes of each run were discarded to allow for T1 stabilization. Functional images were realigned and unwarped using local field maps <span class="citation">(Andersson, Hutton, Ashburner, Turner, &amp; Friston, 2001)</span> and then slice-time corrected <span class="citation">(Sladky et al., 2011)</span>. Each participant’s structural image was segmented into gray matter, white matter, CSF, bone, soft tissue, and air/background images using a nonlinear deformation field to map it onto template tissue probability maps <span class="citation">(Ashburner &amp; Friston, 2005)</span>. This mapping was applied to both structural and functional images to create normalized images in Montreal Neurological Institute (MNI) space. Normalized images were spatially smoothed using a Gaussian kernel (6 mm FWHM). We set a within-run 4 mm affine motion cutoff criterion.”</p>
</blockquote>
<p>Preprocessing and construction of first- and second-level models used standardized pipelines and scripts available at the <a href="https://github.com/metacoglab/MetaLabCore/">MetaLab GitHub page</a></p>
</div>
<div id="regions-of-interest" class="section level5 unnumbered">
<h5>Regions of Interest</h5>
<p>In addition to an exploratory whole-brain analysis (corrected for multiple comparisons at the cluster level), our analysis focused on the following a priori regions of interest, largely following the ROIs used by <span class="citation">Fleming, Van Der Putten, &amp; Daw (2018)</span>:</p>
<ul>
<li><p><em>Frontopolar cortex</em> (FPC, defined anatomically). We used a connectivity-based parcellation <span class="citation">(Neubert, Mars, Thomas, Sallet, &amp; Rushworth, 2014)</span> to define a general FPC region of interest as the total area spanned by areas FPl, FPm and BA46. The right hemisphere mask was mirrored to create a bilateral mask.</p></li>
<li><p><em>Ventromedial prefrontal cortex</em> (vmPFC). The vmPFC ROI was defined as a 8-mm sphere around MNI coordinates [0,46,-7], obtained from a meta-analysis of subjective-value related activations <span class="citation">(Bartra, McGuire, &amp; Kable, 2013)</span> and aligned to the cortical midline.</p></li>
<li><p><em>Bilateral ventral striatum</em>. The ventral striatum ROIs was specified anatomically from the Oxford-Imanova Striatal Strctural Atlas included with <a href="http://fsl.fmrib.ox.ac.uk">FSL</a>.</p></li>
<li><p><em>Posterior medial frontal cortex</em> (pMFC). The pMFC ROI was defined as a 8-mm sphere around MNI coordinates [0, 17, 46], obtained from a functional MRI study on decision confidence) and aligned to the cortical midline <span class="citation">(Fleming, Huijgen, &amp; Dolan, 2012)</span>.</p></li>
<li><p><em>Precuneus</em>. The precuneus ROI was defined as a 8-mm sphere around MNI coordinates [0,-57,18], based on Voxel Based Morphometry studies of metacognitive efficiency <span class="citation">(Fleming, Weil, Nagy, Dolan, &amp; Rees, 2010; McCurdy et al., 2013)</span> and aligned to the cortical midline.</p></li>
</ul>
<p>For the general FPC ROI, small-volume correction was applied to individual voxels within the ROI for all univariate contrasts. For the multivariate analysis, we used a searchlight approach to scan for spatial patterns within the ROI, followed by a correction for multiple comparisons. For all other ROIs, a GLM was fitted to the mean time course of voxels within the region, and multivariate analysis was performed on all voxels within the ROI.
While our pre-registered analysis defined the frontopolar cortex as a single region, we subsequently decided to separately analyze its 3 separate anatomical subregions identified by <span class="citation">Neubert, Mars, Thomas, Sallet, &amp; Rushworth (2014)</span> (FPl, FPm and BA46). The decision to separate the FPC ROI to its subcomponents was made <em>after</em> data collection. These anatomical subregions should not be taken as prior ROIs.</p>
</div>
<div id="univariate-analysis" class="section level5 unnumbered">
<h5>Univariate analysis</h5>
<p>Univariate analysis was based on a design matrix in which different trial types are modeled by different regressors (main design matrix, below). Additionally, to examine the global effect of confidence across trial types, a simpler design matrix was fitted to the data as a first step (global confidence design matrix, below). Experimental runs for each subject were temporally concatenated before estimating the GLM coefficients. This was done in order to maximize sensitivity to response- and task-specific modulations of confidence, given the limited and varying number of trials within each experimental run.</p>
<div id="DM-1" class="section level6 unnumbered">
<h6>Main Design Matrix (DM-1)</h6>
<p>The main design matrix for the univariate GLM analysis consisted of 16 regressors of interest. There was a regressor for each of the eight combinations of task x condition x response: For example, a regressor for detection trials where a signal was present and the subject reported seeing a signal with a ‘yes’ response. The relevant trials were modeled by a boxcar regressor with nonzero entries at the 4300 millisecond interval starting at the offset of the stimulus and ending immediately after the confidence rating phase, convolved with the canonical hemodynamic response function (HRF).
Each of these primary regressors was accompanied by a linear parametric modulation of the confidence reported for each trial. Together, the design matrix included 16 regressors of interest (see table <a href="4-ch-fMRI.html#tab:fMRIDM1">4.1</a>)</p>
<table>
<caption><span id="tab:fMRIDM1">Table 4.1: </span> List of regressors in the main design matrix (DM-1).</caption>
<thead>
<tr class="header">
<th></th>
<th></th>
<th>Task</th>
<th>Stimulus</th>
<th>Response</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>CW_CW</td>
<td>Discrimination</td>
<td>Clockwise</td>
<td>Clockwise</td>
</tr>
<tr class="even">
<td>2</td>
<td>CW_ACW_conf</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>3</td>
<td>CW_ACW</td>
<td>Discrimination</td>
<td>Clockwise</td>
<td>anticlockwise</td>
</tr>
<tr class="even">
<td>4</td>
<td>CW_ACW_conf</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>5</td>
<td>CW_CW</td>
<td>Discrimination</td>
<td>anticlockwise</td>
<td>Clockwise</td>
</tr>
<tr class="even">
<td>6</td>
<td>CW_CW_conf</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>7</td>
<td>ACW_ACW</td>
<td>Discrimination</td>
<td>anticlockwise</td>
<td>anticlockwise</td>
</tr>
<tr class="even">
<td>8</td>
<td>ACW_ACW_conf</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>9</td>
<td>Y_Y</td>
<td>Detection</td>
<td>Signal</td>
<td>Yes</td>
</tr>
<tr class="even">
<td>10</td>
<td>Y_Y_conf</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>11</td>
<td>Y_N</td>
<td>Detection</td>
<td>Signal</td>
<td>No</td>
</tr>
<tr class="even">
<td>12</td>
<td>Y_N_conf</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>13</td>
<td>N_Y</td>
<td>Detection</td>
<td>Noise</td>
<td>Yes</td>
</tr>
<tr class="even">
<td>14</td>
<td>N_Y_conf</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>15</td>
<td>N_N</td>
<td>Detection</td>
<td>Noise</td>
<td>No</td>
</tr>
<tr class="even">
<td>16</td>
<td>N_N_conf</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Trials in which the participant did not respond within the 1500 millisecond time frame were modeled by a separate regressor. The design matrix also include a run-wise constant term regressor, an instruction-screen regressor for the beginning of each block, motion regressors (the 6 motion parameters and their first derivatives as extracted by SPM in the head motion correction preprocessing phase) and regressors for physiological measures. Button presses were modeled as stick functions, convolved with the canonical HRF, in three regressors: two regressors for the right and left right-hand buttons, and one regressor for both up and down left-hand presses. We decided to have one regressor for both types of left-hand presses due to the strong positive correlation of the final confidence rating with the number of ‘increase confidence’ button presses, and the strong negative correlation with the number of ‘decrease confidence’ button presses.</p>
</div>
<div id="global-confidence-design-matrix-gc-dm" class="section level6 unnumbered">
<h6>Global Confidence Design Matrix (GC-DM)</h6>
<p>The global confidence design matrix consisted of 4 regressors of interest. The first two primary regressors were ‘correct trials’ (trials in which the participant was correct, across tasks and responses) and ‘incorrect trials’ (trials in which the participant was incorrect, across tasks and responses). Single events were modeled by a boxcar regressor with nonzero entries at the interval starting at the offset of the stimulus and ending immediately after the confidence rating phase, convolved with the canonical hemodynamic response function (HRF). The duration of this interval was 4300 milliseconds, and not 4000 milliseconds as mistakenly indicated in the preregistration document. Additionally, the design matrix included a confidence parametric modulator for each of the first two regressors. The construction of the regressors and the additional nuisance regressors was handled similarly to the main design.</p>
</div>
<div id="QC-DM" class="section level6 unnumbered">
<h6>Quadratic-Confidence Design Matrix (post-hoc analysis; QC-DM)</h6>
<p>The quadratic-confidence design matrix for the univariate GLM analysis consisted of 12 regressors of interest. There was a regressor for each of the four responses: ‘yes,’ ‘no,’ ‘clockwise’ and ‘anticlockwise.’ Similar to the main design matrix, the relevant trials were modeled by a boxcar regressor with nonzero entries at the 4300 millisecond interval starting at the offset of the stimulus and ending immediately after the confidence rating phase, convolved with the canonical hemodynamic response function (HRF).
Each of these primary regressors was accompanied by two parametric modulators, representing the linear and quadratic effects of confidence. Together, the design matrix included 12 regressors (4 responses + 4 linear confidence regressors + 4 quadratic confidence regressors). The QC-DM included the same set of nuisance regressors as the main design matrix.</p>
</div>
<div id="categoricalDM" class="section level6 unnumbered">
<h6>Categorical-Confidence Design Matrices (post-hoc analysis; CC-DM)</h6>
<p>In order to better understand the nature of the linear interaction between confidence in ‘yes’ and ‘no’ responses, we specified a pair of design matrices - one for each task - in which confidence level was modeled as a categorical variable. Instead of the 8 primary regressors in the main design matrix, this design matrix consisted of only one regressor of interest for all trials, modeled by a boxcar with nonzero entries at the 4300 millisecond interval starting at the offset of the stimulus and ending immediately after the confidence rating phase, convolved with the canonical hemodynamic response function (HRF). This regressor was in turn modulated by a series of 12 dummy (0/1) parametric modulators - one for every response (‘yes’ and ‘no’ for detection and ‘clockwise’ and ‘anticlockwise’ for discrimination) and confidence rating (1-6 for both tasks). Using two design matrices instead of one allowed us to set discrimination trials to be the baseline category for detection, and detection trials as the baseline for discrimination. These design matrices included the same set of nuisance regressors as the main design matrix.</p>
<p>For each participant, we used the beta-estimates from the categorical-confidence design matrices as the input to four response-specific multiple linear regression models, with linear confidence and quadratic confidence as predictors, in addition to an intercept term. The subject-specific coefficients were then subjected to ordinary least squares group-level inference, to compare linear and quadratic effects of confidence between responses. The rational for choosing this two-step approach was its ambivalence to differences in the confidence distributions for the four responses, that may bias the estimation of the quadratic and linear terms.</p>
</div>
</div>
<div id="multivariate-analysis" class="section level5 unnumbered">
<h5>Multivariate analysis</h5>
<p>Multi-voxel pattern analysis <span class="citation">(Norman, Polyn, Detre, &amp; Haxby, 2006)</span> was used to test for consistent spatial patterns in the fMRI data. We used The Decoding Toolbox <span class="citation">(Hebart, Görgen, &amp; Haynes, 2015)</span> and followed the procedures described by <span class="citation">Morales, Lau, &amp; Fleming (2018)</span>. In order to identify brain regions that are implicated in inference about presence and absence, we trained and tested a linear classifier on detection decisions. We classified hits and correct rejections, instead of hits and misses as originally planned, due to an insufficient number of detection misses in some experimental blocks. We then compared the resulting classification accuracy with the cross-classification accuracy of training on detection responses and testing on discrimination confidence and vice versa. The purpose of this comparison was to isolate neural correlates of inference about stimulus absence or presence that should be specific to detection from more general neural correlates of stimulus visibility, that are also expected to affect confidence in discrimination judgements.</p>
<p>The other prespecified multivariate tests were designed to find universal and response-specific spatially multivariate representations of confidence. After conducting this analysis we came to realize that our experimental design was not appropriate for estimating the degree to which the representation of confidence is “response-general.” In our experimental design, confidence is confounded with visual feedback during the confidence-rating phase, such that “response-general” representations of confidence could appear if the spatial pattern of activation was sensitive to the visual feedback in the confidence rating. For completeness, we include the results of this analysis in the appendix (<a href="E-supp.-materials-for-ch.-4.html#app3-cross">E.7</a>), but do not interpret them further.</p>
</div>
</div>
<div id="statistical-inference-1" class="section level4" number="4.2.4.4">
<h4><span class="header-section-number">4.2.4.4</span> Statistical inference</h4>
<p>T-test and anova Bayes factors use a Jeffrey-Zellner-Siow Prior for the null distribution, with a unit prior scale <span class="citation">(Rouder, Morey, Speckman, &amp; Province, 2012; Rouder, Speckman, Sun, Morey, &amp; Iverson, 2009)</span>. Whole-brain fMRI significance was corrected for family-wise error rate at the cluster level (p&lt;0.05), with a cluster defining threshold of p&lt;0.001.</p>
</div>
</div>
</div>
<div id="results-7" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Results</h2>
<p>35 participants performed two perceptual decision-making tasks while being scanned in a 3T MRI scanner: an orientation discrimination task (<em>“was the grating tilted clockwise or anticlockwise?”</em>), and a detection task (<em>“was any grating presented at all?”</em>). At the end of each trial, participants rated their confidence in the accuracy of their decision on a 6-point scale. We adjusted the difficulty of the two tasks in a preceding behavioural session to achieve equal performance of around 70% accuracy. At scanning, 10 discrimination and detection blocks were presented in 5 scanner runs.</p>
<div id="behavioural-results" class="section level3" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Behavioural results</h3>
<p>Task performance was similar for detection (75% accuracy, d’=1.48) and discrimination blocks (76% accuracy, d’=1.50). Repeated measures t-tests failed to detect a difference between tasks both in mean accuracy (<span class="math inline">\(t(34) = -0.90, p = 0.37\)</span>, <span class="math inline">\(BF_{01}= 5.15\)</span>), and d’ ( <span class="math inline">\(t(34) = -0.30, p = 0.76\)</span>, <span class="math inline">\(BF_{01}=7.29\)</span>), indicating that performance was well matched. Responses were also balanced for the two tasks. The probability of responding ‘yes’ (target present) in the detection task was <span class="math inline">\(0.49 \pm 0.11\)</span>, and not significantly different from 0.5 (<span class="math inline">\(t(34) = -0.39, p = 0.70\)</span>, <span class="math inline">\(BF_{01}=7.07\)</span>). The probability of responding ‘clockwise’ in the discrimination task was <span class="math inline">\(0.50 \pm 0.08\)</span>, and not significantly different from 0.5 (<span class="math inline">\(t(34) = 0.22, p=0.87\)</span>, <span class="math inline">\(BF_{01}=7.43\)</span>).</p>
<p>The distribution of confidence ratings was generally similar between the two tasks and four responses. For all four responses, participants were most likely to report the highest confidence rating compared to any other option. Within detection, a significant difference in mean confidence was observed between ‘yes’ (target present) and ‘no’ (target absent) responses, such that participants were more confident in their ‘yes’ responses (<span class="math inline">\(t(34) = -4.85, p&lt;0.0001\)</span>; see Figure <a href="4-ch-fMRI.html#fig:fMRI-exp1-behaviour">4.2</a>). This difference in mean confidence was mostly driven by the higher proportion of maximum confidence ratings in ‘yes’ responses compared to ‘no’ responses (46% of all ‘yes’ responses compared to 26% of all ‘no’ responses, <span class="math inline">\(t(34)=5.63, p&lt;0.00001\)</span>), but persisted even when ignoring the highest ratings (<span class="math inline">\(t(34)=2.39, p&lt;0.05\)</span>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fMRI-exp1-behaviour"></span>
<img src="figure/fMRI/behaviour.png" alt="Upper panels: response conditional type-2 ROC curves. In parentheses: the mean area under the curve. Lower panels: distribution of confidence ratings for the two tasks and four responses. Right panel: Mean accuracy for both tasks. Error bars represent the standard error of the mean." width="\linewidth" />
<p class="caption">
Figure 4.2: Upper panels: response conditional type-2 ROC curves. In parentheses: the mean area under the curve. Lower panels: distribution of confidence ratings for the two tasks and four responses. Right panel: Mean accuracy for both tasks. Error bars represent the standard error of the mean.
</p>
</div>
<p>Metacognitive sensitivity, quantified as the area under the type-II ROC curve, was significantly higher for ‘yes’ compared to ‘no’ responses (<span class="math inline">\(t(34) = 7.83, p&lt;10-8\)</span>; see Figure <a href="4-ch-fMRI.html#fig:fMRI-exp1-behaviour">4.2</a>, as expected <span class="citation">(Meuwese, Loon, Lamme, &amp; Fahrenfort, 2014)</span>. In other words, confidence ratings about the presence of a target stimulus were more diagnostic of accuracy than ratings about target absence, even though both sets of ratings tended to cover the full range of the scale, from low to high confidence. Taking metacognitive sensitivity following discrimination responses as a baseline, we found that this effect was driven by a decrease in metacognitive sensitivity for ‘no’ responses (<span class="math inline">\(t(34) = -4.89, p&lt;0.0001\)</span>), whereas a quantitative increase in metacognitive sensitivity for ‘yes’ responses compared to discrimination was not significant (<span class="math inline">\(t(34)=1.84, p=0.07\)</span>). No difference was observed in metacognitive sensitivity between the two discrimination responses (‘clockwise’ and ‘anticlockwise’; <span class="math inline">\(t(34) = 0.06, p=0.95\)</span>, <span class="math inline">\(BF_{01}=7.6\)</span>). Taken together, these results are consistent with the previously reported selective asymmetry in the fidelity of metacognitive evaluation following judgments about target absence <span class="citation">(Kanai, Walsh, &amp; Tseng, 2010; Meuwese, Loon, Lamme, &amp; Fahrenfort, 2014)</span>.</p>
<p>Response times were faster on average for correct responses (<span class="math inline">\(849 \pm 79\)</span> milliseconds) compared to incorrect responses (<span class="math inline">\(938 \pm 95\)</span> milliseconds; <span class="math inline">\(t(34)=10.59, p&lt;10^{-11}\)</span> for a paired t-test on the log-transformed response times). Within the detection task, ‘yes’ responses were significantly faster than ‘no’ responses (<span class="math inline">\(850 \pm 90\)</span> milliseconds and <span class="math inline">\(896 \pm 103\)</span> milliseconds, respectively; <span class="math inline">\(t(34)=3.16, p&lt;0.005\)</span> for a paired t-test on the log-transformed response times).</p>
</div>
<div id="imaging-results" class="section level3" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Imaging results</h3>
<div id="parametric-effect-of-confidence" class="section level4 unnumbered">
<h4>Parametric effect of confidence</h4>
<p>We next turned to our fMRI data to ask whether confidence-related responses were similar or distinct across tasks (detection / discrimination) and response (target present: ‘yes’ / target absent: ‘no’). We first established the presence of linear confidence-related effects in our a priori ROIs, both across tasks and response types and across correct and incorrect responses, in line with previous findings of “generic” or task-invariant confidence signals in these regions <span class="citation">(Morales, Lau, &amp; Fleming, 2018)</span>. Specifically, high confidence ratings were associated with increased activation in the ventromedial prefrontal cortex (vmPFC), the ventral striatum, and the precuneus. Conversely, activations in the posterior medial frontal cortex (pMFC) were negatively correlated with confidence (see figure <a href="4-ch-fMRI.html#fig:fMRI-exp1-globalConf">4.3</a>). For the confidence effect pattern obtained from the Global-Confidence Design Matrix (GC-DM), see supplementary figure <a href="E-supp.-materials-for-ch.-4.html#app3-GC-DM">E.3</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fMRI-exp1-globalConf"></span>
<img src="figure/fMRI/figure3.png" alt="Univariate parametric effect of confidence. a) Glass brain visualization of global effect of confidence, thresholded at the single voxel level for visualization (p&lt;0.001, uncorrected). Negative confidence effects appears in blue, and positive effects in red. b) Whole brain contrast between confidence in ‘yes’ (target-present) and ‘no’ (target-absent) detection responses, corrected for family-wise error rate at the cluster level (p&lt;0.05) with a cluster defining threshold of p&lt;0.001, uncorrected. c. upper panel: BOLD signal in the rTPJ cluster from panel b as a function of response and confidence. lower panel: mean coefficients of response- and subject-specific multiple linear regression models, predicting rTPJ activation as a linear and quadratic function of confidence.  * - p&lt;0.05; uncorrected for multiple comparisons across the four tests." width="\linewidth" />
<p class="caption">
Figure 4.3: Univariate parametric effect of confidence. a) Glass brain visualization of global effect of confidence, thresholded at the single voxel level for visualization (p&lt;0.001, uncorrected). Negative confidence effects appears in blue, and positive effects in red. b) Whole brain contrast between confidence in ‘yes’ (target-present) and ‘no’ (target-absent) detection responses, corrected for family-wise error rate at the cluster level (p&lt;0.05) with a cluster defining threshold of p&lt;0.001, uncorrected. c. upper panel: BOLD signal in the rTPJ cluster from panel b as a function of response and confidence. lower panel: mean coefficients of response- and subject-specific multiple linear regression models, predicting rTPJ activation as a linear and quadratic function of confidence. * - p&lt;0.05; uncorrected for multiple comparisons across the four tests.
</p>
</div>
</div>
<div id="interaction-of-linear-confidence-effects-with-task-and-response" class="section level4 unnumbered">
<h4>Interaction of linear confidence effects with task and response</h4>
<p>We next asked whether the linear parametric relationship between confidence and BOLD activity differed as a function of task (discrimination vs. detection) and response type (‘yes’ vs. ‘no’ in detection). In the pMFC, vmPFC, ventral striatum and precuneus ROIs, the parametric effect of confidence failed to show a significant difference between the two tasks (all p-values &gt; 0.3), between the two discrimination responses (all p-values &gt; 0.24), or between the two detection responses (all p-values &gt; 0.09). Similarly, no cluster within the pre-specified frontopolar ROI showed a differential effect of confidence as a function of task or response. We show below that this absence of a linear interaction should not be taken as evidence of absence of differences between detection and discrimination, due to the presence of nonlinear interaction effects. In the next section we first explain the analysis steps we took to uncover nonlinear effect of confidence.</p>
</div>
<div id="interaction-of-nonlinear-confidence-effects-with-task-and-response" class="section level4 unnumbered">
<h4>Interaction of nonlinear confidence effects with task and response</h4>
<p>An exploratory whole brain analysis (p&lt;0.05, corrected for multiple comparisons at the cluster-level) revealed no differential confidence effect as a function of task anywhere in the brain. However, within detection, whole-brain analysis revealed that the linear effect of confidence was significantly more negative for ‘no’ compared to ‘yes’ responses in the right temporo-parietal junction (rTPJ: 101 voxels, peak voxel: [54,-46, 26], z = 5.10). To further characterize the nature of the interaction between confidence and response in the rTPJ, we fitted a new design matrix for each task (<a href="4-ch-fMRI.html#categoricalDM">CC-DM</a>) where confidence was represented as a categorical variable with 6 levels instead of one parametric modulator. In contrast to our original design matrix (<a href="4-ch-fMRI.html#DM-1">DM-1</a>) that assumed a linear effect of confidence, this analysis is agnostic as to the functional form of the confidence effect. We then plotted the mean activation level for each combination of response and confidence level in the rTPJ cluster (see Figure <a href="4-ch-fMRI.html#fig:fMRI-exp1-globalConf">4.3</a>, panel c).</p>
<p>The categorical-confidence design matrix revealed a positive quadratic effect of confidence on activation levels in the rTPJ, with stronger activation levels for the two extremities of the confidence scale. We confirmed the presence of a significant quadratic effect of confidence in this region by fitting a second-order polynomial to the response-specific confidence curve of each participant (see <a href="4-ch-fMRI.html#categoricalDM">Methods</a>). This analysis revealed a main quadratic effect of confidence in this region (<span class="math inline">\(t(34) = 5.21, p&lt;0.00001\)</span>), an effect which was stronger in detection compared to discrimination (<span class="math inline">\(t(34)=2.06, p&lt;0.05\)</span>). Importantly, the linear interaction of confidence with detection responses remained significant for this quadratic model, establishing that this response-specific effect is not explained by an overall quadratic pattern (<span class="math inline">\(t(33)=2.09, p&lt;0.05\)</span>; see Figure <a href="4-ch-fMRI.html#fig:fMRI-exp1-globalConf">4.3</a>). More generally, these analyses make clear that linear effects of parametric modulators and their interactions are not exhaustive in their characterization of the confidence-related BOLD response – in this region and potentially in our other ROIs too.</p>
<p>To formally test for such nonlinear differences in the activation profile of other ROIs, we extracted the coefficients from the categorical model for each ROI, and fitted a second-order polynomial separately for the ensuing confidence-related response. Within our a priori ROIs, no quadratic effect of confidence was observed in the pMFC, the precuneus, the ventral striatum, or the vmPFC (see supplementary figure <a href="E-supp.-materials-for-ch.-4.html#app3-ROIconf">E.4</a>). In contrast, in all three anatomical subregions of the frontopolar cortex, we found a positive quadratic effect of confidence, with stronger activations for the two extremities of the confidence scale. Strikingly, in both the FPl and the FPm, this positive quadratic effect of confidence was entirely driven by the detection task (FPm: <span class="math inline">\(t(34)=3.04, p&lt;0.005\)</span>; FPl: <span class="math inline">\(t(34)=3.90, p&lt;0.001\)</span>; see Figure <a href="4-ch-fMRI.html#fig:fMRI-exp1-FPl">4.4</a>). Confidence ratings for the discrimination task however showed a quadratic effect that was not statistically different from zero (FPm: <span class="math inline">\(t(34)=-0.54, p=0.59\)</span>, <span class="math inline">\(BF_{01}=6.61\)</span>; FPl: <span class="math inline">\(t(34)=1.42, p=0.16\)</span>, <span class="math inline">\(BF_{01}=2.92\)</span>). In the FPm, the linear effect of confidence was more negative for detection than for discrimination (<span class="math inline">\(t(34) = -2.11, p&lt;0.05\)</span>), and within detection, more negative for confidence in judgments about absence (‘no’ responses; <span class="math inline">\(t(34) = 2.10, p&lt;0.05\)</span>).</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fMRI-exp1-FPl"></span>
<img src="figure/fMRI/figure4.png" alt="Confidence effect as a function of response in the frontopolar cortex separated into its three anatomical subcomponents: FPm, FPl, and BA 46. Same conventions as in Figure 4.4 * - p&lt;0.05; uncorrected for multiple comparisons." width="0.7\linewidth" />
<p class="caption">
Figure 4.4: Confidence effect as a function of response in the frontopolar cortex separated into its three anatomical subcomponents: FPm, FPl, and BA 46. Same conventions as in Figure <a href="4-ch-fMRI.html#fig:fMRI-exp1-FPl">4.4</a> * - p&lt;0.05; uncorrected for multiple comparisons.
</p>
</div>
<p>Finally, to test for similar quadratic effects of confidence at the whole-brain level, we constructed a new design matrix (in a departure to our pre-registered analysis plan) in which confidence was modeled by a parametric modulator with a polynomial expansion of 2 (<a href="4-ch-fMRI.html#QC-DM">QC-DM</a>). Three clusters in the right hemisphere showed a significantly stronger quadratic effect of confidence in detection compared to discrimination (Figure <a href="4-ch-fMRI.html#fig:fMRI-exp1-quadConf">4.5</a>). These were located in the right superior temporal sulcus (72 voxels, peak voxel: [60,-43,2], Z=3.99), right pre-SMA (130 voxels, peak voxel: [0,35,47], Z=4.07), and right frontopolar cortex, overlapping with our FPl and FPm frontopolar anatomical subregions (51 voxels, peak voxel: [9,65,-10], Z=4.00).</p>
<p>To visualize activity patterns in these regions, we extracted the mean coefficients from the categorical model for these three clusters, and fitted a second-order polynomial separately to each response estimate (see Figure <a href="4-ch-fMRI.html#fig:fMRI-exp1-quadConf">4.5</a>). In addition to the effect of task on the quadratic effect of confidence in all three clusters, the linear effect of confidence in the right frontopolar cluster was significantly more negative for detection, compared to discrimination (<span class="math inline">\(t(34)=-3.13, p&lt;0.005\)</span>). For both tasks, inter-subject variability in metacognitive efficiency [measured as meta-d’/d’; <span class="citation">Maniscalco &amp; Lau (2010)</span>] was not reliably correlated with linear or quadratic parametric effect of confidence in any of the three regions (see Supplementary Figure <a href="E-supp.-materials-for-ch.-4.html#app3-varianceRat">E.5</a> in the supplementary materials).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fMRI-exp1-quadConf"></span>
<img src="figure/fMRI/figure5.png" alt="Left, top panel: a glass-brain representation of a contrast between the quadratic effects of confidence in detection and in discrimination, whole-brain corrected for family-wise error rate at the cluster-level (p&lt;.05)  with a cluster-defining threshold of p&lt;.001, uncorrected). Remaining panels: mean betas from the categorical model for each of the four responses and six confidence ratings, for the three indicated clusters. The second-order polynomial coefficients for these estimates are presented below each plot. Significance is only indicated for the linear effects, which are orthogonal to the quadratic contrast used to select the clusters. * - p&lt;0.05; ** - p&lt;0.01" width="\linewidth" />
<p class="caption">
Figure 4.5: Left, top panel: a glass-brain representation of a contrast between the quadratic effects of confidence in detection and in discrimination, whole-brain corrected for family-wise error rate at the cluster-level (p&lt;.05) with a cluster-defining threshold of p&lt;.001, uncorrected). Remaining panels: mean betas from the categorical model for each of the four responses and six confidence ratings, for the three indicated clusters. The second-order polynomial coefficients for these estimates are presented below each plot. Significance is only indicated for the linear effects, which are orthogonal to the quadratic contrast used to select the clusters. * - p&lt;0.05; ** - p&lt;0.01
</p>
</div>
</div>
</div>
<div id="computational-models" class="section level3" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> Computational models</h3>
<p>We next considered alternative computational-level explanations for the detection-specific quadratic activation profile. Specifically, we evaluated how latent model variables or belief states change non-linearly as a function of confidence in three candidate model architectures (see <a href="4-ch-fMRI.html#fig:fMRI-exp1-models">4.6</a>): a static ‘Signal Detection’ model, a ‘Dynamic Criterion’ model where policy changes as a function of previous perceptual samples, and an ‘Attention Monitoring’ model in which beliefs about fluctuations in attention inform decisions and confidence judgments. A detailed formal description of the three models is available in the appendix (sections <a href="E-supp.-materials-for-ch.-4.html#app3-SDT">E.8</a>, <a href="E-supp.-materials-for-ch.-4.html#app3-Dynamic">E.9</a> and <a href="E-supp.-materials-for-ch.-4.html#app3-Monitoring">E.10</a>), and Matlab implementations are available in the following <a href="https://github.com/matanmazor/detectionVsDiscrimination_fMRI/blob/master/simulation/simulations.mlx">page</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fMRI-exp1-models"></span>
<img src="figure/fMRI/models.png" alt="The three models (left) and their prediction for confidence effects (right). Top panel: In Signal Detection Theory, perceptual decisions and confidence ratings are generated by comparing the sensory evidence to a fixed set of criteria. In detection the ’signal’ distribution is assumed to have higher variance. Plotting the absolute value of the log likelihood ratio as a function of decision and confidence results in a linear curve for discrimination, and a pronounced quadratic effect for 'yes' responses in detection, an effect that is specific to unequal-variance SDT. Middle panel: In a Dynamic Criterion model beliefs about the mean and variance of the perceptual distributions are updated as a function of incoming samples (plotted as circles) and the decision criterion is shifted accordingly. Plotting the absolute change in criterion placement as a function of decision and confidence results in a quadratic effect of confidence for detection responses only. Bottom: In the Attention Monitoring model, beliefs about overall attentiveness (’onTask’ node) probabilistically reflect sensory precision. Plotting beliefs about overall attentiveness as a function of decision and confidence results in an overall quadratic effect of confidence, and an interaction between 'yes' and 'no' responses in detection." width="\linewidth" />
<p class="caption">
Figure 4.6: The three models (left) and their prediction for confidence effects (right). Top panel: In Signal Detection Theory, perceptual decisions and confidence ratings are generated by comparing the sensory evidence to a fixed set of criteria. In detection the ’signal’ distribution is assumed to have higher variance. Plotting the absolute value of the log likelihood ratio as a function of decision and confidence results in a linear curve for discrimination, and a pronounced quadratic effect for ‘yes’ responses in detection, an effect that is specific to unequal-variance SDT. Middle panel: In a Dynamic Criterion model beliefs about the mean and variance of the perceptual distributions are updated as a function of incoming samples (plotted as circles) and the decision criterion is shifted accordingly. Plotting the absolute change in criterion placement as a function of decision and confidence results in a quadratic effect of confidence for detection responses only. Bottom: In the Attention Monitoring model, beliefs about overall attentiveness (’onTask’ node) probabilistically reflect sensory precision. Plotting beliefs about overall attentiveness as a function of decision and confidence results in an overall quadratic effect of confidence, and an interaction between ‘yes’ and ‘no’ responses in detection.
</p>
</div>
<p>First, we consider the static Signal Detection Theory (SDT) model. In SDT models of confidence formation, the log likelihood-ratio between the two competing hypotheses (<span class="math inline">\(LLR=log\frac{p(x|S1)}{p(x|S2)}\)</span>) is a useful measure for determining the certainty with which one should commit to a choice. The mapping between the perceptual sample x and the LLR is linear for equal-variance SDT, which is often used to model discrimination, but quadratic for unequal-variance SDT, which is often used to model detection. It then follows that if confidence is proportional to the distance of the sample x from the decision criterion, neuronal populations that represent the relative likelihood of a choice being correct (be it LLR or an analogue quantity) will show a quadratic tuning function of confidence in detection and a linear tuning function in discrimination, similar to that observed in FPC, pre-SMA and STS. However, LLR is also expected to scale more strongly with confidence in yes responses (see simulation results in Figure <a href="4-ch-fMRI.html#fig:fMRI-exp1-models">4.6</a>, upper panel), which was not observed in these brain regions. This model also predicts a stronger quadratic effect of confidence in participants for which the variance ratio between the signal and noise distributions is particularly high. However, the variance ratio was not significantly correlated with the quadratic effect of confidence in any of these regions, as would be expected if they were representing LLR or a similar quantity (see Appendix section <a href="E-supp.-materials-for-ch.-4.html#app3-varianceRat">E.5</a>).</p>
<p>For the next two models, confidence was assumed to be directly proportional to the LLR, with the measured signal representing internal beliefs about hidden model parameters. In the ‘Dynamic Criterion’ model, we considered whether a quadratic effect of confidence in detection may reflect the active tuning of decision policy in the absence of explicit feedback <span class="citation">(Guggenmos, Wilbertz, Hebart, &amp; Sterzer, 2016; Ko &amp; Lau, 2012)</span>. In the model, beliefs about the underlying distributions are updated on a trial-to-trial basis, and in turn affect the placement of decision criterion. The Dynamic Criterion model predicts that the magnitude of shift in decision criterion will display a positive quadratic relation to confidence (LLR) in detection but not discrimination (see simulation results in Figure <a href="4-ch-fMRI.html#fig:fMRI-exp1-models">4.6</a>, middle panel). This is because the problem is asymmetric in detection, and decision policy should depend on beliefs about both sensory precision (or the relative variance of the noise and signal distribution) and expected signal strength (mean of the signal distribution), which is not the case for a symmetric discrimination problem.</p>
<p>Notably, the pattern of criterion shifts in the Dynamic Criterion model resembled the task-specific effect of confidence in the FPC, STS and pre-SMA. As a post-hoc test of a role for these regions in criterion adjustment, we examined sequential pairs of trials of the same stimulus category (for example, a signal present trial that was followed by a signal present trial), and contrasted ‘repeat’ trials with ‘switch’ trials (for example, [‘yes,’ ‘yes’] vs. [‘yes,’ ‘no’]). The Dynamic Criterion model predicts stronger activation in switch compared to stay trials in both detection and discrimination. The FPl showed a weak effect in this direction (<span class="math inline">\(t = 2.03, p=0.05, d = 0.34\)</span>), whereas FPm, pre-SMA, right BA10 and STS did not (all p-values&gt;0.15).</p>
<p>Finally, we considered a higher-order ‘Attention Monitoring’ model in which beliefs about one’s current attentional state (precision or inverse variance in SDT) are taken into account when making perceptual decisions and confidence ratings on detection trials. This model formalizes the notion that after not detecting a target the participant may ask ‘Given my current attentional state, would I have missed the target?’ The Attention Monitoring model thus makes different predictions for confidence in detection ‘no’ (target absent) responses, where the participant is assumed to reflect on the detection-likelihood of hypothetical targets, compared to ‘yes’ (target absent) responses, similar to the activation profile observed in the rTPJ. However, this model also predicts a pronounced quadratic confidence profile for all four responses, which we do not see in our data.</p>
</div>
</div>
<div id="discussion-3" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Discussion</h2>
<p>Previous studies of the neural basis of human perceptual decision-making have tended to focus on discrimination judgments, such as sorting stimuli into category A or B. The general computational architecture supporting discrimination judgments can be naturally extended to support detection (for instance, within signal detection theory). However, computational considerations and behavioral findings suggest that forming confidence in detection judgments may rest on qualitatively distinct cognitive and neural processes in comparison to generating confidence in discrimination judgments.</p>
<p>To test for such differences, here we acquired functional MRI data from 35 participants who reported their subjective confidence in judgments about stimulus type (discrimination), and target presence or absence (detection). These judgments were given on separate trials that were well-matched for stimulus characteristics, response requirements and task difficulty. Across both tasks, we found the expected linear effects of confidence in our pre-specified regions of interest in the prefrontal and parietal cortex. Specifically, in the precuneus, vmPFC, pMFC and ventral striatum, the effect of confidence was invariant to task and response. In contrast, having adjusted our planned design matrix to be sensitive to non-monotonic effects of confidence, we observed a quadratic effect of confidence in detection judgments in the frontopolar cortex (medial and lateral surfaces of BA10), that was absent for discrimination judgments. Similar quadratic activation profiles were observed for both ‘yes’ and ‘no’ responses. Whole-brain analysis revealed a similar effect of task on the quadratic effect of confidence in the right STS and the pre-SMA. Since task performance was matched across the two tasks and since we did not observe overall differences in activation between detection and discrimination, these differences in confidence profiles are unlikely to originate from experimental confounds such as task difficulty, but instead indicate a unique neurocognitive contribution to metacognition of detection judgments. In what follows we will unpack what this contribution might be.</p>
<p>The three regions that showed an interaction of the quadratic expansion of confidence with task in our whole-brain analysis (right frontopolar cortex, right STS, and pre-SMA), as well as two anatomical subcomponents of our frontopolar ROI (FPl and FPm), all shared a very similar activation profile. In detection, the quadratic effect of confidence was positive, but was almost entirely absent for the discrimination task. Follow-up analysis confirmed that this difference was not driven by motor aspects of the confidence rating procedure, such as the number of increase or decrease confidence steps taken to reach the desired confidence level, which was similar for the two tasks (see Appendix <a href="E-supp.-materials-for-ch.-4.html#app3-buttonpresses">E.1</a>). Ours is not the first report of a quadratic relation between activation in prefrontal cortical structures and different subjective ratings. For example, in a study by <span class="citation">Christensen, Ramsøy, Lund, Madsen, &amp; Rowe (2006)</span>, participants were presented with masked stimuli and gave subjective visibility ratings on a three-point scale. The right frontopolar cortex showed decreased activation for ‘clear perception’ and ‘no perception’ categories relative to a middle ‘vague perception’ category. Similarly, <span class="citation">De Martino, Bobadilla-Suarez, Nouguchi, Sharot, &amp; Love (2017)</span> reported a quadratic effect of product desirability in the pMFC. However, for both of the above cases, a quadratic effect can reflect a monotonic relationship with an implicit representation of subjective confidence <span class="citation">(Lebreton, Abitbol, Daunizeau, &amp; Pessiglione, 2015)</span>. For example, participants may be more confident in the ‘clear perception’ and ‘no perception’ responses compared to the ‘vague perception’ option, or more confident about liking or not liking a product, compared to when using the middle parts of the liking scale. This explanation cannot account for the observed quadratic trend in our case, where in addition to strong activation levels for the highest confidence ratings in target presence and absence, we also find strong activation levels for the lowest levels of confidence.</p>
<p>We are unable to determine whether this effect originates from one homogeneous population of neurons that shows a quadratic effect of detection confidence, or from two overlapping populations that show nonlinear positive and negative effects of detection confidence – summing to an overall quadratic effect at the voxel level [similar to positive and negative confidence-selective neurons in the human posterior parietal cortex; <span class="citation">Rutishauser, Aflalo, Rosario, Pouratian, &amp; Andersen (2018)</span>]. Addressing this question would require higher spatial resolution, for example using single-cell recordings in patients. Furthermore, because confidence judgments were always preceded by perceptual decisions in our design, we cannot determine whether the observed effects reflect an implicit representation of uncertainty, computed in parallel with the perceptual decision itself, or a higher-order representation that emerges at the explicit confidence rating phase. Future studies which use model-based estimates of covert decision confidence <span class="citation">(Bang &amp; Fleming, 2018)</span> or EEG-informed fMRI to resolve early and late processing stages <span class="citation">(Gherman &amp; Philiastides, 2018)</span> may answer this question.</p>
<p>We considered three alternative computational models that were able to account for asymmetries between detection and discrimination activation profiles. An unequal variance signal detection theory model provided a simple account of the asymmetry between detection and discrimination, but could not account for the similar quadratic profiles observed for ‘yes’ and ‘no’ responses. A more direct test of the proposal that a detection-specific quadratic effect of confidence originates from the unequal-variance properties of stimulus distributions in detection would be to test for similar effects in a discrimination task in which one category of stimuli is of higher variance <span class="citation">(e.g., Denison, Adler, Carrasco, &amp; Ma, 2018)</span>. In contrast, the Dynamic Criterion model provided good qualitative accounts for distinct regional activation profiles, and the Attention Monitoring account predicted an interaction between confidence in judgments about presence and absence. However, the Attention Monitoring model also predicted a quadratic effect in discrimination, which we did not see.</p>
<p>Notably, both of these models share the need to learn (in the Dynamic Criterion model) or estimate (in the Attention Monitoring model) the current level of precision (inverse variance) in detection. Such online precision estimation evinces a profound asymmetry between detection and discrimination tasks: in discrimination tasks, one simply has to evaluate the relative evidence for different causes of sensory samples, under some prior belief about sensory precision; namely, the precision of the likelihood that any particular cause (e.g., clockwise or anticlockwise orientation) would generate sensory samples. In contrast, detection presents a difficult (ill-posed, dual estimation) problem. When assessing the evidence for the absence of a target, there could be no sensory evidence because the target is not there or because precision is low (or both). This puts pressure on the estimation of precision to resolve conditional dependencies between posterior beliefs about target presence and the precision with which it can be detected. In short, two things have to be estimated; the posterior expectation about the target and posterior beliefs about precision <span class="citation">(Clark, 2013; Feldman &amp; Friston, 2010; Haarsma et al., 2018; Palmer, Auksztulewicz, Ondobaka, &amp; Kilner, 2019; Parr, Benrimoh, Vincent, &amp; Friston, 2018)</span>.</p>
<p>In line with a role in monitoring of attention or precision, right TPJ showed a negative effect of confidence that was stronger for ‘target absent’ responses compared to ‘target present’ responses in detection. This cluster was closest to the posterior subdivision of the right TPJ [TPJp-R; <span class="citation">Igelström, Webb, &amp; Graziano (2015)</span>], which is most strongly associated with reasoning about others’ beliefs <span class="citation">(Igelström, Webb, Kelly, &amp; Graziano, 2016)</span>. In addition to its role in Theory of Mind <span class="citation">(Lee &amp; McCarthy, 2016; Saxe &amp; Wexler, 2005)</span>, previous work has highlighted the importance of the rTPJ in controlling attention <span class="citation">(Dugué, Merriam, Heeger, &amp; Carrasco, 2018; Geng &amp; Vossel, 2013; Lee &amp; McCarthy, 2016; Marois, Yi, &amp; Chun, 2004)</span> and filtering distractors in visual search <span class="citation">(Shulman, Astafiev, McAvoy, d’Avossa, &amp; Corbetta, 2007)</span>. Furthermore, damage to the rTPJ can result in visual hemineglect: a condition in which stimuli in the left visual hemifield fail to reach awareness <span class="citation">(Shulman, Astafiev, McAvoy, d’Avossa, &amp; Corbetta, 2007)</span>. Together, these observations have led to a proposal (the ‘Attention Schema Theory’) that the rTPJ is maintaining a simplified representation of one’s own and others’ attentional states, and that this function makes this region essential for maintaining conscious awareness <span class="citation">(Graziano &amp; Webb, 2015)</span>.</p>
<p>The current Attention Monitoring model fits well with the Attention Schema Theory. A representation of one’s current attentional state is a useful source of information for determining confidence in detection judgments, because stimuli are more likely to be missed when participants are not paying careful attention. This will be specifically useful for judgments about stimulus absence: if a target was not observed, the participant may reason something along the lines of ‘given my current state of attention, I was not very likely to miss a target, therefore I can be very confident that a target was not presented.’ In support of this idea, the typically poor metacognitive evaluations of decisions about stimulus absence are partially recovered when task difficulty is controlled by manipulating attention rather than stimulus visibility <span class="citation">(Kanai, Walsh, &amp; Tseng, 2010; Kellij, Fahrenfort, Lau, Peters, &amp; Odegaard, 2018)</span>, suggesting that subjects may harness information about their attentional state to inform their confidence judgments. Interestingly, the frontopolar cortex, which showed a detection-specific quadratic effect of confidence in our experiment, has also been implicated in attentional control via the gating of internal and external modes of attention <span class="citation">(Burgess, Gilbert, &amp; Dumontheil, 2007)</span> and in discriminating between imagined and externally perceived memory items <span class="citation">(Simons, Davis, Gilbert, Frith, &amp; Burgess, 2006; Turner, Simons, Gilbert, Frith, &amp; Burgess, 2008)</span>. Together, the engagement of this set of regions in detection confidence hints at a potential role for self-monitoring of attention in metacognition of detection.</p>
<p>To conclude, we find a quadratic effect of confidence in detection judgments in several brain regions, including the frontopolar cortex and rTPJ. In the frontopolar cortex, this quadratic effect was not seen for discrimination judgments. In the rTPJ, we also found a linear effect of confidence that was more negative for judgments about stimulus absence compared to judgments about stimulus presence. We consider three computational accounts of our results, two of which implicate the learning and estimation of signal-to-noise statistics as promising accounts of the observed detection-specific activation profiles. However, while each of these accounts could explain some of our findings, none of the models could provide a complete account of the data. Further work is needed to decide between these alternatives, or to suggest new ones.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="3-ch-RC.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="5-ch-asymmetry.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["thesis.pdf", "PDF"], ["thesis.epub", "EPUB"], ["thesis.docx", "Word"]],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
