<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Evidence weightings in confidence judgments for detection and discrimination | Self-Modelling in Inference about Absence</title>
  <meta name="description" content="Chapter 3 Evidence weightings in confidence judgments for detection and discrimination | Self-Modelling in Inference about Absence" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Evidence weightings in confidence judgments for detection and discrimination | Self-Modelling in Inference about Absence" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Evidence weightings in confidence judgments for detection and discrimination | Self-Modelling in Inference about Absence" />
  
  
  

<meta name="author" content="Matan Mazor" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="2-ch-MVS.html"/>
<link rel="next" href="4-ch-fMRI.html"/>
<style type="text/css">
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
</style>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"></a></li>
<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preliminary Content</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#impact-statement"><i class="fa fa-check"></i>Impact Statement</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="0.1" data-path="introduction.html"><a href="introduction.html#inference-about-absence"><i class="fa fa-check"></i><b>0.1</b> Inference about absence</a></li>
<li class="chapter" data-level="0.2" data-path="introduction.html"><a href="introduction.html#formalabsence"><i class="fa fa-check"></i><b>0.2</b> Probabilistic reasoning, criterion setting, and self knowledge</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#symmetrical-definition"><i class="fa fa-check"></i>Symmetrical definition:</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#dissymmetrical-definition"><i class="fa fa-check"></i>Dissymmetrical definition:</a></li>
<li class="chapter" data-level="0.2.1" data-path="introduction.html"><a href="introduction.html#intro-2nd-order"><i class="fa fa-check"></i><b>0.2.1</b> Second-order cognition</a></li>
<li class="chapter" data-level="0.2.2" data-path="introduction.html"><a href="introduction.html#detectionmodels"><i class="fa fa-check"></i><b>0.2.2</b> Computational models of detection</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#htm"><i class="fa fa-check"></i>The High-Threshold model</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#sdt"><i class="fa fa-check"></i>Signal Detection Theory</a></li>
</ul></li>
<li class="chapter" data-level="0.3" data-path="introduction.html"><a href="introduction.html#detection-i-would-have-noticed-it"><i class="fa fa-check"></i><b>0.3</b> Detection: “I would have noticed it”</a></li>
<li class="chapter" data-level="0.4" data-path="introduction.html"><a href="introduction.html#intro:search"><i class="fa fa-check"></i><b>0.4</b> Visual search: “I would have found it”</a></li>
<li class="chapter" data-level="0.5" data-path="introduction.html"><a href="introduction.html#memory-i-would-have-remembered-it"><i class="fa fa-check"></i><b>0.5</b> Memory: “I would have remembered it”</a></li>
<li class="chapter" data-level="0.6" data-path="introduction.html"><a href="introduction.html#the-development-of-a-self-model"><i class="fa fa-check"></i><b>0.6</b> The development of a self-model</a></li>
<li class="chapter" data-level="0.7" data-path="introduction.html"><a href="introduction.html#this-thesis"><i class="fa fa-check"></i><b>0.7</b> This thesis</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-ch-termination.html"><a href="1-ch-termination.html"><i class="fa fa-check"></i><b>1</b> Efficient search termination without task experience: the role of second-order knowledge about visual search</a>
<ul>
<li class="chapter" data-level="1.1" data-path="1-ch-termination.html"><a href="1-ch-termination.html#introduction-1"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="1-ch-termination.html"><a href="1-ch-termination.html#experiment-1"><i class="fa fa-check"></i><b>1.2</b> Experiment 1</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="1-ch-termination.html"><a href="1-ch-termination.html#participants"><i class="fa fa-check"></i><b>1.2.1</b> Participants</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-ch-termination.html"><a href="1-ch-termination.html#procedure"><i class="fa fa-check"></i><b>1.2.2</b> Procedure</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-ch-termination.html"><a href="1-ch-termination.html#randomization"><i class="fa fa-check"></i><b>1.2.3</b> Randomization</a></li>
<li class="chapter" data-level="1.2.4" data-path="1-ch-termination.html"><a href="1-ch-termination.html#data-analysis"><i class="fa fa-check"></i><b>1.2.4</b> Data analysis</a></li>
<li class="chapter" data-level="1.2.5" data-path="1-ch-termination.html"><a href="1-ch-termination.html#results"><i class="fa fa-check"></i><b>1.2.5</b> Results</a></li>
<li class="chapter" data-level="1.2.6" data-path="1-ch-termination.html"><a href="1-ch-termination.html#additional-analysis-first-trial-only"><i class="fa fa-check"></i><b>1.2.6</b> Additional analysis: first trial only</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-ch-termination.html"><a href="1-ch-termination.html#experiment-2"><i class="fa fa-check"></i><b>1.3</b> Experiment 2</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="1-ch-termination.html"><a href="1-ch-termination.html#participants-1"><i class="fa fa-check"></i><b>1.3.1</b> Participants</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-ch-termination.html"><a href="1-ch-termination.html#procedure-1"><i class="fa fa-check"></i><b>1.3.2</b> Procedure</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-ch-termination.html"><a href="1-ch-termination.html#results-1"><i class="fa fa-check"></i><b>1.3.3</b> Results</a></li>
<li class="chapter" data-level="1.3.4" data-path="1-ch-termination.html"><a href="1-ch-termination.html#additional-analyses"><i class="fa fa-check"></i><b>1.3.4</b> Additional Analyses</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-ch-termination.html"><a href="1-ch-termination.html#discussion"><i class="fa fa-check"></i><b>1.4</b> Discussion</a></li>
<li class="chapter" data-level="1.5" data-path="1-ch-termination.html"><a href="1-ch-termination.html#conclusion"><i class="fa fa-check"></i><b>1.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html"><i class="fa fa-check"></i><b>2</b> Internal models of visual search are rich, person-specific, and mostly accurate</a>
<ul>
<li class="chapter" data-level="2.1" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#introduction-2"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#experiments-1-and-2-shape-orientation-and-color"><i class="fa fa-check"></i><b>2.2</b> Experiments 1 and 2: shape, orientation, and color</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#participants-2"><i class="fa fa-check"></i><b>2.2.1</b> Participants</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#procedure-2"><i class="fa fa-check"></i><b>2.2.2</b> Procedure</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#results-2"><i class="fa fa-check"></i><b>2.2.3</b> Results</a></li>
<li class="chapter" data-level="" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#estimation-accuracy"><i class="fa fa-check"></i>Estimation accuracy</a></li>
<li class="chapter" data-level="" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#a-graded-representation-of-search-efficiency"><i class="fa fa-check"></i>A graded representation of search efficiency</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#experiments-3-and-4-complex-unfamiliar-stimuli"><i class="fa fa-check"></i><b>2.3</b> Experiments 3 and 4: complex, unfamiliar stimuli</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#participants-3"><i class="fa fa-check"></i><b>2.3.1</b> Participants</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#procedure-3"><i class="fa fa-check"></i><b>2.3.2</b> Procedure</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#results-3"><i class="fa fa-check"></i><b>2.3.3</b> Results</a></li>
<li class="chapter" data-level="" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#estimation-time"><i class="fa fa-check"></i>Estimation time</a></li>
<li class="chapter" data-level="" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#asymmetry"><i class="fa fa-check"></i>Visual search asymmetry</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-ch-MVS.html"><a href="2-ch-MVS.html#discussion-1"><i class="fa fa-check"></i><b>2.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-ch-RC.html"><a href="3-ch-RC.html"><i class="fa fa-check"></i><b>3</b> Evidence weightings in confidence judgments for detection and discrimination</a>
<ul>
<li class="chapter" data-level="3.1" data-path="3-ch-RC.html"><a href="3-ch-RC.html#introduction-3"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="3-ch-RC.html"><a href="3-ch-RC.html#experiment-1-1"><i class="fa fa-check"></i><b>3.2</b> Experiment 1</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="3-ch-RC.html"><a href="3-ch-RC.html#methods"><i class="fa fa-check"></i><b>3.2.1</b> Methods</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-ch-RC.html"><a href="3-ch-RC.html#randomization-1"><i class="fa fa-check"></i><b>3.2.2</b> Randomization</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-ch-RC.html"><a href="3-ch-RC.html#analysis"><i class="fa fa-check"></i><b>3.2.3</b> Analysis</a></li>
<li class="chapter" data-level="3.2.4" data-path="3-ch-RC.html"><a href="3-ch-RC.html#results-4"><i class="fa fa-check"></i><b>3.2.4</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-ch-RC.html"><a href="3-ch-RC.html#experiment-2-1"><i class="fa fa-check"></i><b>3.3</b> Experiment 2</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="3-ch-RC.html"><a href="3-ch-RC.html#methods-1"><i class="fa fa-check"></i><b>3.3.1</b> Methods</a></li>
<li class="chapter" data-level="3.3.2" data-path="3-ch-RC.html"><a href="3-ch-RC.html#randomization-2"><i class="fa fa-check"></i><b>3.3.2</b> Randomization</a></li>
<li class="chapter" data-level="3.3.3" data-path="3-ch-RC.html"><a href="3-ch-RC.html#results-5"><i class="fa fa-check"></i><b>3.3.3</b> Results</a></li>
<li class="chapter" data-level="3.3.4" data-path="3-ch-RC.html"><a href="3-ch-RC.html#detection-signal-trials-1"><i class="fa fa-check"></i><b>3.3.4</b> Detection signal trials</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-ch-RC.html"><a href="3-ch-RC.html#experiment-3"><i class="fa fa-check"></i><b>3.4</b> Experiment 3</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="3-ch-RC.html"><a href="3-ch-RC.html#methods-2"><i class="fa fa-check"></i><b>3.4.1</b> Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-ch-RC.html"><a href="3-ch-RC.html#results-6"><i class="fa fa-check"></i><b>3.4.2</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-ch-RC.html"><a href="3-ch-RC.html#discussion-2"><i class="fa fa-check"></i><b>3.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-ch-fMRI.html"><a href="4-ch-fMRI.html"><i class="fa fa-check"></i><b>4</b> Distinct neural contributions to metacognition for detecting (but not discriminating) visual stimuli</a>
<ul>
<li class="chapter" data-level="4.1" data-path="4-ch-fMRI.html"><a href="4-ch-fMRI.html#introduction-4"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="4-ch-fMRI.html"><a href="4-ch-fMRI.html#methods-and-materials"><i class="fa fa-check"></i><b>4.2</b> Methods and Materials</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="4-ch-fMRI.html"><a href="4-ch-fMRI.html#participants-7"><i class="fa fa-check"></i><b>4.2.1</b> Participants</a></li>
<li class="chapter" data-level="4.2.2" data-path="4-ch-fMRI.html"><a href="4-ch-fMRI.html#design-and-procedure"><i class="fa fa-check"></i><b>4.2.2</b> Design and procedure</a></li>
<li class="chapter" data-level="4.2.3" data-path="4-ch-fMRI.html"><a href="4-ch-fMRI.html#scanning-parameters"><i class="fa fa-check"></i><b>4.2.3</b> Scanning parameters</a></li>
<li class="chapter" data-level="4.2.4" data-path="4-ch-fMRI.html"><a href="4-ch-fMRI.html#analysis-1"><i class="fa fa-check"></i><b>4.2.4</b> Analysis</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-ch-fMRI.html"><a href="4-ch-fMRI.html#results-7"><i class="fa fa-check"></i><b>4.3</b> Results</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="4-ch-fMRI.html"><a href="4-ch-fMRI.html#behavioural-results"><i class="fa fa-check"></i><b>4.3.1</b> Behavioural results</a></li>
<li class="chapter" data-level="4.3.2" data-path="4-ch-fMRI.html"><a href="4-ch-fMRI.html#imaging-results"><i class="fa fa-check"></i><b>4.3.2</b> Imaging results</a></li>
<li class="chapter" data-level="4.3.3" data-path="4-ch-fMRI.html"><a href="4-ch-fMRI.html#computational-models"><i class="fa fa-check"></i><b>4.3.3</b> Computational models</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-ch-fMRI.html"><a href="4-ch-fMRI.html#discussion-3"><i class="fa fa-check"></i><b>4.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html"><i class="fa fa-check"></i><b>5</b> Metacognitive asymmetries in visual perception</a>
<ul>
<li class="chapter" data-level="5.1" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#introduction-5"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#methods-3"><i class="fa fa-check"></i><b>5.2</b> Methods</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#participants-8"><i class="fa fa-check"></i><b>5.2.1</b> Participants</a></li>
<li class="chapter" data-level="5.2.2" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#procedure-4"><i class="fa fa-check"></i><b>5.2.2</b> Procedure</a></li>
<li class="chapter" data-level="5.2.3" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#data-analysis-1"><i class="fa fa-check"></i><b>5.2.3</b> Data analysis</a></li>
<li class="chapter" data-level="5.2.4" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#analysis-plan"><i class="fa fa-check"></i><b>5.2.4</b> Dependent variables and analysis plan</a></li>
<li class="chapter" data-level="5.2.5" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#statistical-power"><i class="fa fa-check"></i><b>5.2.5</b> Statistical power</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#data-availability"><i class="fa fa-check"></i><b>5.3</b> Data availability</a></li>
<li class="chapter" data-level="5.4" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#code-availability"><i class="fa fa-check"></i><b>5.4</b> Code availability</a></li>
<li class="chapter" data-level="5.5" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#deviations"><i class="fa fa-check"></i><b>5.5</b> Deviations from pre-registration</a></li>
<li class="chapter" data-level="5.6" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#results-8"><i class="fa fa-check"></i><b>5.6</b> Results</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#experiment-1-q-vs.-o"><i class="fa fa-check"></i><b>5.6.1</b> Experiment 1: <em>Q</em> vs. <em>O</em></a></li>
<li class="chapter" data-level="5.6.2" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#experiment-2-c-vs.-o"><i class="fa fa-check"></i><b>5.6.2</b> Experiment 2: C vs. O</a></li>
<li class="chapter" data-level="5.6.3" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#experiment-3-tilted-vs.-vertical-lines"><i class="fa fa-check"></i><b>5.6.3</b> Experiment 3: tilted vs. vertical lines</a></li>
<li class="chapter" data-level="5.6.4" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#experiment-4-curved-vs.-straight-lines"><i class="fa fa-check"></i><b>5.6.4</b> Experiment 4: curved vs. straight lines</a></li>
<li class="chapter" data-level="5.6.5" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#experiment-5-upward-tilted-vs.-downward-tilted-cubes"><i class="fa fa-check"></i><b>5.6.5</b> Experiment 5: upward-tilted vs. downward-tilted cubes</a></li>
<li class="chapter" data-level="5.6.6" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#experiment-6-flipped-vs.-normal-letters"><i class="fa fa-check"></i><b>5.6.6</b> Experiment 6: flipped vs. normal letters</a></li>
<li class="chapter" data-level="5.6.7" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#asymmetry-summary"><i class="fa fa-check"></i><b>5.6.7</b> Experiments 1-6: summary</a></li>
<li class="chapter" data-level="5.6.8" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#experiment-7-exploratory-grating-vs.-noise"><i class="fa fa-check"></i><b>5.6.8</b> Experiment 7 (exploratory): grating vs. noise</a></li>
<li class="chapter" data-level="5.6.9" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#exploratory-analysis"><i class="fa fa-check"></i><b>5.6.9</b> Exploratory analysis</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#discussion-4"><i class="fa fa-check"></i><b>5.7</b> Discussion</a></li>
<li class="chapter" data-level="5.8" data-path="5-ch-asymmetry.html"><a href="5-ch-asymmetry.html#conclusion-1"><i class="fa fa-check"></i><b>5.8</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html"><i class="fa fa-check"></i>General Discussion</a>
<ul>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html#didnotfind"><i class="fa fa-check"></i>What I didn’t find</a>
<ul>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html#chapter-1-no-correlation-with-explicit-metacognition"><i class="fa fa-check"></i>Chapter 1: no correlation with explicit metacognition</a></li>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html#chapter-3-no-effect-of-confidence-in-signal-presence"><i class="fa fa-check"></i>Chapter 3: no effect of confidence in signal presence</a></li>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html#chapter-4-only-minor-differences-in-brain-activity-between-inference-about-absence-and-presence"><i class="fa fa-check"></i>Chapter 4: only minor differences in brain activity between inference about absence and presence</a></li>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html#chapter-5-no-metacognitive-asymmetry-between-default-complying-and-default-violating-signals"><i class="fa fa-check"></i>Chapter 5: no metacognitive asymmetry between default-complying and default-violating signals</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html#withoutselfmodel"><i class="fa fa-check"></i>Inference about absence without self-modelling</a>
<ul>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html#patch"><i class="fa fa-check"></i>Patch-leaving in foraging</a></li>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html#absenceperception"><i class="fa fa-check"></i>Direct perception</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html#future-directions"><i class="fa fa-check"></i>Future directions</a>
<ul>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html#failures"><i class="fa fa-check"></i>Failures of a self-model</a></li>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html#inference-about-asbence-in-multi-dimensional-and-hierarchical-representational-spaces"><i class="fa fa-check"></i>Inference about asbence in multi-dimensional and hierarchical representational spaces</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="general-discussion.html"><a href="general-discussion.html#conclusion-2"><i class="fa fa-check"></i>Conclusion</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-app1-SDT.html"><a href="A-app1-SDT.html"><i class="fa fa-check"></i><b>A</b> Signal Detection Theory</a>
<ul>
<li class="chapter" data-level="A.1" data-path="A-app1-SDT.html"><a href="A-app1-SDT.html#app1-ROC"><i class="fa fa-check"></i><b>A.1</b> ROC and zROC curves</a></li>
<li class="chapter" data-level="A.2" data-path="A-app1-SDT.html"><a href="A-app1-SDT.html#app1-uvSDT"><i class="fa fa-check"></i><b>A.2</b> Unequal-variance (uv) SDT</a></li>
<li class="chapter" data-level="A.3" data-path="A-app1-SDT.html"><a href="A-app1-SDT.html#app1-mc"><i class="fa fa-check"></i><b>A.3</b> SDT Measures for Metacognition</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="B-app1-RT.html"><a href="B-app1-RT.html"><i class="fa fa-check"></i><b>B</b> Supp. materials for ch. 1</a>
<ul>
<li class="chapter" data-level="B.1" data-path="B-app1-RT.html"><a href="B-app1-RT.html#effect-of-rt-based-trial-exclusion"><i class="fa fa-check"></i><b>B.1</b> Effect of RT-based trial exclusion</a>
<ul>
<li class="chapter" data-level="B.1.1" data-path="B-app1-RT.html"><a href="B-app1-RT.html#experiment-1-2"><i class="fa fa-check"></i><b>B.1.1</b> Experiment 1</a></li>
<li class="chapter" data-level="B.1.2" data-path="B-app1-RT.html"><a href="B-app1-RT.html#experiment-2-2"><i class="fa fa-check"></i><b>B.1.2</b> Experiment 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="C-supp.-materials-for-ch.-2.html"><a href="C-supp.-materials-for-ch.-2.html"><i class="fa fa-check"></i><b>C</b> Supp. materials for ch. 2</a>
<ul>
<li class="chapter" data-level="C.1" data-path="C-supp.-materials-for-ch.-2.html"><a href="C-supp.-materials-for-ch.-2.html#app2-bonus"><i class="fa fa-check"></i><b>C.1</b> Bonus structure</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html"><i class="fa fa-check"></i><b>D</b> Supp. materials for ch. 3</a>
<ul>
<li class="chapter" data-level="D.1" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#additional-analyses-exp.-1"><i class="fa fa-check"></i><b>D.1</b> Additional analyses: Exp. 1</a>
<ul>
<li class="chapter" data-level="D.1.1" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#appRC-asymmetries1"><i class="fa fa-check"></i><b>D.1.1</b> Response time, confidence, and metacognitive sensitivity differences</a></li>
<li class="chapter" data-level="D.1.2" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#zroc-curves"><i class="fa fa-check"></i><b>D.1.2</b> zROC curves</a></li>
<li class="chapter" data-level="D.1.3" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#confidence-response-time-alignment"><i class="fa fa-check"></i><b>D.1.3</b> Confidence response-time alignment</a></li>
<li class="chapter" data-level="D.1.4" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#global-metacognitive-estimates"><i class="fa fa-check"></i><b>D.1.4</b> Global metacognitive estimates</a></li>
</ul></li>
<li class="chapter" data-level="D.2" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#additional-analyses-exp.-2"><i class="fa fa-check"></i><b>D.2</b> Additional analyses: Exp. 2</a>
<ul>
<li class="chapter" data-level="D.2.1" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#appRC-asymmetries2"><i class="fa fa-check"></i><b>D.2.1</b> Response time, confidence, and metacognitive sensitivity differences</a></li>
<li class="chapter" data-level="D.2.2" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#zroc-curves-1"><i class="fa fa-check"></i><b>D.2.2</b> zROC curves</a></li>
</ul></li>
<li class="chapter" data-level="D.3" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#additional-analyses-exp.-3"><i class="fa fa-check"></i><b>D.3</b> Additional analyses: Exp. 3</a>
<ul>
<li class="chapter" data-level="D.3.1" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#appRC-asymmetries3"><i class="fa fa-check"></i><b>D.3.1</b> Response time, confidence, and metacognitive sensitivity differences</a></li>
<li class="chapter" data-level="D.3.2" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#appRC-standardonly"><i class="fa fa-check"></i><b>D.3.2</b> Reverse correlation analysis of standard trials only</a></li>
</ul></li>
<li class="chapter" data-level="D.4" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#appRC-PDRC"><i class="fa fa-check"></i><b>D.4</b> Pseudo-discrimination analysis</a>
<ul>
<li class="chapter" data-level="D.4.1" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#exp.-1"><i class="fa fa-check"></i><b>D.4.1</b> Exp. 1</a></li>
<li class="chapter" data-level="D.4.2" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#exp.-2"><i class="fa fa-check"></i><b>D.4.2</b> Exp. 2</a></li>
</ul></li>
<li class="chapter" data-level="D.5" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#app2-simulation"><i class="fa fa-check"></i><b>D.5</b> Stimulus-dependent noise model</a>
<ul>
<li class="chapter" data-level="D.5.1" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#discrimination"><i class="fa fa-check"></i><b>D.5.1</b> Discrimination</a></li>
<li class="chapter" data-level="D.5.2" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#detection-2"><i class="fa fa-check"></i><b>D.5.2</b> Detection</a></li>
<li class="chapter" data-level="D.5.3" data-path="D-appRC-everything.html"><a href="D-appRC-everything.html#effects-of-evidence-on-decision-and-confidence-exp.-2-and-3"><i class="fa fa-check"></i><b>D.5.3</b> Effects of evidence on decision and confidence: Exp. 2 and 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="E" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html"><i class="fa fa-check"></i><b>E</b> Supp. materials for ch. 4</a>
<ul>
<li class="chapter" data-level="E.1" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#app3-buttonpresses"><i class="fa fa-check"></i><b>E.1</b> Confidence button presses</a></li>
<li class="chapter" data-level="E.2" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#app3-zROC"><i class="fa fa-check"></i><b>E.2</b> zROC curves</a></li>
<li class="chapter" data-level="E.3" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#app3-GC-DM"><i class="fa fa-check"></i><b>E.3</b> Global confidence design matrix</a></li>
<li class="chapter" data-level="E.4" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#app3-ROIconf"><i class="fa fa-check"></i><b>E.4</b> Effect of confidence in our pre-specified ROIs</a></li>
<li class="chapter" data-level="E.5" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#app3-varianceRat"><i class="fa fa-check"></i><b>E.5</b> SDT variance ratio correlation with the quadratic confidence effect</a></li>
<li class="chapter" data-level="E.6" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#app3-efficiency"><i class="fa fa-check"></i><b>E.6</b> Correlation of metacognitive efficiency with linear and quadratic confidence effects</a></li>
<li class="chapter" data-level="E.7" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#app3-cross"><i class="fa fa-check"></i><b>E.7</b> Confidence-decision cross classification</a></li>
<li class="chapter" data-level="E.8" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#app3-SDT"><i class="fa fa-check"></i><b>E.8</b> Static Signal Detection Theory</a>
<ul>
<li class="chapter" data-level="E.8.1" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#discrimination-1"><i class="fa fa-check"></i><b>E.8.1</b> Discrimination</a></li>
<li class="chapter" data-level="E.8.2" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#detection-3"><i class="fa fa-check"></i><b>E.8.2</b> Detection</a></li>
</ul></li>
<li class="chapter" data-level="E.9" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#app3-Dynamic"><i class="fa fa-check"></i><b>E.9</b> Dynamic Criterion</a>
<ul>
<li class="chapter" data-level="E.9.1" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#discrimination-2"><i class="fa fa-check"></i><b>E.9.1</b> Discrimination</a></li>
<li class="chapter" data-level="E.9.2" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#detection-4"><i class="fa fa-check"></i><b>E.9.2</b> Detection</a></li>
</ul></li>
<li class="chapter" data-level="E.10" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#app3-Monitoring"><i class="fa fa-check"></i><b>E.10</b> Attention Monitoring</a>
<ul>
<li class="chapter" data-level="E.10.1" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#discrimination-3"><i class="fa fa-check"></i><b>E.10.1</b> Discrimination</a></li>
<li class="chapter" data-level="E.10.2" data-path="E-supp.-materials-for-ch.-4.html"><a href="E-supp.-materials-for-ch.-4.html#detection-5"><i class="fa fa-check"></i><b>E.10.2</b> Detection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="F" data-path="F-supp.-materials-for-ch.-5.html"><a href="F-supp.-materials-for-ch.-5.html"><i class="fa fa-check"></i><b>F</b> Supp. materials for ch. 5</a>
<ul>
<li class="chapter" data-level="F.1" data-path="F-supp.-materials-for-ch.-5.html"><a href="F-supp.-materials-for-ch.-5.html#robustness-region"><i class="fa fa-check"></i><b>F.1</b> Robustness Region</a></li>
</ul></li>
<li class="chapter" data-level="G" data-path="G-reproducibility-receipt.html"><a href="G-reproducibility-receipt.html"><i class="fa fa-check"></i><b>G</b> Reproducibility receipt</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Self-Modelling in Inference about Absence</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch-RC" class="section level1" number="3">
<h1><span class="header-section-number">Chapter 3</span> Evidence weightings in confidence judgments for detection and discrimination</h1>
<div id="matan-mazor-lucie-charles-roni-or-maimon-mor-stephen-m.-fleming" class="section level4 unnumbered">
<h4>Matan Mazor, Lucie Charles, Roni Or Maimon Mor &amp; Stephen M. Fleming</h4>
<p>In Chapters <a href="1-ch-termination.html#ch-termination">1</a> and <a href="2-ch-MVS.html#ch-MVS">2</a> I examined inference about absence and its relation to self-modelling in visual search, where a target is present or absent in an array of distractors. In this Chapter, I examine inference about absence in a near-threshold detection setting, where the location of the target is known and no distractors are present. Previous studies of near-threshold discrimination revealed a <em>positive evidence bias</em> (PEB) in discrimination confidence: confidence in perceptual decisions is more sensitive to evidence in support of the decision than to conflicting evidence. Recent theoretical proposals suggest that a PEB is due to observers adopting a detection-like strategy when rating their confidence, one that has functional benefits for metacognition in real-world settings where detectability and discriminability often go hand in hand. In three experiments (one lab-based and two online) we first successfully replicate a PEB in discrimination confidence. We then show that a PEB is observed in detection decisions, where participants report the presence or absence of a stimulus, regardless of its identity. We discuss our findings in relation to models that account for a positive evidence bias as emerging from a confidence-specific heuristic, and alternative models where decision and confidence are generated by the same, Bayes-rational process.</p>
</div>
<div id="introduction-3" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Introduction</h2>
<!-- First-year statistics courses teach that â€œthe absence of evidence is not evidence of absenceâ€ [@altman1995statistics]. In other words, failing to find a statistically significant effect is not sufficient justification for that an effect does not exist. To make claims about the absence of an effect, one needs to incorporate information about test sensitivity (statistical power), specificity (significance level), and the probability of an effect to exist prior to seeing any data. Similarly, when facing a perceptual detection problem (*is there any signal in the noise?*), failure to detect a signal is not by itself sufficient to conclude that a signal is absent. In this case, information of potential relevance is the sensitivity of oneâ€™s senses to external signals, the tendency to hallucinate in the absence of true signal, and the prior probability of signal presence. The same considerations are also relevant when forming subjective confidence in the presence or absence of external signals. -->
<!-- Adult humans have been shown to rationally integrate information about the prior of a signal and the statistical power of the test when forming confidence in the absence of a signal in an abstract decision making task [@hsu2017absence]. However, it remains unknown whether similar computational considerations apply in perceptual settings, where uncertainty about the presence or absence of a signal is affected by both external (signal intensity) and internal (sensory precision) sources, and where these sources are not explicitly signaled. What information is incorporated into confidence ratings in visual detection, and whether this varies between decisions about signal absence or presence, is still an open question. Specifically, while confidence in decisions about target presence can scale with stimulus intensity, it is unclear what stimulus features, if any, will affect confidence in stimulus absence, given that decisions about absence can only be based on the lack of perceptual evidence for stimulus presence.  -->
<p>When considering two alternative hypotheses, the probability of a chosen hypothesis to be correct is not only a function of the likelihood of observations under the chosen hypothesis, but also under the unchosen one. For example, when deciding that a random dot display was drifting to the right and not to the left, confidence should not only positively weigh motion energy to the right (<em>positive evidence</em>), but also negatively weigh motion energy to the left (<em>negative evidence</em>). However, when rating their subjective confidence, subjects place disproportional weight on evidence in favour of the choice, giving rise to a <em>positive evidence bias</em> <span class="citation">(Koizumi, Maniscalco, &amp; Lau, 2015; Peters et al., 2017; Rollwage et al., 2020; Samaha &amp; Denison, 2020; Sepulveda et al., 2020; Zylberberg, Barttfeld, &amp; Sigman, 2012)</span>. Put differently, confidence ratings in discrimination are sensitive not only to the <em>relative evidence</em> of the chosen hypothesis compared with the unchosen one (also termed <em>balance of evidence</em>), but also to the <em>sum evidence</em> for the two hypotheses [which for perceptual decisions is often related to <em>visibility</em>; see Fig. <a href="3-ch-RC.html#fig:RC-2dmodel">3.1</a>, left panel; <span class="citation">Rausch, Hellmann, &amp; Zehetleitner (2018)</span>].</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RC-2dmodel"></span>
<img src="figure/RC/2dmodel_enhanced.png" alt="Discrimination and detection in a two-dimensional Signal Detection Theory model. Left: in a two-dimensional SDT model, percepts \(e\) are sampled from one of two Gaussian distributions (here centered at (0,1) and (1,0)). We define relative evidence as \(e_{S1}-e_{S2}\) and sum evidence as \(e_{S1}+e_{S2}\). Circles represent cross-sections of two-dimensional distributions. Center and Left: response and confidence accuracy are maximized when based on a log-likelihood ratio for the two stimulus categories. Center: in discrimination, this yields optimal decision and confidence criteria that are based on relative evidence (distance from the main diagonal), irrespective of sum evidence. Right: in detection, percepts in the absence of a stimulus are sampled from a Gaussian distribution centered at (0,0). This yields optimal decision and confidence that are based on a non-linear interaction between relative and sum evidence." width="\textwidth" />
<p class="caption">
Figure 3.1: Discrimination and detection in a two-dimensional Signal Detection Theory model. Left: in a two-dimensional SDT model, percepts <span class="math inline">\(e\)</span> are sampled from one of two Gaussian distributions (here centered at (0,1) and (1,0)). We define relative evidence as <span class="math inline">\(e_{S1}-e_{S2}\)</span> and sum evidence as <span class="math inline">\(e_{S1}+e_{S2}\)</span>. Circles represent cross-sections of two-dimensional distributions. Center and Left: response and confidence accuracy are maximized when based on a log-likelihood ratio for the two stimulus categories. Center: in discrimination, this yields optimal decision and confidence criteria that are based on relative evidence (distance from the main diagonal), irrespective of sum evidence. Right: in detection, percepts in the absence of a stimulus are sampled from a Gaussian distribution centered at (0,0). This yields optimal decision and confidence that are based on a non-linear interaction between relative and sum evidence.
</p>
</div>
<p>To account for this apparently irrational discounting of incongruent evidence in confidence formation, <span class="citation">Maniscalco, Peters, &amp; Lau (2016)</span> point out that outside of a lab setting, representational spaces are so high-dimensional that keeping track of evidence for every possible stimulus category is not feasible. For example, to be confident that an object is an apple, one would have to incorporate evidence for this object not being an orange, a banana, a book and a ferret, among an infinite many other unsupported hypotheses. To resolve this engineering challenge, metacognitive systems may have evolved to weigh evidence for the chosen hypothesis only, while ignoring conflicting evidence. This is similar to rating confidence not in the identity of a stimulus relative to other hypothetical stimuli, but in the presence of a stimulus relative to absence. Such a strategy is reasonable, as in Signal Detection space, samples that are farther away from the origin (high visibility) are on average farther away from the discrimination criterion (high discriminability). This strategy is then carried over to the lab, where decisions are made in low-dimensional representational spaces, and where keeping track of evidence for the two alternative stimulus categories is in fact feasible.</p>
<p>A more recent model identified the origin of this response-congruent heuristic not in the curse of dimensionality, but in the variance structure of perceptual evidence <span class="citation">(Miyoshi &amp; Lau, 2020)</span>. In a series of simulations, the authors augmented a bidimensional Signal Detection model with realistic assumptions about the sensory encoding of signal and noise, most importantly that the variance of signal tends to be higher than that of noise. In these settings, a Response Congruent Evidence (RCE) heuristic provided more accurate confidence judgments, meaning ones that are more aligned with objective accuracy, than did a Balance of Evidence (BE) heuristic. Again, this model implies that adopting a detection-like strategy when rating one’s confidence might have functional benefits for metacognition.</p>
<p>Notably, both models imply a link between confidence in discrimination, and detection judgments about the presence or absence of a stimulus. In a detection setting with multiple possible targets, the likelihood ratio between stimulus presence and absence is more sensitive to positive evidence for the detected stimulus compared to evidence for the absence of other, undetected stimuli (see Fig. <a href="3-ch-RC.html#fig:RC-2dmodel">3.1</a>, right panel). Perhaps surprisingly, however, despite several recent studies finding that discrimination confidence is detection-like, there has been limited focus on the complementary question: do detection decisions share features of discrimination confidence, such as a positive evidence bias? In other words, when faced with a detection task where targets are drawn from two stimulus classes, would detection decisions be sensitive to sum evidence (like discrimination confidence), or to the relative evidence for presence for one category over the other? Moreover, little is known about confidence in these detection responses: would confidence in the presence of a target stimulus be susceptible to the same positive evidence bias as confidence in stimulus type? Finally, we asked whether detection confidence ratings would be sensitive to some form of positive evidence bias not only in decisions about target presence, but also in decisions about target absence.</p>
<!-- A detection disposition to confidence judgments in discrimination is then a case of bounded rationality under limited cognitive resources. Keeping track of evidence for all stimulus categories is hard (or practically impossible in natural representational spaces), so people base their confidence in the identity of a stimulus on evidence that is easily available to their cognitive system, namely evidence for the presence of this particular stimulus.  -->
<!-- Focusing on sum evidence is rational if subjects are rating their confidence not in the identity of the stimulus, but in the presence or absence of a signal. For example, when evidence is equally high for both stimulus categories, confidence in stimulus identity should be low (low relative evidence; see Fig. \@ref(fig:RC-2dmodel), middle panel), but confidence in the presence of a stimulus, regardless of its identity, should be high (high sum evidence; see Fig. \@ref(fig:RC-2dmodel), right panel). A positive evidence bias in discrimination judgments may indicate that participants are rating their confidence not in the accuracy of their choice, but in the presence of a signal. Indeed, a model that assumed a detection disposition toward discrimination confidence accounted for a set of behavioural findings, including this positive evidence bias [@maniscalco2016heuristic]. -->
<!-- Some decisions however are asymmetric by design, not just in the cognitive or metacognitive processing of incoming evidence. For example, when deciding if a target object is present or absent in a display, evidence can only be available to support presence, and inference about absence is based on a failure to accumulate evidence for presence, not on the accumulation of evidence for absence.  -->
<p>To examine these questions, we conducted three experiments: one lab-based (N=10, 1800 trials per participant) and two online (N=102/100, 112/168 trials per participant). Participants performed discrimination and detection decisions on noisy stimuli, and rated their confidence in their decisions. Using reverse correlation analysis, we measured the influence of random fluctuations in stimulus energy on both responses and confidence ratings, and tested for the existence of processing asymmetries between detection ‘yes’ and ‘no’ responses [in response time, general confidence, and metacognitive sensitivity; <span class="citation">Meuwese, Loon, Lamme, &amp; Fahrenfort (2014)</span>; <span class="citation">Mazor, Friston, &amp; Fleming (2020)</span>; <span class="citation">Kellij, Fahrenfort, Lau, Peters, &amp; Odegaard (2021)</span>; <span class="citation">Mazor, Moran, &amp; Fleming (2021)</span>]. In all three experiments, we replicated previous findings of a positive evidence bias in confidence in discrimination of motion direction and relative luminance <span class="citation">(Zylberberg, Barttfeld, &amp; Sigman, 2012)</span>. In contrast, our understanding of decision and confidence formation in detection has evolved and changed following each experiment, as evident in our pre-registration documents. When considering the results of all three experiments together, we conclude that, similar to discrimination confidence, detection decisions and confidence ratings are also sensitive to a positive evidence bias (we use the word bias here to mean a deviation from equal weighting of positive and negative evidence, and not in the sense of a deviation from rationality). We discuss our findings with respect to recent theoretical proposals regarding the origin of a positive evidence bias in discrimination confidence.</p>
<!-- In Exp. 2, where reverse correlation revealed an accumulation of positive evidence for stimulus absence, we find no metacognitive asymmetry between the two detection responses. We discuss our findings as drawing a link between discrimination confidence ratings and detection responses, but not detection confidence ratings.  -->
</div>
<div id="experiment-1-1" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Experiment 1</h2>
<div id="methods" class="section level3" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Methods</h3>
<div id="participants-4" class="section level4" number="3.2.1.1">
<h4><span class="header-section-number">3.2.1.1</span> Participants</h4>
<p>The research complied with all relevant ethical regulations, and was approved by the Research Ethics Committee of University College London (study ID number 1260/003). 10 participants were recruited via the UCL’s psychology subject pool, and gave their informed consent prior to their participation. Each participant performed four sessions of 600 trials each, in blocks of 100 trials. Sessions took place on different days and consisted of 3 discrimination blocks interleaved with 3 detection blocks.</p>
</div>
<div id="experimental-procedure" class="section level4" number="3.2.1.2">
<h4><span class="header-section-number">3.2.1.2</span> Experimental procedure</h4>
<p>The experimental procedure for Exp. 1 largely followed the procedure described in <span class="citation">Zylberberg, Barttfeld, &amp; Sigman (2012)</span>, Exp. 1. Participants observed a random-dot kinematogram for a fixed duration of 700 ms. In discrimination trials, the direction of motion was one of two opposite directions with equal probability, and participants reported the observed direction by pressing one of two arrow keys on a standard keyboard. In detection blocks participants reported whether there was coherent motion by pressing one of two arrow keys on a standard keyboard. In half of the detection trials dots moved coherently to one of two opposite directions, and in the other half they moved randomly.</p>
<p>In both detection and discrimination blocks, following a decision participants indicated their confidence in their decision. Confidence was reported on a continuous scale ranging from chance to complete certainty. To avoid systematic response biases affecting confidence reports, the orientation (vertical or horizontal) and polarity (e.g., right or left) of the scale was set to agree with the type 1 response. For example, following a down arrow press, a vertical confidence bar was presented where ‘guess’ is at the center of the screen and ‘certain’ appeared at the lower end of the scale (see Fig. <a href="3-ch-RC.html#fig:RC-exp1-design">3.2</a>).</p>
<p>To control for response requirements, for five subjects the dots moved to the right or to the left, and for the other five subjects they moved upward or downward. The first group made discrimination judgments with the right and left keys and detection judgments with the up and down keys, and this mapping was reversed for the second group. The number of coherently moving dots (‘motion coherence’) was adjusted to maintain performance at around 70% accuracy for detection and discrimination tasks independently. This was achieved by measuring mean accuracy after every 20 trials, and adjusting coherence by a step of 3% if accuracy fell below 60% or went above 80%.</p>
<p>Stimuli for discrimination blocks were generated using the exact same procedure reported in <span class="citation">Zylberberg, Barttfeld, &amp; Sigman (2012)</span><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. Trials started with a presentation of a fixation cross for one second, immediately followed by stimulus presentation. The stimulus consisted of 152 white dots (diameter = 0.14°), presented within a 6.5° circular aperture centered on the fixation point for 700 milliseconds (42 frames, frame rate = 60 Hz). Dots were grouped in two sets of 56 dots each. Every other frame, the dots of one set were replaced with a new set of randomly positioned dots. For a coherence value of <span class="math inline">\(c&#39;\)</span>, a proportion of <span class="math inline">\(c&#39;\)</span> of the dots from the second set moved coherently in one direction by a fixed distance of 0.33°, while the remaining dots in the set moved in random directions by a fixed distance of 0.33°. On the next update, the sets were switched, to prevent participants from tracing the position of specific dots. Frame-specific coherence values were sampled for each screen update from a normal distribution centred around the coherence value <span class="math inline">\(c\)</span> with a standard deviation of 0.07, with the constraint that <span class="math inline">\(c&#39;\)</span> must be a number between 0 and 1.</p>
<p>Stimuli for detection blocks were generated using a similar procedure, with the only difference being that on a random half of the trials coherence was set to 0%, without random sampling of coherence values for different frames (see Fig. 1).</p>
<p>At the end of each experimental block (100 trials), participants estimated the number of correct responses they have made.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RC-exp1-design"></span>
<img src="figure/RC/designExp1.png" alt="Task design for Experiment 1. In both discrimination and detection blocks, participants viewed 700 milliseconds of a random dot motion array, after which they made a keyboard response to indicate their decision (motion direction in discrimination, signal absence or presence in detection), followed by a continuous confidence report using the mouse. 5 participants viewed vertically moving dots and indicated their detection responses on a horizontal scale, and 5 participants viewed horizontally moving dots and indicated their detection responses on a vertical scale. " width="\textwidth" />
<p class="caption">
Figure 3.2: Task design for Experiment 1. In both discrimination and detection blocks, participants viewed 700 milliseconds of a random dot motion array, after which they made a keyboard response to indicate their decision (motion direction in discrimination, signal absence or presence in detection), followed by a continuous confidence report using the mouse. 5 participants viewed vertically moving dots and indicated their detection responses on a horizontal scale, and 5 participants viewed horizontally moving dots and indicated their detection responses on a vertical scale.
</p>
</div>
</div>
</div>
<div id="randomization-1" class="section level3" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Randomization</h3>
<p>The order and timing of experimental events was determined pseudo-randomly by the Mersenne Twister pseudorandom number generator, initialized in a way that ensures registration time-locking <span class="citation">(Mazor, Mazor, &amp; Mukamel, 2019)</span>.</p>
</div>
<div id="analysis" class="section level3" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Analysis</h3>
<p>Experiment 1 was pre-registered (pre-registration document is available here: <a href="https://osf.io/z2s93/">https://osf.io/z2s93/</a>). Our full pre-registered analysis of behavioural data is available in Appendix <a href="D-appRC-everything.html#appRC-everything">D</a>.</p>
<!-- Our pre-registered objectives for this study were to: -->
<!-- 1. Replicate the finding that metacognitive sensitivity for â€˜noâ€™ responses is lower than for â€˜yesâ€™ responses in detection [@meuwese2014subjective; @kanai2010subjective; @kellij2018foundations], and generalize these findings to a different task (determining whether or not some dots moved coherently in a random dot kinematogram; RDK). -->
<!-- 2. Estimate the goodness of fit of an unequal-variance SDT model to perceptual detection data, and compare it to the fit of models that assume a qualitative difference between confidence in absence and confidence in presence. -->
<!-- 3. Replicate the results of @zylberberg2012construction for perceptual discrimination. Namely, show that confidence in a motion discrimination task is mostly influenced by evidence for the selected direction within a short time window around 300 milliseconds after stimulus onset.  -->
<!-- 4. Test the generality of the results of @zylberberg2012construction to perceptual detection, where, by definition, no evidence can be collected to support a â€˜noâ€™ decision. Examine the contribution of signal variance. -->
<div id="reverse-correlation-analysis" class="section level4 unnumbered">
<h4>Reverse correlation analysis</h4>
<p>For the reverse correlation analysis, we followed a procedure similar to the one described in <span class="citation">Zylberberg, Barttfeld, &amp; Sigman (2012)</span>. For each of the four directions (right, left, up and down), we applied two spatiotemporal filters to the frames of the dot motion stimuli as described in previous studies <span class="citation">(Adelson &amp; Bergen, 1985; Zylberberg, Barttfeld, &amp; Sigman, 2012)</span>. The outputs of the two filters were squared and summed, resulting in a three-dimensional matrix with motion energy in a specific direction as a function of x, y, and time. We then took the mean of this matrix across the x and y dimensions to obtain an estimate of the overall temporal fluctuations in motion energy in the selected direction. Additionally, for every time point we extracted the variance along the x and y dimensions, to obtain a measure of temporal fluctuations in spatial variance. Using this filter, we obtained estimates of temporal fluctuations in the mean and variance of motion energy for upward, downward, leftward and rightward motion within each trial. Given a high correlation between our mean and variance estimates, we focused our analysis on the mean motion energy.</p>
<p>In order to distill random fluctuations in motion energy from mean differences between stimulus categories, we subtracted the mean motion energy from trial-specific motion energy vectors. The mean motion energy vectors were extracted at the group level, separately for each motion coherence level and as a function of motion direction. We chose this approach instead of the linear regression approach used by <span class="citation">Zylberberg, Barttfeld, &amp; Sigman (2012)</span> in order to control for nonlinear effects of coherence on motion energy.</p>
</div>
<div id="statistical-inference" class="section level4 unnumbered">
<h4>Statistical inference</h4>
<p>Statistics were extracted separately for each participant, and group-level inference was then performed on the first-order statistics. T-test Bayes factors were used to quantify the evidence for the null when appropriate, using a Jeffrey-Zellner-Siow Prior for the null distribution, with a unit prior scale <span class="citation">(Rouder, Speckman, Sun, Morey, &amp; Iverson, 2009)</span>.</p>
</div>
</div>
<div id="results-4" class="section level3" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> Results</h3>
<div id="response-accuracy" class="section level4" number="3.2.4.1">
<h4><span class="header-section-number">3.2.4.1</span> Response accuracy</h4>
<p>Overall proportion correct was 0.74 in the discrimination and 0.72 in the detection task. Performance for discrimination was significantly higher than for detection (<span class="math inline">\(M_d = 0.02\)</span>, 95% CI <span class="math inline">\([0.00\)</span>, <span class="math inline">\(0.04]\)</span>, <span class="math inline">\(t(9) = 2.43\)</span>, <span class="math inline">\(p = .038\)</span>). This difference in task performance reflected a slower convergence of the staircasing procedure for the discrimination task during the first session. When discarding all data from the first session and analyzing only data from the last three sessions (1800 trials per participant), task performance was equated between the two tasks at the group level (<span class="math inline">\(M_d = 0.00\)</span>, 95% CI <span class="math inline">\([-0.02\)</span>, <span class="math inline">\(0.02]\)</span>, <span class="math inline">\(t(9) = -0.05\)</span>, <span class="math inline">\(p = .962\)</span>; <span class="math inline">\(\mathrm{BF}_{\textrm{01}} = 3.24\)</span>). In order to avoid confounding differences between discrimination and detection decision and confidence profiles with more general task performance effects, the first session was excluded from all subsequent analyses.</p>
</div>
<div id="overall-properties-of-response-time-and-confidence-distributions" class="section level4" number="3.2.4.2">
<h4><span class="header-section-number">3.2.4.2</span> Overall properties of response time and confidence distributions</h4>
<p>In detection, participants were more likely to respond ‘yes’ than ‘no’ (mean proportion of ‘yes’ responses: <span class="math inline">\(M = 0.59\)</span>, 95% CI <span class="math inline">\([0.53\)</span>, <span class="math inline">\(0.64]\)</span>, <span class="math inline">\(t(9) = 3.45\)</span>, <span class="math inline">\(p = .007\)</span>). We did not observe a consistent response bias for the discrimination data (mean proportion of ‘rightward’ or ‘upward’ responses: <span class="math inline">\(M = 0.52\)</span>, 95% CI <span class="math inline">\([0.47\)</span>, <span class="math inline">\(0.57]\)</span>, <span class="math inline">\(t(9) = 1.00\)</span>, <span class="math inline">\(p = .344\)</span>).</p>
<p>Replicating previous studies <span class="citation">(Kellij, Fahrenfort, Lau, Peters, &amp; Odegaard, 2021; Mazor, Friston, &amp; Fleming, 2020; Mazor, Moran, &amp; Fleming, 2021; Meuwese, Loon, Lamme, &amp; Fahrenfort, 2014)</span>, we find the typical asymmetries between detection ‘yes’ and ‘no’ responses in response time, overall confidence, and the alignment between subjective confidence and objective accuracy (also termed metacognitive sensitivity, here measured as the area under the response-conditional type 2 ROC curve; see Fig. <a href="3-ch-RC.html#fig:RC-exp1-asymmetries">3.3</a>). ‘No’ responses were slower compared to ‘yes’ responses (median difference: 85.37 ms), and accompanied by lower levels of subjective confidence (mean difference of 0.08 on a 0-1 scale). Metacognitive sensitivity was higher for detection ‘yes’ compared with detection ‘no’ responses (mean difference in area under the curve units: 0.11). No difference in response time, confidence, or metacognitive sensitivity was found between the two discrimination responses. For a detailed statistical analysis of these behavioural asymmetries see Appendix <a href="D-appRC-everything.html#appRC-asymmetries1">D.1.1</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RC-exp1-asymmetries"></span>
<img src="figure/RC/RC-exp1-asymmetries-enhanced.png" alt="Behavioural asymmetries in metacognitive sensitivity, response time, and overall confidence, in Exp. 1. Top row: Response conditional type 2 ROC curves for the two tasks and four responses in Exp. 1. The area under the type 2 ROC curve is a measure of metacognitive sensitivity, and the difference in areas between the two responses a measure of metacognitive asymmetry. Single-subject curves are presented in low opacity. Second, third, and fourth rows: distributions of the area under the type 2 ROC curve, median response time, and mean confidence for the four responses, across participants. Box edges and central lines represent the 25, 50 and 75 quantiles. Whiskers cover data points within four inter-quartile ranges around the median. Stars represent significance in a two-sided t-test: **: p&lt;0.01, ***: p&lt;0.001" width="\textwidth" />
<p class="caption">
Figure 3.3: Behavioural asymmetries in metacognitive sensitivity, response time, and overall confidence, in Exp. 1. Top row: Response conditional type 2 ROC curves for the two tasks and four responses in Exp. 1. The area under the type 2 ROC curve is a measure of metacognitive sensitivity, and the difference in areas between the two responses a measure of metacognitive asymmetry. Single-subject curves are presented in low opacity. Second, third, and fourth rows: distributions of the area under the type 2 ROC curve, median response time, and mean confidence for the four responses, across participants. Box edges and central lines represent the 25, 50 and 75 quantiles. Whiskers cover data points within four inter-quartile ranges around the median. Stars represent significance in a two-sided t-test: **: p&lt;0.01, ***: p&lt;0.001
</p>
</div>
<!-- #### zROC curves -->
</div>
<div id="reverse-correlation" class="section level4" number="3.2.4.3">
<h4><span class="header-section-number">3.2.4.3</span> Reverse Correlation</h4>
<p>Random fluctuations in motion energy made it possible to apply reverse correlation to test which stimulus features are incorporated into decisions and confidence ratings in detection and discrimination. Following <span class="citation">Zylberberg, Barttfeld, &amp; Sigman (2012)</span>, our statistical analysis focused on the first 300 milliseconds after stimulus onset.</p>
<div id="e1-disc-RC" class="section level5 unnumbered">
<h5>Discrimination</h5>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RC-exp1-discrimination-RC"></span>
<img src="figure/RC/RC-exp1-discrimination-RC-enhanced.png" alt="Decision and confidence discrimination kernels, Experiment 1. Upper left: motion energy in the chosen (green) and unchosen (purple) direction as a function of time. Lower left: a subtraction between energy in the chosen and unchosen directions. Upper right: confidence effects for motion energy in the chosen (green) and unchosen (purple) directions. Lower right: a subtraction between confidence effects in the chosen and unchosen directions. Shaded areas represent the the mean \(\pm\) one standard error. The first 300 milliseconds of the trial are marked in yellow. Stars represent significance in a two-sided t-test: *: p&lt;0.05, **: p&lt;0.01, ***: p&lt;0.001. In the upper row, stars represent the significance of a positive evidence bias in evidence weighting." width="\textwidth" />
<p class="caption">
Figure 3.4: Decision and confidence discrimination kernels, Experiment 1. Upper left: motion energy in the chosen (green) and unchosen (purple) direction as a function of time. Lower left: a subtraction between energy in the chosen and unchosen directions. Upper right: confidence effects for motion energy in the chosen (green) and unchosen (purple) directions. Lower right: a subtraction between confidence effects in the chosen and unchosen directions. Shaded areas represent the the mean <span class="math inline">\(\pm\)</span> one standard error. The first 300 milliseconds of the trial are marked in yellow. Stars represent significance in a two-sided t-test: *: p&lt;0.05, **: p&lt;0.01, ***: p&lt;0.001. In the upper row, stars represent the significance of a positive evidence bias in evidence weighting.
</p>
</div>
<p>Reverse correlation analysis quantified the effect of random fluctuations in motion energy on the probability of responding ‘right’ and ‘left’ (or ‘up’ and ‘down’), and the temporal dynamics of decision formation. Similar to the results obtained by <span class="citation">Zylberberg, Barttfeld, &amp; Sigman (2012)</span>., participants’ decisions were sensitive to motion energy fluctuations during the first 300 milliseconds of the trial (<span class="math inline">\(t(9) = 7.73\)</span>, <span class="math inline">\(p &lt; .001\)</span>; see Fig. <a href="3-ch-RC.html#fig:RC-exp1-discrimination-RC">3.4</a>, left panels). The symmetry of the two time courses around the x axis does not by itself entail an equal contribution of negative and positive evidence to the final decision, because due to the demeaning procedure, with enough trials negative and positive evidence at each time point should mathematically sum to zero. Instead, we tested the contribution of motion energy in the true and opposite directions of motion (defined with respect to the stimulus, and independently of decision) to discrimination decisions. Fluctuations in motion energy in both directions contributed significantly to discrimination decisions (<span class="math inline">\(t(9) = 8.38\)</span>, <span class="math inline">\(p &lt; .001\)</span>), with no significant difference between them (<span class="math inline">\(t(9) = -0.65\)</span>, <span class="math inline">\(p = .529\)</span>). In other words, positive and negative evidence equally contributed to discrimination decisions, even when defined independently of the decision.</p>
<p>We then turned to the contribution of motion energy to subjective confidence ratings. The median confidence rating in each experimental session was used to split all motion energy vectors into four groups, according to decision (chosen or unchosen directions) and confidence level (high or low). Confidence kernels for the chosen and unchosen directions were then extracted by subtracting the mean low confidence vectors from the mean high confidence vectors for both the chosen and unchosen directions. We observed a significant effect of motion energy on confidence within this time window (<span class="math inline">\(t(19) = 2.52\)</span>, <span class="math inline">\(p = .021\)</span>; see Fig. <a href="3-ch-RC.html#fig:RC-exp1-discrimination-RC">3.4</a>, right panels). This effect was significantly stronger for motion energy in the chosen direction, compared to the unchosen direction (<span class="math inline">\(t(9) = 2.81\)</span>, <span class="math inline">\(p = .020\)</span>). In other words, confidence ratings in the discrimination task were more sensitive to positive evidence than to negative evidence. This is a replication of the Positive Evidence Bias observed in <span class="citation">Zylberberg, Barttfeld, &amp; Sigman (2012)</span>.</p>
</div>
<div id="detection" class="section level5 unnumbered">
<h5>Detection</h5>
<p>Reverse correlation analysis for detection introduces a challenge: while ‘no’ responses reflect a belief in the absence of any coherent motion, ‘yes’ responses can result from detection of any type of coherent motion going in either direction (or both). We chose to have two possible motion directions in the detection task in order to prevent participants from making ‘no’ responses based on significant motion in an unexpected direction. While this choice ensured that participants cannot trivially accumulate evidence for absence, it also made the reverse correlation analysis more difficult, as we did not have full access to participants’ beliefs about the stimulus when they responded ‘yes.’</p>
<p>As a first approximation, we tested whether sum motion energy along the relevant dimension (horizontal or vertical), regardless of direction (up/down or left/right), affected the probability of a ‘yes’ response. Sum motion energy did not have a significant effect on participants’ responses during the first 300 milliseconds (<span class="math inline">\(t(9) = 1.23\)</span>, <span class="math inline">\(p = .249\)</span>) or at any other time point. The effect of sum motion energy on decision confidence during the first 300 milliseconds was positive and marginally significant (<span class="math inline">\(t(9) = 2.15\)</span>, <span class="math inline">\(p = .060\)</span>). Response-specific effects of sum motion energy on decision confidence were not significant for either response.</p>
</div>
</div>
<div id="detection-signal-trials" class="section level4 unnumbered">
<h4>Detection signal trials</h4>
<p>A failure to find significant effects of sum motion energy on detection decisions and confidence may be due to the fact that participants were sensitive to relative evidence (e.g., ‘more dots are moving to the right than to the left’) rather than to the sum motion along the relevant axis (‘many dots are moving to the right and to the left’). However, as we mention above, on any single trial, we cannot tell whether a ‘yes’ response means ‘I perceived coherent motion to the right’ or ‘I perceived coherent motion to the left.’ Instead, in order to approximate participants’ belief states during ‘yes’ responses, we focused only on trials in which coherent motion was presented in one of the two directions (signal trials). In these trials, we reasoned that a ‘yes’ response is most likely to reflect the detection of the true direction of motion. We therefore asked whether fluctuations in the true and opposite directions of motion contributed to detection decision and confidence. This was done by subtracting the motion energy vectors for ‘yes’ and ‘no’ responses in the true and opposite motion directions.</p>
<p>Similar to discrimination decisions, detection decisions were most sensitive to perceptual evidence in the first 300 milliseconds of the trial (see Fig. <a href="3-ch-RC.html#fig:RC-exp1-signal-figure">3.5</a>, left panels). However, in contrast to discrimination, an asymmetric evidence weighting was apparent in the decision itself: when deciding whether a stimulus contained coherent motion, participants were more sensitive to fluctuations in motion energy that strengthened the true direction of motion, in comparison to fluctuations that weakened motion in the opposite direction (<span class="math inline">\(t(9) = 2.31\)</span>, <span class="math inline">\(p = .046\)</span>).</p>
<!-- We note again that the apparent symmetry in the discrimination decision kernel is an artefact of the analysis method. In the discrimination analysis, 'chosen' and 'unchosen' vectors mirror each other in the limit, because they must sum to zero. This is not true for this analysis, where we subtract motion energy for 'yes' and 'no' responses within each of the two vectors.  -->
<p>Motion fluctuations in the first 300 milliseconds of the trial also contributed to confidence in detection ‘yes’ responses (contrasting high and low confidence hit trials; <span class="math inline">\(t(9) = 6.13\)</span>, <span class="math inline">\(p &lt; .001\)</span>). But unlike in the discrimination task here we found no positive evidence bias in confidence ratings (<span class="math inline">\(t(9) = 0.11\)</span>, <span class="math inline">\(p = .913\)</span>). To reiterate, while detection decisions were mostly sensitive to fluctuations in motion energy toward the true direction of motion, confidence in detection ‘yes’ responses was equally sensitive to fluctuations in the true and opposite directions of motion.
Confidence in ‘miss’ trials was independent of motion energy (<span class="math inline">\(t(9) = 0.16\)</span>, <span class="math inline">\(p = .874\)</span>). This was true for motion energy in the true direction of motion (<span class="math inline">\(t(9) = 0.12\)</span>, <span class="math inline">\(p = .908\)</span>) as well as for motion energy in the opposite direction (<span class="math inline">\(t(9) = -0.08\)</span>, <span class="math inline">\(p = .941\)</span>). However, and to anticipate the results of Exp. 3 presented below, we note that this equal weighting of positive and negative evidence in detection confidence was not replicated in an experiment designed to directly test this surprising result with an experimental manipulation.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RC-exp1-signal-figure"></span>
<img src="figure/RC/RC-exp1-signal-RC-enhanced.png" alt="Decision and confidence detection kernels in signal trials, Experiment 1. Upper left: difference in motion energy between ‘yes’ and ‘no’ responses in the true (blue) and opposite (red) directions as a function of time. Upper middle and right: confidence effects for motion energy in the true and opposite directions for ‘yes’ and ‘no’ responses, respectively. Lower panels: the subtraction of decision and confidence kernels for the true and opposite directions. Shaded areas represent the the mean \(\pm\) one standard error. The first 300 milliseconds of the trial are marked in yellow. Stars represent significance in a two-sided t-test: *: p&lt;0.05, **: p&lt;0.01, ***: p&lt;0.001. In the upper row, stars represent the significance of a positive evidence bias in evidence weighting." width="\textwidth" />
<p class="caption">
Figure 3.5: Decision and confidence detection kernels in signal trials, Experiment 1. Upper left: difference in motion energy between ‘yes’ and ‘no’ responses in the true (blue) and opposite (red) directions as a function of time. Upper middle and right: confidence effects for motion energy in the true and opposite directions for ‘yes’ and ‘no’ responses, respectively. Lower panels: the subtraction of decision and confidence kernels for the true and opposite directions. Shaded areas represent the the mean <span class="math inline">\(\pm\)</span> one standard error. The first 300 milliseconds of the trial are marked in yellow. Stars represent significance in a two-sided t-test: *: p&lt;0.05, **: p&lt;0.01, ***: p&lt;0.001. In the upper row, stars represent the significance of a positive evidence bias in evidence weighting.
</p>
</div>
<!-- #### Perceptual sample analysis {-} -->
<p><!-- According to  --></p>
</div>
</div>
</div>
<div id="experiment-2-1" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Experiment 2</h2>
<p>In Exp. 1, we replicated previous observations of a positive evidence bias in discrimination confidence, such that evidence in support of a decision was given more weight in the construction of confidence than evidence against it. In contrast, in detection a positive evidence bias was apparent for the decision, but not for the confidence kernels. Equal weighting of positive and negative evidence suggests that detection confidence followed not the presence or absence of a signal, but the clarity of its identity. Furthermore, confidence in detection ‘no’ responses was not at all affected by fluctuations in motion energy.</p>
<p>In Exp. 2 we tested the robustness of these findings by employing a different type of stimuli (flickering patches) and mode of data collection (a ~10 minute online experiment). Our pre-registered objectives (documented here: <a href="https://osf.io/8u7dk/">https://osf.io/8u7dk/</a>) were to first, replicate a positive evidence bias in discrimination, second, replicate the absence of a positive evidence bias in detection confidence ratings, and third, replicate the absence of an effect for either positive or negative evidence on confidence in ‘no’ judgments.</p>
<div id="methods-1" class="section level3" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Methods</h3>
<div id="participants-5" class="section level4" number="3.3.1.1">
<h4><span class="header-section-number">3.3.1.1</span> Participants</h4>
<p>The research complied with all relevant ethical regulations, and was approved by the Research Ethics Committee of University College London (study ID number 1260/003). 147 participants (median reported age: 32; range: [19-78]) were recruited via Prolific (prolific.co), and gave their informed consent prior to their participation. They were selected based on their acceptance rate (&gt;95%) and for being native English speakers. Following our pre-registration, we aimed to collect data until we had reached 100 included participants based on our pre-specified inclusion criteria (see <a href="https://osf.io/8u7dk/">https://osf.io/8u7dk/</a>). Our final data set includes observations from 102 included participants. The entire experiment took around 10 minutes to complete. Participants were paid £1.25 for their participation, equivalent to an hourly wage of £7.5.</p>
</div>
<div id="experimental-paradigm" class="section level4" number="3.3.1.2">
<h4><span class="header-section-number">3.3.1.2</span> Experimental paradigm</h4>
<p>The experiment was programmed using the jsPsych and P5 JavaScript packages <span class="citation">(De Leeuw, 2015; McCarthy, 2015)</span>, and was hosted on a JATOS server <span class="citation">(Lange, Kuhn, &amp; Filevich, 2015)</span>. It consisted of two tasks (Detection and Discrimination) presented in separate blocks. A total of 56 trials of each task were delivered in 2 blocks of 28 trials each. The order of experimental blocks was interleaved, starting with discrimination.</p>
<p>The first discrimination block started after an instruction section, which included instructions about the stimuli and confidence scale, four practice trials and four confidence practice trials. Further instructions were presented before the second block. Instruction sections were followed by multiple-choice comprehension questions, to monitor participants’ understanding of the main task and confidence reporting interface. To encourage concentration, feedback was delivered at the end of the second and fourth blocks about overall performance and mean confidence in the task.</p>
<p>Importantly, unlike the lab-based experiment, there was no calibration of difficulty for the two tasks. The rationale for this is that in Exp. 1 perceptual thresholds for motion discrimination were highly consistent across participants, and staircasing took a long time to converge. Furthermore, in Exp. 1 we aimed to control for task difficulty, but this introduced differences between the stimulus intensity in detection and discrimination. To complement our findings, here we aimed to match stimulus intensity between the two tasks, and accept that task performance might vary.</p>
<div id="trial-structure" class="section level5 unnumbered">
<h5>Trial structure</h5>
<p>In discrimination blocks, trial structure closely followed Exp. 2 from <span class="citation">Zylberberg, Barttfeld, &amp; Sigman (2012)</span>, with a few adaptations. Following a fixation cross (500 ms), two sets of four adjacent vertical gray bars were presented as a rapid serial visual presentation (RSVP; 12 frames, presented at 25Hz), displayed to the left and right of the fixation cross (see Fig. <a href="3-ch-RC.html#fig:RC-exp2-design">3.6</a>). On each frame, the luminance of the bars was randomly sampled from a Gaussian distribution with a standard deviation of 10/255 units in the standard RGB 0-255 coordinate system. The average luminance of one set of bars was that of the background (128/255). The average luminance of the other set was 133/255, making this patch brighter on average. Participants then reported which of the two sets was brighter on average using the ‘D’ and ‘F’ keys on the keyboard. After their response, they rated their confidence on a continuous scale, by controlling the size of a colored circle with their mouse. High confidence was mapped to a big, blue circle, and low confidence to a small, red circle. To discourage hasty confidence ratings, the confidence rating scale stayed on the screen for at least 2000 milliseconds. Feedback about response accuracy was delivered after the confidence rating phase.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RC-exp2-design"></span>
<img src="figure/RC/designExp2.png" alt="Task design for Experiment 2. In both tasks, participants viewed 480 milliseconds of two flicketing patches, after which they made a keyboard response to indicate which of the patches was brighter (discrimination) or whether any of the patches was brighter than the background (detection). " width="\textwidth" />
<p class="caption">
Figure 3.6: Task design for Experiment 2. In both tasks, participants viewed 480 milliseconds of two flicketing patches, after which they made a keyboard response to indicate which of the patches was brighter (discrimination) or whether any of the patches was brighter than the background (detection).
</p>
</div>
<p>Detection blocks were similar to discrimination blocks, with the exception that decisions were made about whether the average luminance of either of the two sets was brighter than the gray background, or not. In ‘different’ trials, the luminance of the four bars in one of the sets was sampled from a Gaussian distribution with mean 133/255, and the luminance of the other set from a Gaussian distribution with mean 128/255. In ‘same’ trials, the luminance of both sets was sampled from a distribution centered at 128/255. Decisions in Detection trials were reported using the ‘Y’ and ‘N’ keys. Confidence ratings and feedback were as in the discrimination task.</p>
</div>
</div>
</div>
<div id="randomization-2" class="section level3" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Randomization</h3>
<p>The order and timing of experimental events was determined pseudo-randomly by the Mersenne Twister pseudorandom number generator, initialized in a way that ensures registration time-locking <span class="citation">(Mazor, Mazor, &amp; Mukamel, 2019)</span>.</p>
</div>
<div id="results-5" class="section level3" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Results</h3>
<div id="response-accuracy-1" class="section level4" number="3.3.3.1">
<h4><span class="header-section-number">3.3.3.1</span> Response accuracy</h4>
<p>Overall proportion correct was 0.85 in the discrimination and 0.67 in the detection task. Performance for discrimination was significantly higher than for detection (<span class="math inline">\(M_d = 0.18\)</span>, 95% CI <span class="math inline">\([0.16\)</span>, <span class="math inline">\(0.20]\)</span>, <span class="math inline">\(t(101) = 18.01\)</span>, <span class="math inline">\(p &lt; .001\)</span>). Unlike in Exp. 1, where we aimed to control for task difficulty, here we decided to match stimulus intensity between the two tasks, so a difference between detection and discrimination performance was expected <span class="citation">(Wickens, 2002, p. 104)</span>.</p>
</div>
<div id="overall-properties-of-response-and-confidence-distributions" class="section level4" number="3.3.3.2">
<h4><span class="header-section-number">3.3.3.2</span> Overall properties of response and confidence distributions</h4>
<p>Similar to Exp. 1, participants were more likely to respond ‘yes’ than ‘no’ in the detection task (mean proportion of ‘yes’ responses: <span class="math inline">\(M = 0.54\)</span>, 95% CI <span class="math inline">\([0.53\)</span>, <span class="math inline">\(0.56]\)</span>, <span class="math inline">\(t(101) = 4.78\)</span>, <span class="math inline">\(p &lt; .001\)</span>). We did not observe a consistent response bias in discrimination (mean proportion of ‘right’ responses: <span class="math inline">\(M = 0.50\)</span>, 95% CI <span class="math inline">\([0.48\)</span>, <span class="math inline">\(0.51]\)</span>, <span class="math inline">\(t(101) = -0.62\)</span>, <span class="math inline">\(p = .537\)</span>).</p>
<p>As in Exp. 1, we also found behavioural asymmetries between the two detection responses (see Fig. <a href="3-ch-RC.html#fig:RC-exp2-asymmetries">3.7</a>), with ‘yes’ responses being faster (median difference of 77.12 ms) and accompanied by higher levels of confidence (mean difference of 0.10 on a 0-1 scale). Unlike in Exp. 1, here we found no evidence for a difference in metacognitive sensitivity between ‘yes’ and ‘no’ responses (mean difference of 0.02 in AUC units). No asymmetries were observed between the two discrimination responses. For a detailed statistical analysis see Appendix <a href="D-appRC-everything.html#appRC-asymmetries2">D.2.1</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RC-exp2-asymmetries"></span>
<img src="figure/RC/RC-exp2-asymmetries-enhanced.png" alt="Behavioural asymmetries in metacognitive sensitivity, response time, and overall confidence, in Exp. 2. Same conventions as in Fig. 3.3." width="\textwidth" />
<p class="caption">
Figure 3.7: Behavioural asymmetries in metacognitive sensitivity, response time, and overall confidence, in Exp. 2. Same conventions as in Fig. <a href="3-ch-RC.html#fig:RC-exp1-asymmetries">3.3</a>.
</p>
</div>
</div>
<div id="reverse-correlation-1" class="section level4" number="3.3.3.3">
<h4><span class="header-section-number">3.3.3.3</span> Reverse Correlation</h4>
<p>Stimuli in Exp. 2 consisted of two flickering patches, each comprising 4 gray bars presented for 12 frames. Together, this summed to 96 random luminance values per trial, which we subjected to reverse correlation analysis, following the analysis procedure of Exp 2. in <span class="citation">Zylberberg, Barttfeld, &amp; Sigman (2012)</span>.</p>
<div id="discrimination-decisions" class="section level5 unnumbered">
<h5>Discrimination decisions</h5>
<p>First, we asked whether random fluctuations in luminance had an effect on participants’ discrimination responses. Similar to the results obtained by Zylberberg et. al., discrimination decisions were sensitive to fluctuations in luminance during the first 300 milliseconds of the trial (<span class="math inline">\(t(101) = 10.98\)</span>, <span class="math inline">\(p &lt; .001\)</span>; see Fig. <a href="3-ch-RC.html#fig:RC-exp2-discrimination-RC">3.8</a>, left panels). As per our approach to the reverse correlation analysis of Exp. 1, in order to test for decision biases we need to divide evidence based on a criterion that is independent of participants’ decision. When sorting evidence based on the location of the true signal, participants’ decisions were significantly more sensitive to fluctuations in luminance in the non-signal compared with the signal stimulus within the first 300 milliseconds of the trial (<span class="math inline">\(t(100) = -2.29\)</span>, <span class="math inline">\(p = .024\)</span>). Importantly, this asymmetry (effectively, a negative evidence bias) is in the opposite direction to what we later find in discrimination confidence and detection decisions.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RC-exp2-discrimination-RC"></span>
<img src="figure/RC/RC-exp2-discrimination-RC-enhanced.png" alt="Decision and confidence discrimination kernels, Experiment 2. Upper panels: decision (left) and confidence (right) kernels for the flickering patch stimuli. Black frames signify a significant effect at the 0.05 significance level controlling for family-wise error rate across the 48 (12 timepoint x 4 positions) comparisons. middle panels: decision and confidence kernels, averaged across the four bars to yield a single timecourse for the chosen (green) and unchosen (purple) stimuli. Lower panels: subtraction of luminance timecourses for the chosen and unchosen stimuli. Same plotting conventions as Fig. 3.4." width="\textwidth" />
<p class="caption">
Figure 3.8: Decision and confidence discrimination kernels, Experiment 2. Upper panels: decision (left) and confidence (right) kernels for the flickering patch stimuli. Black frames signify a significant effect at the 0.05 significance level controlling for family-wise error rate across the 48 (12 timepoint x 4 positions) comparisons. middle panels: decision and confidence kernels, averaged across the four bars to yield a single timecourse for the chosen (green) and unchosen (purple) stimuli. Lower panels: subtraction of luminance timecourses for the chosen and unchosen stimuli. Same plotting conventions as Fig. <a href="3-ch-RC.html#fig:RC-exp1-discrimination-RC">3.4</a>.
</p>
</div>
</div>
<div id="discrimination-confidence" class="section level5 unnumbered">
<h5>Discrimination confidence</h5>
<p>We observed a significant effect of luminance on confidence within the first 300 milliseconds of the stimulus (<span class="math inline">\(t(100) = 7.14\)</span>, <span class="math inline">\(p &lt; .001\)</span>; see Fig. <a href="3-ch-RC.html#fig:RC-exp2-discrimination-RC">3.8</a>, right panels). Replicating <span class="citation">Zylberberg, Barttfeld, &amp; Sigman (2012)</span>, this effect was significantly stronger for luminance in the chosen stimulus, compared to the unchosen one (<span class="math inline">\(t(100) = 2.56\)</span>, <span class="math inline">\(p = .012\)</span>), consistent with a positive evidence bias.</p>
</div>
<div id="e2-det-RC" class="section level5 unnumbered">
<h5>Detection decisions</h5>
<!-- To make a detection decision, participants could either compare the luminance of the right and left stimuli and respond 'yes' if they thought one was brighter than the other, or alternatively pool luminance values from both stimuli and respond 'yes' if the average stimulus was brighter than the background. Participants who employ the second strategy should be more likely to respond 'yes' when both stimuli are equally bright, but are brighter than the background. The second strategy however predicts that overall luminance should have no effect on detection decision.  -->
<p>We pooled luminance values from both right and left stimuli and contrasted the resulting values as a function of detection response. Sum luminance had a significant effect on participants’ detection responses during the first 300 milliseconds (<span class="math inline">\(t(101) = 6.10\)</span>, <span class="math inline">\(p &lt; .001\)</span>; see Fig. <a href="3-ch-RC.html#fig:RC-exp2-detection-RC-figure">3.9</a>, left panel), suggesting that participants were sensitive to sum evidence in their detection responses, as expected from a model in which detection is rationally based on the likelihood ratio between signal presence and absence (see Fig. <a href="3-ch-RC.html#fig:RC-2dmodel">3.1</a>).</p>
<p>We then asked if overall luminance had an effect on decision confidence, such that participants are more confident in their ‘yes’ responses for brighter displays, and more confident in their ‘no’ responses for darker displays. Interestingly, and in contrast with our hypothesis, sum luminance had no effect on decision confidence in ‘yes’ responses (<span class="math inline">\(t(99) = -0.02\)</span>, <span class="math inline">\(p = .983\)</span>), but had a significant negative effect on confidence in ‘no’ responses (<span class="math inline">\(t(99) = -2.43\)</span>, <span class="math inline">\(p = .017\)</span>; see Fig. <a href="3-ch-RC.html#fig:RC-exp2-detection-RC-figure">3.9</a>, middle and right panels). However, to again anticipate our pre-registered Exp. 3, we find an effect of sum luminance on both ‘yes’ and ‘no’ responses, suggesting that this surprising absence of an effect for ‘yes’ responses is likely to be a type-2 error.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RC-exp2-detection-RC-figure"></span>
<img src="figure/RC/RC-exp2-detection-RC-enhanced.png" alt="Decision and confidence detection kernels, Experiment 2. Upper panels: decision (left) and confidence (right) kernels for the flickering patch stimuli, showing the effect of sum evidence (sum luminance across both stimuli) on decisions and confidence. Black frames signify a significant effect at the 0.05 significance level controlling for family-wise error rate across the 48 (12 timepoint x 4 positions) comparisons. Lower panels: decision and confidence kernels, averaged across the four bars to yield a single timecourse for the difference in luminance effects in ‘yes’ and ‘no’ responses. Same conventions as in Fig. 3.8." width="\textwidth" />
<p class="caption">
Figure 3.9: Decision and confidence detection kernels, Experiment 2. Upper panels: decision (left) and confidence (right) kernels for the flickering patch stimuli, showing the effect of sum evidence (sum luminance across both stimuli) on decisions and confidence. Black frames signify a significant effect at the 0.05 significance level controlling for family-wise error rate across the 48 (12 timepoint x 4 positions) comparisons. Lower panels: decision and confidence kernels, averaged across the four bars to yield a single timecourse for the difference in luminance effects in ‘yes’ and ‘no’ responses. Same conventions as in Fig. <a href="3-ch-RC.html#fig:RC-exp2-discrimination-RC">3.8</a>.
</p>
</div>
</div>
</div>
</div>
<div id="detection-signal-trials-1" class="section level3" number="3.3.4">
<h3><span class="header-section-number">3.3.4</span> Detection signal trials</h3>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RC-exp2-signal-RC"></span>
<img src="figure/RC/RC-exp2-signal-RC-enhanced.png" alt="Decision and confidence kernels for detection signal trials, Experiment 2. Upper left: mean difference in luminance between ‘yes’ and ‘no’ responses for the target and non-target stimuli. Upper middle and right panels: mean effect of luminance on confidence in the target and non-target stimuli, in ‘yes’ and ‘no’ responses. Middle panels: the effects of luminance on decision and confidence, averaged across the four spatial locations. Lower panels: a subtraction between the effects of luminance in the target and non-target stimuli. Same conventions as Fig. 3.8" width="\textwidth" />
<p class="caption">
Figure 3.10: Decision and confidence kernels for detection signal trials, Experiment 2. Upper left: mean difference in luminance between ‘yes’ and ‘no’ responses for the target and non-target stimuli. Upper middle and right panels: mean effect of luminance on confidence in the target and non-target stimuli, in ‘yes’ and ‘no’ responses. Middle panels: the effects of luminance on decision and confidence, averaged across the four spatial locations. Lower panels: a subtraction between the effects of luminance in the target and non-target stimuli. Same conventions as Fig. <a href="3-ch-RC.html#fig:RC-exp2-discrimination-RC">3.8</a>
</p>
</div>
<p>We next focused on detection signal trials. This analysis diverged from our pre-registered plan; for the pre-registered analysis, please see Appendix section <a href="D-appRC-everything.html#appRC-PDRC">D.4</a>. In these trials, we could separate stimuli to a signal channel (the brighter, target stimulus) and a noise channel (the darker, non-target stimulus), and ask how random variability in luminance in each channel affected detection decisions and confidence. As in Exp. 1, a positive evidence bias effect in detection was apparent in the decision itself: when deciding whether one of the flickering patches was brighter, participants were more sensitive to positive noise in the brighter patch than to negative noise in the darker patch (<span class="math inline">\(t(101) = 6.10\)</span>, <span class="math inline">\(p &lt; .001\)</span>). Random fluctuations in luminance in the first 300 milliseconds of the trial also contributed to confidence in detection ‘yes’ responses (hit trials; <span class="math inline">\(t(99) = 5.08\)</span>, <span class="math inline">\(p &lt; .001\)</span>). In contrast, confidence in ‘no’ responses was negatively sensitive to the overall luminance of the display. A negative effect of luminance on confidence in ‘no’ responses was significant for the non-target stimulus (<span class="math inline">\(t(98) = -2.64\)</span>, <span class="math inline">\(p = .010\)</span>), and marginally significant for the target stimulus (<span class="math inline">\(t(98) = -1.67\)</span>, <span class="math inline">\(p = .099\)</span>). Consistent with the results of Exp. 1, confidence in ‘miss’ trials was independent of the contrast in luminance between the right and left stimuli (<span class="math inline">\(t(98) = 1.26\)</span>, <span class="math inline">\(p = .210\)</span>). Importantly, for both stimuli higher confidence in these trials was associated with lower luminance values, in line with our observation that confidence in detection ‘no’ responses was based on the overall darkness of the display, rather than on relative evidence. Finally, and similar to the results of Exp. 1, detection confidence was not susceptible to a positive evidence bias (<span class="math inline">\(t(99) = -0.12\)</span>, <span class="math inline">\(p = .901\)</span>). Exp. 3 below was designed to replicate this surprising symmetric weighting of positive and negative evidence in detection confidence (the absence of a positive evidence bias) in a highly-powered design.</p>
</div>
</div>
<div id="experiment-3" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Experiment 3</h2>
<p>In Exp. 3 we aimed to replicate our findings using an experimental manipulation, in addition to employing reverse-correlation analysis to random variations between stimuli. Our pre-registered objectives (see our pre-registration document: <a href="https://osf.io/hm3fn/">https://osf.io/hm3fn/</a>) were to first, replicate a positive evidence bias in discrimination, second, replicate a positive evidence bias in detection decisions, and third, replicate the absence of a positive evidence bias in detection confidence.</p>
<div id="methods-2" class="section level3" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Methods</h3>
<div id="participants-6" class="section level4" number="3.4.1.1">
<h4><span class="header-section-number">3.4.1.1</span> Participants</h4>
<p>The research complied with all relevant ethical regulations, and was approved by the Research Ethics Committee of University College London (study ID number 1260/003). 173 participants (median reported age: 31; range: [18-71]) were recruited via Prolific (prolific.co), and gave their informed consent prior to their participation. They were selected based on their acceptance rate (&gt;95%) and for being native English speakers. Following our pre-registration, we aimed to collect data until we had reached 100 included participants based on our pre-specified inclusion criteria (see <a href="https://osf.io/hm3fn/">https://osf.io/hm3fn/</a>). Our final data set includes observations from 100 included participants. The entire experiment took around 20 minutes to complete. Participants were paid £2.5 for their participation, equivalent to an hourly wage of £7.5.</p>
</div>
<div id="experimental-paradigm-1" class="section level4" number="3.4.1.2">
<h4><span class="header-section-number">3.4.1.2</span> Experimental paradigm</h4>
<p>Experiment 3 was identical to Experiment 2 with two changes. First, on half of the trials (<em>high-luminance</em> trials) the luminance of both sets of bars was increased by 2/255 for the entire duration of the display. Second, in order to increase our statistical power for detecting response-specific effects in detection, participants performed four detection blocks and two discrimination blocks. Each block comprised 56 trials. The order of blocks was [detection, discrimination, detection, discrimination, detection, detection] for all participants.</p>
</div>
</div>
<div id="results-6" class="section level3" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Results</h3>
<div id="response-accuracy-2" class="section level4" number="3.4.2.1">
<h4><span class="header-section-number">3.4.2.1</span> Response accuracy</h4>
<p>Overall proportion correct was 0.88 in the discrimination and 0.67 in the detection task. Performance for discrimination was significantly higher than for detection (<span class="math inline">\(M_d = 0.21\)</span>, 95% CI <span class="math inline">\([0.19\)</span>, <span class="math inline">\(0.22]\)</span>, <span class="math inline">\(t(97) = 29.87\)</span>, <span class="math inline">\(p &lt; .001\)</span>), as expected.</p>
</div>
<div id="overall-properties-of-response-and-confidence-distributions-1" class="section level4" number="3.4.2.2">
<h4><span class="header-section-number">3.4.2.2</span> Overall properties of response and confidence distributions</h4>
<p>Similar to Experiments 1 and 2, participants were more likely to respond ‘yes’ than ‘no’ in the detection task (mean proportion of ‘yes’ responses: <span class="math inline">\(M = 0.53\)</span>, 95% CI <span class="math inline">\([0.51\)</span>, <span class="math inline">\(0.54]\)</span>, <span class="math inline">\(t(97) = 3.73\)</span>, <span class="math inline">\(p &lt; .001\)</span>). We did not observe a consistent response bias in discrimination (mean proportion of ‘right’ responses: <span class="math inline">\(M = 0.50\)</span>, 95% CI <span class="math inline">\([0.48\)</span>, <span class="math inline">\(0.53]\)</span>, <span class="math inline">\(t(97) = 0.46\)</span>, <span class="math inline">\(p = .647\)</span>).</p>
<p>As in both Experiments 1 and 2, we found behavioural asymmetries between the two detection responses, with ‘yes’ responses being faster (median difference of 71.81 ms), and accompanied by higher levels of confidence (mean difference of 0.09 on a 0-1 scale). As in Exp. 1, we find a difference in metacognitive sensitivity between ‘yes’ and ‘no’ responses (mean difference of 0.03 in AUC units). No asymmetries were observed between the two discrimination responses. For a detailed statistical analysis see Appendix <a href="D-appRC-everything.html#appRC-asymmetries3">D.3.1</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RC-exp3-asymmetries"></span>
<img src="figure/RC/RC-exp3-asymmetries-enhanced.png" alt="Behavioural asymmetries in metacognitive sensitivity, response time, and overall confidence, in Exp. 3. Same conventions as in Fig. 3.3." width="\textwidth" />
<p class="caption">
Figure 3.11: Behavioural asymmetries in metacognitive sensitivity, response time, and overall confidence, in Exp. 3. Same conventions as in Fig. <a href="3-ch-RC.html#fig:RC-exp1-asymmetries">3.3</a>.
</p>
</div>
</div>
<div id="reverse-correlation-2" class="section level4" number="3.4.2.3">
<h4><span class="header-section-number">3.4.2.3</span> Reverse correlation</h4>
<div id="discrimination-decisions-1" class="section level5 unnumbered">
<h5>Discrimination decisions</h5>
<p>We first focused on reverse correlation analyses that collapsed across high-luminance and standard trials, in order to replicate the same approach used in Exps. 1 and 2. When focusing on standard trials only, the results are qualitatively similar, with the exception of confidence in detection ‘no’ responses (see Appendix <a href="D-appRC-everything.html#appRC-standardonly">D.3.2</a>). Discrimination decisions were sensitive to fluctuations in luminance during the first 300 milliseconds of the trial (<span class="math inline">\(t(97) = 12.01\)</span>, <span class="math inline">\(p &lt; .001\)</span>; see Fig. <a href="3-ch-RC.html#fig:RC-exp3-discrimination-RC">3.12</a>, left panels). We found no evidence for a positive evidence bias in discrimination decisions, even when grouping evidence based on the location of the true signal rather than subjects’ decisions (<span class="math inline">\(t(97) = 0.83\)</span>, <span class="math inline">\(p = .407\)</span>).</p>
</div>
<div id="discrimination-confidence-1" class="section level5 unnumbered">
<h5>Discrimination confidence</h5>
<p>Luminance within the first 300 milliseconds had a significant effect on confidence ratings (<span class="math inline">\(t(97) = 7.23\)</span>, <span class="math inline">\(p &lt; .001\)</span>; see Fig. <a href="3-ch-RC.html#fig:RC-exp3-discrimination-RC">3.12</a>, right panels). A positive evidence bias in discrimination confidence was only marginally significant in this sample (<span class="math inline">\(t(97) = 1.63\)</span>, <span class="math inline">\(p = .106\)</span>).</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RC-exp3-discrimination-RC"></span>
<img src="figure/RC/RC-exp3-discrimination-RC-enhanced.png" alt="Decision and confidence discrimination kernels, Experiment 3. Same conventions as Fig. 3.8." width="\textwidth" />
<p class="caption">
Figure 3.12: Decision and confidence discrimination kernels, Experiment 3. Same conventions as Fig. <a href="3-ch-RC.html#fig:RC-exp2-discrimination-RC">3.8</a>.
</p>
</div>
</div>
</div>
<div id="detection-1" class="section level4" number="3.4.2.4">
<h4><span class="header-section-number">3.4.2.4</span> Detection</h4>
<p>Similar to Exp. 2, sum luminance had a significant effect on participants’ detection responses during the first 300 milliseconds (<span class="math inline">\(t(97) = 10.94\)</span>, <span class="math inline">\(p &lt; .001\)</span>; see Fig. <a href="3-ch-RC.html#fig:RC-exp3-detection-RC-figure">3.13</a>, left panel). Recall that a surprising finding in Exp. 1 was that sum luminance on detection decisions had no effect on participants’ confidence in their judgments of stimulus presence. In contrast, in Exp. 3 sum luminance had a significant positive effect on decision confidence when reporting target presence (‘yes’ responses; <span class="math inline">\(t(97) = 3.54\)</span>, <span class="math inline">\(p = .001\)</span>), and a significant negative effect on confidence when reporting target absence (‘no’ responses; <span class="math inline">\(t(97) = -3.04\)</span>, <span class="math inline">\(p = .003\)</span>; see Fig. <a href="3-ch-RC.html#fig:RC-exp3-detection-RC-figure">3.13</a>, middle and right panels).</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RC-exp3-detection-RC-figure"></span>
<img src="figure/RC/RC-exp3-detection-RC-enhanced.png" alt="Decision and confidence detection kernels, Experiment 3. Same conventions as Fig. 3.9." width="\textwidth" />
<p class="caption">
Figure 3.13: Decision and confidence detection kernels, Experiment 3. Same conventions as Fig. <a href="3-ch-RC.html#fig:RC-exp2-detection-RC-figure">3.9</a>.
</p>
</div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RC-exp3-signal-RC"></span>
<img src="figure/RC/RC-exp3-signal-RC-enhanced.png" alt="Decision and confidence kernels for detection signal trials, Experiment 3. Same conventions as Fig. 3.10." width="\textwidth" />
<p class="caption">
Figure 3.14: Decision and confidence kernels for detection signal trials, Experiment 3. Same conventions as Fig. <a href="3-ch-RC.html#fig:RC-exp2-signal-RC">3.10</a>.
</p>
</div>
</div>
<div id="detection-signal-trials-2" class="section level4" number="3.4.2.5">
<h4><span class="header-section-number">3.4.2.5</span> Detection signal trials</h4>
<p>As in Exp. 2, here we also asked how random variability in luminance in the target (brighter) and non-target (darker) channels affected detection decision and confidence. When deciding whether one of the two flickering patches was brighter than the background, participants were sensitive to positive noise in the target patch more than to negative noise in the non-target patch (<span class="math inline">\(t(97) = 10.94\)</span>, <span class="math inline">\(p &lt; .001\)</span>), consistent with a positive evidence bias in detection decisions and replicating findings from Exps. 1 and 2. Random fluctuations in luminance in the first 300 milliseconds of the trial also contributed to confidence in detection ‘yes’ responses (hit trials; <span class="math inline">\(t(97) = 6.07\)</span>, <span class="math inline">\(p &lt; .001\)</span>). Importantly, however, and in contrast to the results of Exp. 1 and 2, confidence in ‘yes’ responses was more sensitive to positive evidence than to conflicting evidence (<span class="math inline">\(t(97) = 3.49\)</span>, <span class="math inline">\(p = .001\)</span>). Together these results are consistent with a positive evidence bias not only for detection decisions, but also for detection confidence.</p>
<!-- In fact, the only comparison that survived a correction for multiple comparisons for 12 timepoints and 4 spatial positions was in the foil stimulus, such that participants were more confident in their 'yes' responses if the foil stimulus was darker 75 ms after stimulus onset.  -->
<p>Confidence in ‘miss’ trials was independent of the contrast in luminance between the right and left stimuli (<span class="math inline">\(t(96) = 0.89\)</span>, <span class="math inline">\(p = .374\)</span>) but, as described above, confidence in ‘no’ responses was sensitive to the overall luminance of the display. This negative effect of luminance on confidence in ‘no’ responses was significant for the non-target stimulus (<span class="math inline">\(t(96) = -2.91\)</span>, <span class="math inline">\(p = .005\)</span>), and marginally significant for the target stimulus (<span class="math inline">\(t(96) = -1.67\)</span>, <span class="math inline">\(p = .099\)</span>). In other words, and similar to our findings in Exp. 2, for both stimuli higher confidence was associated with lower luminance values. This is again consistent with our observation that confidence in judgments about stimulus absence is based on the overall darkness of the display.</p>
</div>
<div id="evidence-weighting" class="section level4" number="3.4.2.6">
<h4><span class="header-section-number">3.4.2.6</span> Evidence-weighting</h4>
<p>In Experiments 1 and 2, confidence in judgments about stimulus presence was invariant to sum evidence (overall motion energy in Exp.1, sum luminance in Exp. 2). This was surprising for two reasons. First, in both cases sum motion energy did have a significant effect on detection decisions. Second, incorporating information about sum evidence into confidence in the presence of a stimulus is rational: a target stimulus is more likely to be present when both stimuli are brighter. As we document above, however, this surprising finding did not replicate in Exp. 3, where detection confidence was now sensitive to the overall brightness of the display. We contrasted the two luminance conditions as a direct experimental test of differential evidence weighting on detection decisions and confidence.</p>
<p>In order to increase statistical power for tests of a positive evidence bias, in Exp. 3 half of the trials consisted of slightly brighter stimuli. In detection, participants were more likely to respond ‘yes’ on these high-luminance trials (<span class="math inline">\(M = 0.09\)</span>, 95% CI <span class="math inline">\([0.07\)</span>, <span class="math inline">\(0.11]\)</span>, <span class="math inline">\(t(97) = 8.48\)</span>, <span class="math inline">\(p &lt; .001\)</span>). Overall luminance is a valid cue for signal presence, so relying on it for detection judgments is rational. In discrimination, participants were also more confident in high-luminance trials (<span class="math inline">\(M = 0.02\)</span>, 95% CI <span class="math inline">\([0.01\)</span>, <span class="math inline">\(0.04]\)</span>, <span class="math inline">\(t(97) = 3.28\)</span>, <span class="math inline">\(p = .001\)</span>), replicating a positive evidence bias for discrimination confidence.</p>
<p>In line with the reverse correlation analysis of Exp. 3 (and in contrast to the findings of Experiments 1 and 2), participants were more confident in their ‘yes’ responses when overall luminance was higher (<span class="math inline">\(M = 0.02\)</span>, 95% CI <span class="math inline">\([0.01\)</span>, <span class="math inline">\(0.03]\)</span>, <span class="math inline">\(t(97) = 3.01\)</span>, <span class="math inline">\(p = .003\)</span>). Our pre-registered Bayesian analysis provided strong evidence for the alternative hypothesis that detection confidence is affected by this manipulation (<span class="math inline">\(\mathrm{BF}_{\textrm{10}} = 10.90\)</span>). Furthermore, this increase in ‘yes’ response confidence as a function of the brightness manipulation was not significantly different from that observed for discrimination confidence (<span class="math inline">\(M = -0.01\)</span>, 95% CI <span class="math inline">\([-0.03\)</span>, <span class="math inline">\(0.01]\)</span>, <span class="math inline">\(t(97) = -0.55\)</span>, <span class="math inline">\(p = .584\)</span>).</p>
<p>Finally, and in line with Exp. 2, overall luminance had a significant negative effect on confidence in ‘no’ responses (<span class="math inline">\(M = -0.02\)</span>, 95% CI <span class="math inline">\([-0.03\)</span>, <span class="math inline">\(-0.01]\)</span>, <span class="math inline">\(t(97) = -3.01\)</span>, <span class="math inline">\(p = .003\)</span>), indicating that participants were more confident in the absence of a target when overall luminance was lower.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RC-exp3-conf-boost"></span>
<img src="figure/RC/confidence_boost_enhanced.png" alt="Difference in confidence between standard and higher-luminance trials for the three response categories (detection ‘yes’ and ‘no’ responses, and discrimination responses) in Exp. 3. Box edges and central lines represent the 25, 50 and 75 quantiles. Whiskers cover data points within four inter-quartile ranges around the median. Stars represent significance in a two-sided t-test: **: p&lt;0.01" width="\textwidth" />
<p class="caption">
Figure 3.15: Difference in confidence between standard and higher-luminance trials for the three response categories (detection ‘yes’ and ‘no’ responses, and discrimination responses) in Exp. 3. Box edges and central lines represent the 25, 50 and 75 quantiles. Whiskers cover data points within four inter-quartile ranges around the median. Stars represent significance in a two-sided t-test: **: p&lt;0.01
</p>
</div>
</div>
</div>
</div>
<div id="discussion-2" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Discussion</h2>
<p>In three experiments, we compared the perceptual drivers of decisions and confidence ratings in discrimination and detection, matched for difficulty (Exp. 1) and signal strength (Exp. 2 and 3). In order to measure the contribution of perceptual evidence to confidence in detection and discrimination confidence ratings, we followed <span class="citation">Zylberberg, Barttfeld, &amp; Sigman (2012)</span> and applied reverse correlation to noisy stimuli in perceptual decision making tasks. We fully replicated the main results of Zylberberg and colleagues: decisions and confidence were affected by perceptual evidence in the first 300 milliseconds of the trial, peaking at around 200 milliseconds. We also successfully replicated a positive evidence bias for discrimination confidence: confidence in the discrimination task was more affected by supporting than by conflicting evidence. A positive evidence bias in discrimination confidence judgments may indicate that participants adopt a detection-like disposition in their metacognitive judgments, focusing on sum evidence rather than relative evidence when rating their confidence.</p>
<p>In Experiments 1 and 2, detection decisions but not confidence ratings also showed a positive evidence bias: when making a detection response participants mostly ignored random fluctuations in stimulus energy that were not aligned with the true, presented signal, but these fluctuations were later taken into account when rating their confidence. Based on this surprising finding, in Experiment 3 we pre-registered an hypothesis that detection confidence should be equally sensitive to positive and negative evidence. To increase our statistical sensitivity, we doubled the number of detection trials and included a direct manipulation of positive evidence. Results from Experiment 3 provided clear evidence against the hypothesis derived from Experiments 1 and 2, and support an unequal weighting of positive and negative evidence not only for detection decisions, but also for detection confidence judgments.</p>
<p>Previous accounts of the positive evidence bias in discrimination confidence presented it as a heuristic that participants adopt due to cognitive constraints in the face of unreasonably vast representational spaces <span class="citation">(Maniscalco, Peters, &amp; Lau, 2016)</span> or due to an asymmetric encoding of signal and noise <span class="citation">(Miyoshi &amp; Lau, 2020)</span>. A heuristic use of evidence in confidence ratings, but not in the decision itself, in turn implies that different processes are involved in the generation of decisions and confidence ratings, and that participants are in some sense being irrational when discarding relevant evidence that could be used in constructing confidence.</p>
<p>Similarly, a positive-evidence bias was recently demonstrated in an artificial neural network trained to classify hand-written digits and, in parallel, predict its classification accuracy <span class="citation">(Webb, Miyoshi, So, &amp; Lau, 2021)</span>. The network was trained on images varying in contrast and visual noise, and later tested on overlays of two digits, varying in contrast only. Under this training regime, the network was more confident for high-contrast images, controlling for classification accuracy. Similar to the heuristic account of <span class="citation">Maniscalco, Peters, &amp; Lau (2016)</span>, here also a positive evidence bias reflected the application of an inductive bias acquired in real life, or in training, to a test setting where doing so is maladaptive.</p>
<p>An alternative possibility is that a single, Bayes-rational model with a valid prior is governing both choice and confidence ratings, but that the form of that model is yet to be specified. For instance, one possible driver of a positive evidence bias in discrimination confidence ratings is the higher informational value of signal than noise. If the signal channel holds more information about signal identity or stimulus presence, giving more weight to information from this channel is rational. This is the case in unequal-variance SDT settings, where signal is sampled from a wider range of values than noise. As an example, if noise is sampled from a Gaussian distribution with mean 0 and variance 1 and signal from a Gaussian distribution with mean 2 and variance 9, sampling the value 7 (two standard deviations to the right of the signal distribution) is much more informative about the presence or absence of a signal than sampling the value -2 (two standard deviations to the left of the noise distribution), because the first is only likely if sampled from the signal distribution (<span class="math inline">\(\frac{p(x|signal)}{p(x|noise)}&gt;1,000,000,000\)</span>), but the second is likely under both distributions (<span class="math inline">\(\frac{p(x|signal)}{p(x|noise)}=1.5\)</span>).<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> Similarly, if the representation of coherent motion is more variable across trials than the representation of random motion, participants would be rational to give more weight to evidence for coherent motion in one channel than evidence for its absence in the other channel.</p>
<p>Higher variability in the representation of signal is often built into the experiment itself. For example, in our Exp. 1, following <span class="citation">Zylberberg, Barttfeld, &amp; Sigman (2012)</span>, the number of coherently moving dots was itself randomly determined, sampled from a Gaussian distribution once every four frames. This means that there were two sources of variability for the true direction of motion (variability in the direction of randomly moving dots and variability in the number of coherently moving dots), but only one source of variability for the opposite direction (variability in the direction of randomly moving dots). But even when signal is not made more variable by design, the representation of signal is expected to be more variable due to the Weber-Fechner law <span class="citation">(Fechner &amp; Adler, 1860)</span> and the coupling between firing rate mean and variability implied by a Poisson form of neuronal firing rates.</p>
<p>To obtain qualitative predictions for such effects, we simulated a stimulus-dependent noise model (full simulation details, including source code are available in appendix <a href="D-appRC-everything.html#app2-simulation">D.5</a>). To model the unequal variance nature of the perception of signal and noise, perceptual noise was sampled from a normal distribution with mean 0 and a standard deviation proportional to the exponent of the sensory sample (<span class="math inline">\(x&#39;=x+\epsilon; \epsilon \sim \mathcal{N}(0,2^x)\)</span>). We chose to use the exponent of the sensory sample in order to have positive values only for the standard deviation, but qualitatively similar results are obtained for a linear mapping from sensory samples to sensory noise. A Bayes-rational agent had full knowledge of this generative model for extracting a Log Likelihood Ratio in the process of making a decision and rating their confidence.</p>
<p>This simulation gave rise to a pronounced positive evidence bias in discrimination confidence ratings and in detection decisions (see Fig. <a href="3-ch-RC.html#fig:RC-2d-model-2">3.16</a>). The agent was more sensitive to variations in the signal channel both for deciding whether a signal was present or not, when rating its confidence in discriminating between two stimulus classes, and when rating its confidence in decisions about stimulus presence. This is in line with our empirical findings. Importantly, in this model decision and confidence ratings are the output of the same Bayes-rational process applied to a situation where perceptual noise scales with signal strength, and do not reflect any suboptimality in evidence weighting.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RC-2d-model-2"></span>
<img src="figure/RC/2dmodel_2_enhanced.png" alt="Model predictions for a stimulus-dependent noise model. Sensory noise is higher for stronger sensory samples. For each point on the grid, we simulated 200 trials by sampling a sensory sample and extracting a decision and a confidence rating according to the log likelihood ratio of the two hypotheses. Left: two example sensory samples, marked with an X, have the same relative evidence, but absolute evidence is higher for the sample on the upper right. This sample was given a higher-confidence rating, consistent with a positive evidence bias in behaviour, here emerging from a Bayes-rational response and confidence strategy." width="\textwidth" />
<p class="caption">
Figure 3.16: Model predictions for a stimulus-dependent noise model. Sensory noise is higher for stronger sensory samples. For each point on the grid, we simulated 200 trials by sampling a sensory sample and extracting a decision and a confidence rating according to the log likelihood ratio of the two hypotheses. Left: two example sensory samples, marked with an X, have the same relative evidence, but absolute evidence is higher for the sample on the upper right. This sample was given a higher-confidence rating, consistent with a positive evidence bias in behaviour, here emerging from a Bayes-rational response and confidence strategy.
</p>
</div>
<p>However, a stimulus-dependent-noise model makes predictions not only for the effect of sum evidence on discrimination and detection confidence ratings, but also for the effect of sum evidence on decision performance (<span class="math inline">\(d&#39;\)</span>). Specifically, if stronger stimuli are also noisier, sum evidence should have a positive effect on confidence, but a negative effect on response accuracy. In contrast with this prediction, in Exp. 3, an increase to the luminance of both stimuli had no effect on accuracy (<span class="math inline">\(M = 0.01\)</span>, 95% CI <span class="math inline">\([-0.01\)</span>, <span class="math inline">\(0.03]\)</span>, <span class="math inline">\(t(97) = 1.06\)</span>, <span class="math inline">\(p = .294\)</span>), but boosted discrimination confidence nonetheless. Moreover, the effect of overall luminance on confidence was positively correlated with its effect on decision confidence (<span class="math inline">\(r = .32\)</span>, 95% CI <span class="math inline">\([.13\)</span>, <span class="math inline">\(.49]\)</span>, <span class="math inline">\(t(96) = 3.33\)</span>, <span class="math inline">\(p = .001\)</span>) and not negatively correlated as would be expected if degraded accuracy and higher confidence were both driven by higher sum evidence.</p>
<p>Goal-contingent effects also weigh against a Bayes-rational account of the positive evidence bias in discrimination confidence. In a recent study, <span class="citation">Sepulveda et al. (2020)</span> presented participants with pairs of dot arrays and asked them to choose the array with more white dots. Participants were more confident when both arrays included more dots, replicating a positive evidence bias. Critically, the experiment also included a second condition, in which participants were asked to choose the array with <em>fewer</em> white dots. In this condition, confidence was higher when both arrays included fewer dots: an effect opposite to the positive evidence bias. This effect was also related to participants’ information sampling behaviour in the two conditions: they spent more time fixating their gaze at the array containing more dots under typical instructions, but the opposite was the case when they were instructed to select the array with fewer dots. In a Bayes-rational model, evidence weighting should be identical for these two equivalent ways of framing the task instructions. This finding is also inconsistent with heuristic models that are based at variance differences between the encoding of signal and noise, as those are not expected to change as task instructions change.</p>
<p>Instead, our findings, as well as those of <span class="citation">Sepulveda et al. (2020)</span>, are generally in line with a heuristic account of positive evidence bias that posits limits on cognitive resources when coping with high-dimensional representations <span class="citation">(Maniscalco, Peters, &amp; Lau, 2016)</span>. Basing confidence on positive evidence in such a world frees agents from the need to consider an infinite number of alternative hypotheses. Similarly, the findings of <span class="citation">Sepulveda et al. (2020)</span> can be accounted for by a model in which participants flexibly allocate attention to the choice-consistent dimension of evidence (more vs. fewer dots), while ignoring other dimensions. What constitutes positive evidence is then rationally dependent on an agent’s specific goals and attentional set at the time of performing a task.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>We reused the original Matlab code that was used for Exp. 1 in Zylberberg et. al. (2012), kindly shared by Ariel Zylberberg. <a href="3-ch-RC.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>In other words, the expected log likelihood ratio when sampling from the signal distribution is higher (in absolute terms) than when sampling from the noise distribution, or <span class="math inline">\(D_{KL}(signal||noise)&gt;D_{KL}(noise||signal)\)</span>.<a href="3-ch-RC.html#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="2-ch-MVS.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="4-ch-fMRI.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["thesis.pdf", "PDF"], ["thesis.epub", "EPUB"], ["thesis.docx", "Word"]],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
